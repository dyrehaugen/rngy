[["index.html", "Energy 1 Energy", " Energy Dyrehaugen Web Notebook 2023-12-06 1 Energy Energy is the “universal currency”. "],["energy-overview.html", "2 Energy Overview 2.1 Energy Civilization 2.2 World Energy Outlook 2.3 Energy Power Game 2.4 World Energy Consumption per Capita", " 2 Energy Overview 2.1 Energy Civilization From early humans rubbing sticks together to make fire, to the fossil fuels that drove the industrial revolution, energy has played a central role in our development as a species. But the way we power our societies has also created humanity’s biggest challenge. It’s one that will take all our ingenuity to solve. Energy is the key to humanity’s world domination. We live in a fossil fuel society But while they have lifted ever more of us out of agrarian hardship, and created our global economy and high living standards, the catastrophic climate change they are creating now threatens to derail that society. Just as two centuries ago we reached the limits of what agriculture could do, now global warming is imposing a limit on what coal, oil and gas can safely do. It has created the greatest challenge human society has ever faced - moving back to relying on the daily influx of energy from the Sun to meet the huge energy needs of eight billion people and counting. Rowlatt (BBC) The Real Reason Humans are the Dominant Species 2.2 World Energy Outlook The World Energy Outlook, the IEA’s flagship publication, provides a comprehensive view of how the global energy system could develop in the coming decades. This year’s exceptional circumstances require an exceptional approach. The usual long-term modelling horizons are kept but the focus for the World Energy Outlook 2020 is firmly on the next 10 years, exploring in detail the impacts of the Covid-19 pandemic on the energy sector, and the near-term actions that could accelerate clean energy transitions. The analysis targets the key uncertainties facing the energy sector in relation to the duration of the pandemic and its implications, while mapping out the choices that would pave the way towards a sustainable recovery. The strategic insights from the WEO-2020 are based on detailed modelling of different potential pathways out of the crisis, covering all regions, fuels and technologies and using the latest data on energy markets, policies and costs. (IEA pdf at 120 Euro!! - Why the ….need IEA to sell theirpublications?) Ever since the agency was founded in 1974 to measure the world’s energy systems and anticipate changes, the yearly World Energy Outlook has been a must-read document for policymakers the world over. Over the last two decades, however, the IEA has consistently failed to see the massive growth in renewable energy coming. Not only has the organisation underestimated the take-up of solar and wind, but it has massively overstated the demand for coal and oil. IEA 2020-edition Incredibly cheap Chinese Solar The Chinese approach to renewables is all about energy security,” Mathews says. “At the scale from which they’re building new industries, they would need colossal imports of conventional fossil fuels, which would cripple them economically. “They can get around that problem, which is a geopolitical obstacle, by manufacturing their own energy equipment.” One such innovation is the stackable solar cell. Though still a niche technology very much in the early stages, the basic idea is to lay a material over a solar cell in order to boost its power output. “We think a 40% module, rather than the 22% you can do nowadays with PERC, is what the industry will be doing once we perfect this stacking approach,” Green says. “We’re just trying to find a new cell that will have all the qualities of silicon that we can stack on top of silicon. “The International Energy Agency now says solar is providing the cheapest energy the world has ever seen. But we’re headed towards a future of insanely cheap energy. “It’s a fundamentally different world we’re moving into.” Guardian 2.3 Energy Power Game Tooze Realizing decarbonization is a one-way bet for Europe and Asia changes the oil and gas industry’s competitive game. So long as they could plan for the long term, OPEC and Russia could afford to contemplate a modus vivendi with shale. Once Eurasian decarbonization begins to accelerate in earnest, it will no longer makes sense for OPEC and Russia to continue the game. Faced with the fossil fuel endgame, a final price war is their best strategy. The result will be a massive shock to oil and gas prices. And this time, as demand for fossil fuels progressively shrinks, low prices will be permanent. The losers in that ferocious competition will be high-cost producers around the world. Once decarbonization takes hold, what will dominate the remaining oil and gas markets is leading OPEC members’ ultra-low cost base. Whereas competing with Saudi Arabia and Qatar in dwindling global markets for oil and gas is a fool’s errand, electricity markets are protected by the cost of long-distance transmission. Tooze (2021) Fossil Fuel’s Downfall Could Be America’s Too 2.4 World Energy Consumption per Capita Figure: World per capita energy consumption, with the 1950-1980 period of rapid growth highlighted. World Energy Consumption by source, based on Vaclav Smil’s estimates from Energy Transitions: History, Requirements and Prospects (Appendix) together with data from BP’s Statistical Review of World Energy for 1965 and subsequent years. Population estimates used to produce per capita amounts are based on estimates by Angus Maddison for dates prior to 1950. They are based on UN estimates for more recent years. Chart prepared by Gail Tverberg in 2018.* Tverberg (2023) Today’s energy bottleneck may bring down major governments "],["energy-demand.html", "3 Energy Demand 3.1 Energy Statistics 3.2 Energy Transition not Enough", " 3 Energy Demand Humanity’s rate of energy consumption (i.e., power) of 16.1 TW in 2010 is projected to increase by 94 – 247% by 2050, with a reference scenario at 140% given by SSP2 and RCP 4.5 (van Ruijven et al., 2019). On track with these expectations, humanity reached 18.9 TW in 2018 - a yearly 2% growth since 2010. Leiva (2021) Why the energy transtition is not enough 3.1 Energy Statistics BP Statistical Review of World Energy 3.2 Energy Transition not Enough Leiva Abstract Efforts to accommodate the growth in global energy consumption within a fragile biosphere are primarily focused on managing the transition towards a low-carbon energy mix. We show evidence that a more fundamental problem exists through a scaling relation, akin to Kleiber’s Law, between society’s energy consumption and material stocks. Humanity’s energy consumption scales at 0.78 of its material stocks, which implies predictable environmental pressure regardless of the energy mix. If true, future global energy scenarios imply vast amounts of materials and corresponding environmental degradation, which have not been adequately acknowledged. Thus, limits to energy consumption are needed regardless of the energy mix to stabilize human intervention in the biosphere. Leiva Memo We show evidence that the continued increase of energy throughput faces a more fundamental problem. The use of energy requires prime movers such as people, engines, computers, etc. that are built from materials found originally in nature. Moreover, the use of energy inevitably rearranges materials in the environment. This posits a problem that goes beyond the carbon content and speci c material requirements of given technologies, and thus cannot be solved through substitution. Higher power rates need and provoke more materials (metals and others) being rearranged from otherwise healthy ecosystems into social structures such as rms, cities, and governments and into goods such as furniture, electronics, and food. In fact, the 20th century witnessed a 9-fold increase in humanity’s power alongside a 16-fold increase in its material stocks (Krausmann et al., 2017). We contend that the ecosystem degradation (MEA, 2005), biodiversity loss (IPBES, 2019), and dangerous human intervention in the Earth system (Steffen et al., 2015) that followed from such harvesting of the biosphere could have only partially been avoided with a low-carbon energy mix. We evaluate what we believe is the rst preliminary evidence of a coupling, akin to Kleiber’s Law in biological organisms, between power and the material stocks of social systems. Figure: Panel A depicts Extended-Kleiber’s Law relation between per capita power, population, and material stocks. The yellow plane shows the 0.78 scaling of material stocks as a function of per capita power and population. In 1900, a population of 1.6 billion people averaging 0.8 kW/capita implied material stocks of 35 Gt (black plane). In 1950, a population of 2.5 billion averaging 1.3 kW/capita implied materials stocks of 81Gt (green plane). In 2000, a population of 6.1 billion people averaging 2.1 kW/capita implied material stocks of 565 Gt (blue plane). In 2050, a projected 9.7 billion people averaging 4.0 kW/capita implies material stocks of 1901 G (red plane). The rapidly increasing distance between the material stock planes at equal 50-year intervals depicts the Great Acceleration. Panel B shows the intersections of the yellow plane at each material stock plane of Panel A to depict iso-material stock curves: the compensation between population and per capita power to maintain the respective material stock levels. Note that these curves only show point estimates of the Extended Kleiber ́s Law and therefore need not show the exact historical values of population and per capita power. The dashed-black line in Panel B shows the 2010 iso-material stock curve (2010 level not shown in Panel A). The line depicts 792 Gt of material stocks with 6.9 billion people averaging 2.8 kW/capita (orange marker, actual data for 2010 is 6.9 billion people averaging 2.3 kW/capita). The dashed-black line implies that maintaining Humanity’s materials stock at 792 Gt while growing to 9.7 billion people requires 2.0 kW/capita (red marker). If true, this relation implies a strict limit to power growth that cannot be addressed with low-carbon energy sources. Current projections of energy consumption and population growth imply unsustainable levels of material stocks as they imply considerable additional harvesting of the biosphere. Current material stocks can be maintained while accommodating population growth through 2050 if a 2.0 kW/capita limit is established. Although this limit has been shown to be enough for a digni ed life, it implies a considerable degrowth in more than half of the world ́s countries. Theory Why do social systems ́ power and mass scale this way? One idea is based on fractal geometry (West et al., 1999), where the invariant “length” could be given by the individual person. Brown et al. (2011) use this idea arguing that “The energy and other resources that sustain these systems [animals and economies] are supplied by hierarchically branching networks, such as the blood vessels and lungs of mammals and the oil pipelines, power grids, and transportation networks of nations. Models of these networks suggest that three-quarter-power scaling optimizes distribution of resources”. Another idea is based on size-dependent limitation of resource storage (Maino et al., 2014; Thommen et al., 2019), where the role of macromolecules could be played by energy goods. A third idea is based on the interaction of physiological features with environmental conditions (Koziowski &amp; Weiner, 1997), where growth and reproduction could be given by an economy ́s aggregate investment and consumption. In any case, noting that the theoretical basis of Kleiber’s Law remains controversial after 80 years of research (Escala, 2019; Hulbert, 2014), nding a theoretical explanation for an extended Kleiber ́s Law remains as future research that may become foundational science towards Humankind ́s sustainability. In the meantime, further data on mass and energy of nations is important to broaden the empirical basis of this relationship. If population reductions are not an option, the most reasonable response would be a worldwide 2.0 kW/capita limit. A speci c limit at 2.0 kW/capita has been proposed by the 2000-watt society since 1998 based on the per capita power of western Europe during the 60s and the digni ed life it enabled, and according to our results, it is coincidentally the value required to maintain material stocks in check below 800 Gt globally by 2050 given expected population levels. Leiva (2021) Why the energy transtition is not enough (pdf) Kleiber’s Law (Wikipedia) "],["energy-intensity.html", "4 Energy Intensity 4.1 Power Density", " 4 Energy Intensity Energy intensity is a measure of the energy inefficiency of an economy. It is calculated as units of energy per unit of GDP. Wikipedia GDP and energy consumption per capita variables are cointegrated and Granger cause each other. First conclusion would be that energy consumption per capita affects GDP per capita. In other words, a growth or reduce in energy consumption will increase or decrease GDP. Energy consumption plays an important role on economic growth, directly on labor and capital component and indirectly on production process as well. Electric consumption is an incentive factor and indis- pensable insurance for a sustainable economic growth. Second conclusion would be that GDP per capita affects energy consumption per capita. That is, an increase or decrease GDP per capita will increase or decrease energy consumption per capita. Developing countries that are increasing their aggregate GDP and their production are subject to demand more and more energy sources. Countries that are in short providing the appropriate energy demand will be importing energy. This might eventually cause current and trade deficit. Yildirim (2017) Economic Growth and Energy Consumption OECD (pdf) 4.1 Power Density Smil (Book) In this book, Vaclav Smil argues that power density is a key determinant of the nature and dynamics of energy systems. Any understanding of complex energy systems must rely on quantitative measures of many fundamental variables. Power density—the rate of energy flux per unit of area—is an important but largely overlooked measure. Smil provides the first systematic, quantitative appraisal of power density, offering detailed reviews of the power densities of renewable energy flows, fossil fuels, thermal electricity generation, and all common energy uses. Smil shows that careful quantification, critical appraisals, and revealing comparisons of power densities make possible a deeper understanding of the ways we harness, convert, and use energies. Conscientious assessment of power densities, he argues, proves particularly revealing when contrasting the fossil fuel–based energy system with renewable energy conversions. Smil explains that modern civilization has evolved as a direct expression of the high power densities of fossil fuel extraction. He argues that our inevitable (and desirable) move to new energy arrangements involving conversions of lower-density renewable energy sources will require our society—currently dominated by megacities and concentrated industrial production—to undergo a profound spatial restructuring of its energy system. Vaclav Smil (2015) Power Density (Book) Smil (Primer) Many factors combine to determine their technical difficulty, their cost and their environmental impacts. A great deal of attention has been recently paid to the pace of technical innovation needed for the shift from the world dominated by fossil fuel combustion to the one relying increasingly on renewable energy conversions, to the likely costs and investment needs of this transitions, and to its environmental benefits, particularly in terms of reduced CO 2 emissions. Inexplicably, much less attention has been given to a key component of this grand transition, to the spatial dimension of replacing the burning of fossil fuels by the combustion of biofuels and by direct generation of electricity using water, wind, and solar power. Perhaps the best way to understand the spatial consequences of the unfolding energy transition is to present a series of realistic power density calculations for different modes of electricity generation in order to make revealing comparisons of resources and conversion techniques. Detailed calculations will make it easy to replicate them or to change the assumptions and examine (within realistic constraints) many alternative outcomes. Energy Density vs Power Density Energy density is easy – power density is confusing. Energy density is simply the amount of energy per unit weight (gravimetric energy density) or per unit volume (volumetric energy density). With energy expressed (in proper scientific terms) in joules or less correctly in calories (and in the US, the only modern state that insists on using outdated non-metric measures, in BTUs), with weight in grams (and their multiples), and with volume in cubic centimeters, liters (dm 3 ) or cubic meters, energy density is simply joules per gram (J/g) or joules per cubic centimeter (J/cm 3 ) or, more commonly, megajoules per kilogram (MJ/kg) and megajoules per liter (MJ/L) or gigajoules per ton (GJ/t) and gigajoules per cubic meter (GJ/m 3 ). Power density is a much more complicated variable. Power density expressed as energy flux per unit of horizontal surface universal. \\(W/m^{2}\\) of horizontal area of land or water surface has been receiving more attention because of the growing interest in renewable energy resources and their commercial conversions to fuels and electricity. Invariably, power densities of these stocks and flows are considerably lower than power densities and uses of fossil fuels, those highly concentrated stores of ancient photosynthetic production – and these differences are a key factor in determining the potential contribution of renewable energies to the world’s future fuel and electricity supply. Low-High Power Density estimate Power Source Power Density (\\(W/m^{2}\\)) Natural Gas 200 2000 Coal 100 1000 Solar (PV) 4 9 Solar (CSP) 4 10 Wind 0.5 1.5 Biomass 0.5 0.6 Implications of these differences are manifold. Changing the power density-determined infrastructure of energy systems that were created over more than a century for electricity generation from fossil fuel combustion will not be easy. A fossil-fuelled civilization has been securing the supply of its most flexible form of energy by “shifting downward,” that is by generating electricity with power densities 1-3 orders of magnitude higher than the common power densities with which electricity is used in buildings, factories and cities. In a civilization that would rely only on renewable energy flows, but that would inherit today’s urban and industrial systems, we would produce electricity at best with the same power densities with which they would be used –- but more often we would have to concentrate diffuse flows of solar radiation, wind, and biomass in order to bridge power density gaps of 2-3 orders of magnitude. This new energy infrastructure would increase fixed land requirements and preempt any other form of land use in areas devoted to PV cells, heliostats or fast-growing wood plantations. Most of the area occupied by large wind farms could be used for crops or grazing but other land uses would be excluded, and large areas dotted with wind turbines would require construction and maintenance of access roads as well as the creation of buffer zones not suitable for permanent human habitation. And in all cases of renewable energy conversion, much more land would be needed for more extensive transmission rights-of-way in order to export electricity from sunny and windy regions, or from areas suited for mass-scale biomass production, to major urban and industrial areas. Figure: Power densities of fossil fuel extraction, thermal electricity generation and renewable modes of electricity production. As a result, these new energy infrastructures would have to be spread over areas ten to a thousand times larger than today’s infrastructure of fossil fuel extraction, combustion and electricity generation: this is not an impossible feat, but one posing many regulatory (environmental assessments of affected areas, rights-of-way permission and inevitable lawsuits), technical and logistic challenges. Higher reliance on renewable energies may be desirable (mainly because of perceived environmental and strategic reasons) and technical advances would also make it an increasingly appealing economic choice –- but inherently low power densities of these conversions will require a new system of fuel and electricity supply that will be able to substitute for today’s dominant practices only after decades of gradual development. Vaclav Smil (2010) Power Density Primer (pdf) "],["energy-return.html", "5 Energy Return 5.1 EROEI", " 5 Energy Return 5.1 EROEI Tverberg “Energy Return on Energy Investment” (EROEI) pertains to direct use of energy, rather than energy embodied in added complexity. As a result, EROEI indications tend to suggest that innovations such as wind turbines, solar panels and EVs are more helpful than they really are. Other measures similar to EROEI make a similar mistake. Energy needs are hidden in many areas. For example, to have a complex system, we need a financial system. The cost of this system cannot be added back in. We need modern roads and a system of laws. The cost of a government providing these services cannot be easily discerned. An increasingly complex system needs education to support it, but this cost is also hard to measure. Also, as we note elsewhere, having double systems adds other costs that are hard to measure or predict. Wind and solar generation need complexity to fix their intermittency problems. Complexity has an energy cost, but this cost is virtually impossible to measure. if the available energy supply is reduced, the system will need to simplify. Simplification usually doesn’t happen voluntarily. It is easy for EROEI calculations (and similar calculations) to overstate the benefit of complex types of energy supply. EROEI calculations consider only direct “energy investment” costs. For example, the calculations are not designed to collect information regarding the higher energy cost of a dual system, with parts of the system under-utilized for portions of the year. Annual costs will not necessarily be reduced proportionately. Tverberg (2023) Ramping up wind turbines, solar panels and electric vehicles can’t solve our energy problem Tainter (20??) Energy-Complexity Spiral (pdf) "],["energy-development.html", "6 Energy Development", " 6 Energy Development Energy development is the field of activities focused on obtaining sources of energy from natural resources. These activities include production of conventional, alternative and renewable sources of energy, and for the recovery and reuse of energy that would otherwise be wasted. Energy conservation and efficiency measures reduce the demand for energy development, and can have benefits to society with improvements to environmental issues. Figure: World total primary energy production in quadrillion Btu. Note: world total on left y-axis, while regional figures are shown on right y-axis (approx. figures in parenthesis for 2010 and 2011, respectively). World (510) China (97) United States (78) Russia (56) Europe (45) Africa (37) Central and South America (30) Wikipedia Conversion: 1 Btu = 0.00029329722222222 kWh 1 qudrillion = \\(10^{15}\\) unitjuggler "],["energy-transition.html", "7 Energy Transition 7.1 Minerals in Transition 7.2 IEA Net-zero 2050 7.3 Transition Risks 7.4 Transition Forecasting", " 7 Energy Transition Thompson Vaclav Smil has repeatedly pointed out that a green energy revolution would be qualitatively different than any energy change in human history because it involves moving from more concentrated to less concentrated energy, rather than in the opposite direction. However much companies and governments promise dramatic and rapid change, their deeds and small-print rhetoric point to the hard consequences of this reality. BP may be heralding a low carbon energy future, but its own 2020 annual report presumes the world will still be using between 80 and 100 million barrels of oil a day in 2040. The same report says that ‘significant levels of investment are required for there to be sufficient supplies of oil to meet demand in 2040.’ It takes some cognitive dissonance to believe this oil could still be produced whilst investors shut capital out of the privately-owned oil industry; that is, unless it is accepted that all future oil comes from Russia’s Rosneft and the Middle Eastern state-owned oil companies and is so expensive as to act as an impediment to growth. Still owning 19.75 per cent of Rosneft, BP has clearly hedged how it thinks the energy future will play out. To think about the energy-origins of western prosperity opens up difficult truths about the place of European empire and the United States’ Middle Eastern wars in the economic history of the twentieth and early twenty-first century. Part of climate idealism contains a desire to leave this unpalatability behind, replacing fossil-fuel imperialism with climate justice. But after the Second World War, western economic life depended on the oil that came out of the Persian Gulf, through the Suez Canal, and into pipelines running to the Mediterranean. The counter-factual that eliminates past wrongs takes a lot else with it, including that which most people in western democracies have little inclination to forsake. Given that battery production presently relies on cobalt mining done in grim conditions in the Democratic Republic of Congo, sometimes with child labour, and much of the solar-grade polysilicon used in solar panels is produced in Xinjiang, green energy will bring less ethical relief than often supposed. Our cognitive struggles with energy matters extend to our concepts of historical time. Both the dread of a coming apocalypse and a faith in endless human innovation and moral improvement appear hardwired into western culture, first from Christianity, and then its secular offshoot the Enlightenment. When confronted with collective existential questions, western minds reach rather easily to millenarian fears and hopes. Christianity began, after all, with the expectation of an apocalypse and the imminent arrival of God’s kingdom. Although the Roman Catholic Church made Latin Christianity worldly, and Augustine slammed down millenarianism as delusional, the original Christian spirit lingered, readily available as a lens to give moral meaning to past sins in times of social and economic crisis. The apparent ideational clarity of the apocalyptic moment now permeates radical climate activism, captured in Extinction Rebellion’s name, as well as the movement’s performative, and at times itinerant, political style. Helen Thopmson Tooze Net-zero at zero net cost? The big question is thus not how to mobilise the new money but how to ensure investment happening anyway flows in the right direction. McKinsey trumpets the conclusion that the overall cost to the EU of achieving net-zero by 2050 will be zero: energy savings will cover the costs of the investment. This is great news. But, as McKinsey knows only too well, that is not how trillions of investment are normally justified. They have to produce an adequate rate of return—opportunity cost is the measure that matters. It is a matter not of physical contraints but of political economy and on that score the news is less good. According to McKinsey, between now and 2050, almost half the necessary investment will not meet standard investment criteria. Up to 2030, due to the high cost of renewable technologies in their early stages, less than 40 per cent will be justifiable on commercial grounds. In industry and buildings, two sectors where emissions are hard to abate, a tiny fraction of the necessary investment will generate an adequate profit. It is in closing the gap between the investment that McKinsey has defined as necessary and that which it has defined as justifiable from a business point of view that government comes in. If the gap were to be closed by public expenditure, Europe’s governments would, according to McKinsey, need to mobilise €4.9 trillion in subsidies over 30 years. That is the amount of profit taxpayers would need to offer investors to get them interested in the energy transition—€365 for every man, woman and child in the EU27, every year for 30 years. Painful and unfair, no doubt, but hardly inconceivable. In any case, the public purse is only one way of driving business investment. An alternative is to use carbon pricing. McKinsey estimates that with a carbon price of €100 per ton 80 per cent of the necessary investment could be justified on commercial grounds. The funds generated from an emissions-trading system could then be recycled in subsidies and other promotional spending. In hard-to-abate sectors, direct interventions would remain indispensable. Adam Tooze 7.1 Minerals in Transition CarbonTracker on IEA Study The IEA’s latest piece on minerals[1] critical to the energy transition gives a rather pessimistic spin to what was some very positive data. Looked at from a wider perspective, the note provides another useful source of analytical support for the energy transition. The IEA looked into the amount of minerals needed to fuel the energy transition, and pretty quickly worked out ‘there is no shortage of resources’. The world has plenty of lithium, nickel, rare earth metals and so on. This is what the United States Geological Survey (USGS) has been saying for a while,[2] and fits with the work done by the Energy Transitions Commission[3] on mineral availability. The IEA notes for example that we have 170 times as much lithium reserves as annual demand and that our lithium reserves have increased by 42% over the last eight years as higher prices and the prospect of rising demand have drawn out new investment. Under the IEA’s 1.5 degrees scenario, we will need about twice the amount of critical minerals by 2040 (six times as much for the clean energy industry, but that is only part of global demand), and the IEA put forward a series of sensible suggestions (increase recycling, invest in new supply and so on) to ensure that we get it. However, their take then turns gloomier as we are warned about how hard this is going to be. Impressive charts show that the average electric vehicle uses 210kg of critical minerals compared to only 35kg for an ICE car and that a MW of solar generation capacity needs 6.5 tonnes of critical minerals compared to a coal plant which needs only 3 tonnes. We are then encouraged to think about all the ESG issues and environmental issues associated with the surge in mineral usage and to worry about supplier concentration, water usage, pollution and depletion. Stand back a moment however, and you can see immediately that the IEA are very selective in their presentation of the data. They look only at the stocks (the assets you need to build the generator or car) not the flows (the energy you need to run them). But the flows of energy are 2-3 orders of magnitude larger than the stocks, and this means that many of their conclusions are more useful for fossil fuel advocates than for policymakers. One way to demonstrate this is to look at the weight of the material that is required for a fossil fuel system versus a renewable system, as weight is a pretty good proxy for environmental impact. All that coal and oil has to be extracted and converted and shipped around, and at every stage it requires complex and heavy equipment which has an impact on communities and air quality. So we take IEA data to compare below the critical mineral requirement of the renewable system with the energy requirement of the fossil system, to get a more appropriate comparison. There are of course other materials like steel and cement to consider when building transition infrastructure, and other areas like transport networks, but it is safe to say that these will weigh considerably more for the heavy molecules of the fossil system than the light electrons of the renewable system. For example, take those 6.5 tonnes (half of it from silicon which is not exactly a rare mineral) that you need to build 1MW of solar capacity. It turns out that this is an absolutely immaterial number compared to the weight of the coal that is used to generate electricity. Over its lifetime of 30 years, 1MW of solar capacity will generate over 40,000 MWh of electricity, so the mineral requirement is just 0.15kg per MWh. Compare that to a coal fired generation station where the critical mineral requirement is indeed a bit less, but you need 350 kg of coal to generate that MWh.[4] On this calculation, coal generation will need more than 2,000 times more material by weight than solar generation. It is a similar story in the transport sector. The average car uses about a tonne of oil a year, or 15 tonnes over its lifetime. Compare that with the 210kg of critical minerals that the EV needs and the weight of the oil is 71 times higher than the weight of the minerals. You burn the oil only once, but the minerals can and will be recycled many times. carbonTracker IEA Study Minerals are essential components in many of today’s rapidly growing clean energy technologies – from wind turbines and electricity networks to electric vehicles. Demand for these minerals will grow quickly as clean energy transitions gather pace. This new World Energy Outlook Special Report provides the most comprehensive analysis to date of the complex links between these minerals and the prospects for a secure, rapid transformation of the energy sector. Alongside a wealth of detail on mineral demand prospects under different technology and policy assumptions, it examines whether today’s mineral investments can meet the needs of a swiftly changing energy sector. It considers the task ahead to promote responsible and sustainable development of mineral resources, and offers vital insights for policy makers, including six key IEA recommendations for a new, comprehensive approach to mineral security. IEA Study (pdf) 7.1.1 Copper Tooze With supply from incumbent mines in countries such as Chile and Peru stalling, an estimated $118bn of investment by 2030 is needed to plug a copper supply gap that will by next decade be equivalent to 35 giant-sized mines. For three decades!, international mining companies have tussled with officials and locals over a patch of desert around an extinct volcano in Pakistan’s neglected, insurgent-prone western province of Balochistan. Now, after resolving years of legal disputes, Barrick Gold wants to invest $7bn to revive the mining project, Reko Diq, which experts believe contains one of the world’s largest untapped reserves of copper and gold. … “Reko Diq is one of the bigger copper-gold undeveloped projects in the world,” said Mark Bristow, chief executive of Barrick, which aims to start mining in 2028 subject to an ongoing feasibility study. “It’s a very big deal. Any copper mine right now is a big deal.” … Balochistan, Pakistan’s poorest province, borders Afghanistan and Iran and is suffering a brutal, simmering conflict with separatist militants motivated in part by alleged exploitation of the region’s mineral wealth. Tooze (2023) Copper crunch 7.2 IEA Net-zero 2050 Jeff St. John Net-zero carbon emissions by 2050 — but only if governments redouble their efforts, all fossil fuel investment is halted, and renewable energy capacity and and infrastructure are added at unprecedented scale Over the next decade, $5 trillion must be invested in converting energy used for electricity generation, transportation, industry and buildings from fossil fuels to carbon-free sources. It will entail a colossal undertaking that will require far greater commitments from government and industry than have been made thus far. But it could drive massive economic growth in rich and poor countries alike, the report finds. On the electricity front, solar power will need to reach 630 gigawatts and wind power 390 gigawatts by 2030, representing annual additions at four times the scale of the deployment record set in 2020. Annual investment in transmission and distribution grids to manage these renewable resources must triple from $260 billion today to $820 billion by decade’s end. On the transportation front, sales of new internal combustion cars must end and be replaced by electric vehicles or other carbon-free models by 2035. Public EV charging points must grow from around 1 million today to 40 million in 2030, at a cost of about $90 billion. Manufacturing capacity for batteries for this EV fleet will need to grow from 160 gigawatt-hours to 6,600 GWh by 2030, or the equivalent of nearly 20 gigafactories being added each year. Energy efficiency improvements must average 4 percent per year through decade’s end, roughly three times the rate achieved over the past two decades. And approximately $40 billion per year must be invested to bring electricity to about 785 million people and clean cooking solutions to about 2.6 billion people Jeff St. John IEA (2021) Net-Zero 2050 Study Hickel on IEA The new IEA report on net-zero is a big step in the right direction, and its call to cease all new fossil fuel projects has grabbed headlines, which is welcome. But the report also has some serious problems that are worth discussing: First, in order to maintain the assumption of economic growth-as-usual in rich countries, it relies on unprecedented rates of GDP/energy decoupling, to an extent that has been questioned repeatedly in the empirical literature. Second, it achieves this decoupling in large part by relying on efficiency improvements, but the model does not take adequate account of rebound effects, which have been identified as a significant problem. Third, it relies on a lot of BECCS and other carbon capture and storage approaches, which is a risky gamble and has several big downsides (in terms of land use, biodiversity loss, soil depletion, competition with food crops, energy and water use, etc.) If we dial down our assumptions about decoupling, bioenergy and CCS to safer and more feasible rates, the net-zero pathway will require a bigger reduction of energy and resource use in rich countries. This needs to be part of our discussion. For a review of evidence related to the three points above, and for a discussion of more technologically feasible approaches, this recent paper by (LorenzClimate?) and Manfred Lenzen is useful. Hickel (twitter thread) On IEA Keyzer &amp; Lensen 1.5 °C scenarios reported by the Intergovernmental Panel on Climate Change (IPCC) rely on combinations of controversial negative emissions and unprecedented technological change, while assuming continued growth in gross domestic product (GDP). Thus far, the integrated assessment modelling community and the IPCC have neglected to consider degrowth scenarios, where economic output declines due to stringent climate mitigation. Hence, their potential to avoid reliance on negative emissions and speculative rates of technological change remains unexplored. As a first step to address this gap, this paper compares 1.5 °C degrowth scenarios with IPCC archetype scenarios, using a simplified quantitative representation of the fuel-energy-emissions nexus. Here we find that the degrowth scenarios minimize many key risks for feasibility and sustainability compared to technology-driven pathways, such as the reliance on high energy-GDP decoupling, large-scale carbon dioxide removal and large-scale and high-speed renewable energy transformation. However, substantial challenges remain regarding political feasibility. Nevertheless, degrowth pathways should be thoroughly considered. The results indicate that degrowth pathways exhibit the lowest relative risks for feasibility and sustainability when compared with established IPCC SR1.5 pathways using our socio-technical risk indicators. In comparison, the higher the technological reli- ance of the assessed mitigation pathways, the higher the risks for socio-technical feasibility and sustainability. The reverse is likely the case for socio-political feasibility, which, however, is softer than socio-technical feasibility. This result contrasts strongly with the absolute primacy of technology-driven IAM scenarios in the IPCC SR1.5. (LorenzClimate?) 1.5 °C degrowth scenarios suggest the need for new mitigation pathways (pdf) Fickling One thing worth noting about the radical-sounding (iea?) announcement that no new petroleum fields need to be developed any more — this is more or less the lived reality of oil majors right now, and has been for years. Big Oil stopped investing growth capex around 2016. Fickling (twitter thread) 7.3 Transition Risks Unrealistic Scenario Pathways Keyser 1.5 °C scenarios reported by the Intergovernmental Panel on Climate Change (IPCC) rely on combinations of controversial negative emissions and unprecedented technological change, while assuming continued growth in gross domestic product (GDP). Thus far, the integrated assessment modelling community and the IPCC have neglected to consider degrowth sce- narios, where economic output declines due to stringent climate mitigation. Hence, their potential to avoid reliance on negative emissions and speculative rates of technological change remains unexplored. As a first step to address this gap, this paper compares 1.5 °C degrowth scenarios with IPCC archetype scenarios, using a simplified quantitative repre- sentation of the fuel-energy-emissions nexus. Here we find that the degrowth scenarios minimize many key risks for feasibility and sustainability compared to technology-driven pathways, such as the reliance on high energy-GDP decoupling, large-scale carbon dioxide removal and large-scale and high-speed renewable energy transformation. However, sub- stantial challenges remain regarding political feasibility. Nevertheless, degrowth pathways should be thoroughly considered. Figure: 1.5 °C scenario map under different levels of energy-GDP decoupling, RE speed and NETs. The dimensions are ‘speed of renewable energy transition’ (for the scenarios the 2020–2040 annual average growth in solar, wind and other renewables, in EJ/yr), ‘energy-GDP decoupling’ (for the scenarios the 2020–2040 average difference between GDP growth rate and final energy growth rate, in %) and cumulative CO 2 removal until 2100, including CCS (GtCO 2 ). Historical data points are the rolling averages of the past ten years (e.g., for the 1995 point the period 1986–1995) of the respective indicators. This averaging was chosen (1) because GDP and final energy data are noisy and (2) to emphasise longer-term trends. While historically four years were above a decoupling of 2% since 1986, these are outlieres around a lower, almost constant trend. Historical GDP data (MER, constant 2010 US$) is taken from the World Bank. Keyser (2021) 1.5 °C degrowth scenarios suggest the need for new mitigation pathways (pdf) Unrealistic Assumptions Existing plans to limit global warming rely too much on “increasingly unrealistic assumptions” that societies will be able to remove huge amounts of carbon from the atmosphere while simultaneously maintaining incessant economic growth over the next 50 years, according to a May 2021 study in Nature Communications. These strategies appear to be speeding the planet deeper into the climate crisis. Economic degrowth—strategies to shrink the economies of rich, developed countries while maintaining the wellbeing of the people and environments they are based on—might be less risky, and a better way to meet the goals of the Paris climate agreement. Efforts to slow climate change that are built on structural social changes, like rethinking the way we work, produce food, heat our homes and move around could be more successful than those that rely on uncertain carbon removal technologies. The over-reliance on unprecedented carbon dioxide removal and energy efficiency gains means we risk catastrophic climate change if one of the assumptions does not materialize. InsideClimateNews 7.4 Transition Forecasting Way Summary Rapidly decarbonizing the global energy system is critical for address- ing climate change, but concerns about costs have been a barrier to im- plementation. Most energy-economy models have historically underes- timated deployment rates for renewable energy technologies and overestimated their costs. These issues have driven calls for alternative approaches and more reliable technology forecasting methods. Here, we use an approach based on probabilistic cost forecasting methods that have been statistically validated by backtesting on more than 50 technologies. We generate probabilistic cost forecasts for solar energy, wind energy, batteries, and electrolyzers, conditional on deployment. We use these methods to estimate future energy system costs and explore how technology cost uncertainty propagates through to sys- tem costs in three different scenarios. Compared to continuing with a fossil fuel-based system, a rapid green energy transition will likely result in overall net savings of many trillions of dollars—even without account- ing for climate damages or co-benefits of climate policy. Fig: Decisions about how and when to decarbonize the global energy system are highly influenced by estimates of the likely cost. Here, we generate empirically validated probabilistic forecasts of energy technology costs and use these to estimate future energy system costs under three scenarios. Compared to continuing with a fossil fuel-based system, a rapid green energy transition is likely to result in trillions of net savings, even without accounting for climate damages or climate policy co- benefits. Way (2022) Empirically grounded technology forecasts and the energy transition (pdf) (pdf SI Volts (2022) Learning curves will lead to extremely cheap clean energy "],["batteries.html", "8 Batteries 8.1 Lithium 8.2 Lithium-ion Batteries (LIBs) 8.3 Nickel-Hydrogen 8.4 Iron-Air 8.5 Flow Batteries 8.6 Zinc 8.7 Sodium-ion 8.8 Sodium-Sulphur 8.9 Liquid-Metal 8.10 QuantumScape 8.11 CO2 8.12 Graphene", " 8 Batteries The battery market has seen dozens of chemistries come and go, but four have stuck and scaled to achieve mass-market penetration: lead acid, nickel-cadmium (Ni-Cd), nickel-metal hydride (NiMH) and lithium-ion (Li-ion). Most of the developing world still uses lead-acid batteries, a $45 billion global market. But lithium-ion batteries have been gaining ground rapidly in wealthy markets. LIBs have hit on a combination of anode, cathode and electrolyte that performs well enough along several criteria (especially cost) to work for most short-duration applications today. They have become cheap, and manufacturing capacity has converged around them. Roberts 8.1 Lithium 8.1.1 Portugal’s Lithium Reserves According to the European commission, Europe will need 60 times more lithium by 2050 (target year for carbon neutrality) for electric cars and energy storage alone, which is fueling an international race to extract lithium from the different sources where such deposits can be found, such as hard rocks, salt brines and geothermal water. But for the people of Covas do Barroso, this scramble for raw materials and the prospect of an open-air mine translate into fears of deforestation, air pollution, water contamination, noise and an end to their way of life. Interviews conducted by Euronews with a dozen residents, revealed that the vast majority of them were against mining lithium from the mountain rocks near their village, while a few were indifferent. No one was in favour. “I think it won’t bring anything good,” said Paulo Pires, a local shepherd. “It will consume a lot of water, which we need for the sheep and for their fields. Instead of hearing birds, I will hear explosions, machines…there will be a lot of pollution.” “I’m not against lithium. But I’m not in favour of polluting my village and other villages like mine in order to depollute cities” Pires added. A study by the Portuguese University of Minho, conducted for Savannah Resources, found that Portugal’s 60,000 tons of known lithium reserves (0,4% of world’s reserves) are “insufficient to meet the demand for lithium derivatives for the production of batteries in Europe”. However, the report also adds that these reserves “are very relevant in reducing Europe’s dependence on other regions of the globe and increasing the security of Europe’s supply chain.” Euronews 8.2 Lithium-ion Batteries (LIBs) Roberts The global market for EV batteries alone is expected to hit almost $1 trillion by 2030. The more energy-dense, cheap and safe LIBs can get, the faster the electrification of transportation will happen. LIBs are being used both for distributed, building-level energy storage and for large, grid-scale storage installations. The global storage market is expected to grow at an average of 31 percent a year over the coming decade, reaching 741 gigawatt-hours of cumulative capacity by 2030. The vast bulk of the demand for batteries is going to come from transportation In 2019, the three chemists behind the initial development of lithium-ion technology won the Nobel Prize in chemistry. LIBs boast incredibly high energy density and specific energy, which is to say, they cram lots of oomph into a small, lightweight package, and they are capable of cycling many more times than their predecessors. Lithium-ion battery pack prices, which were above $1,100 per kilowatt-hour in 2010, have fallen 89% in real terms to $137/kWh in 2020. By 2023, average prices will be close to $100/kWh. With foreseeable improvements in LIB chemistry, prices could hit $40 or even $30/kWh in coming decades. LIBs are going to hit limits, even if it’s just the base price of raw materials, before they become economical for long-duration grid storage. They are being installed for 4- to 6-hour storage applications, sometimes 8 hours, and someday may even aspire to 12 hours. But beyond that — for the weekly or even seasonal storage a renewables-based grid will need — some other technology or technologies will have to step in. Before Tesla was founded, Li-ion batteries were almost exclusively used in consumer electronics — mainly laptops and cell phones. At the time of the launch of the Tesla Roadster in 2008, the total global Li-ion manufacturing capacity was approximately 20 GWh per year. By 2030, we expect over 2,000 GWh of annual production capacity based on already announced plans by cell manufacturers. That would be 100× growth in 22 years. LIBs do face restraining pressures, especially materials and safety concerns. Roberts LIBs have been around in commercial form since the early 1990s, though obviously they’ve improved quite a bit since then. Today’s most common and popular LIBs use graphite (carbon) as the anode, a lithium compound as the cathode and some organic goo as an electrolyte. They boast two key advantages over prior battery chemistries. First, they need very little electrolyte. They are what’s known as “intercalation” batteries, which means the same lithium ions nestled (intercalated) in the structure of the anode transfer to be intercalated in the cathode during discharge. The electrolyte only has to serve as a conduit; it doesn’t have to store many ions. Consequently, the cell doesn’t need much of it. Saving on electrolyte saves space and weight. (Bonus: The process is almost perfectly reversible, which gives LIBs their high cycle life.) Second, LIBs squeeze lots of energy into a small space. Lithium is the lightest metal (at the upper left corner of the periodic table) and extremely energy-dense, so LIB cells can work with electrodes only 0.1 millimeters thick. (Compare lead-acid electrodes, which are several millimeters thick.) This also makes LIBs smaller and lighter. Even the biggest grid battery is just stacks upon stacks of cells, like Lego bricks. LIBs are extremely modular — they can be scaled precisely to need. The most common LIB chemistries used today are lithium nickel manganese cobalt oxide (NMC) and lithium nickel cobalt aluminum (NCA), which use compounds of those metals as the cathode. Lithium and nickel turn out to be a knockout combo: light and energy-dense. Some batteries, particularly those with cobalt, are prone to “thermal runaway,” which means that if one cell goes haywire and heats up, it heats up the next one, and so on and so on in a self-reinforcing process that results in fires List of LIB chemistries: Lithium nickel manganese cobalt oxide (NMC cathode) Lithium nickel cobalt aluminum (NCA cathode) Lithium ferro phosphate or lithium iron phosphate (LFP cathode) Lithium manganese oxide (LMO cathode) and lithium manganese nickel oxide (LMNO cathode) Lithium sulfur (Li-S, sulfur cathode) Lithium metal (anode) and solid state Lithium titanate (LTO anode) Lithium air (Li-air, lithium anode) Cobalt, used in standard NMC and NCA chemistries, is highly toxic, comes almost entirely from the Democratic Republic of the Congo, and is mined in terrible working conditions that frequently spur charges of human rights abuses. Nickel and lithium are less nasty in and of themselves, but they may run into supply constraints as the market grows. (Nickel, in particular, is a source of current stress.) Smart manufacturers such as Tesla and others are diversifying their battery lines in anticipation of supply issues, trying to evolve away from cobalt and attempting to secure a steady supply of lithium and nickel. Roberts Sionic Sionic is betting that its approach will prove to be easier and cheaper to adopt for the companies that have already sunk massive investment into their factories. Instead of re-engineering silicon to tamp down on its propensity to expand and contract during cycling, Sionic uses low-cost micron silicon and designs around that property. “If you fight physics, physics always wins,” said Sionic CTO Surya Moganty. “We are controlling, or engineering, that expansion of silicon so it does not cause problems.” Sionic also leans on its corporate pedigree to design an electrolyte that’s optimized for safe performance with the new anode. That’s important, because changing one key component of a battery typically affects how the other pieces perform. Spector/Canary Lition-ion Varieties There are a few clear leaders — lithium nickel manganese cobalt oxide (NMC), lithium nickel cobalt aluminum (NCA), and lithium ferro phosphate (LFP). Most EV makers use NMC batteries; Tesla uses NCA. In the past, it’s been difficult to push down the amount of cobalt in these batteries (it plays an important balancing role), but manufacturer LG recently introduced an NMC 811 battery: 80 percent nickel, 10 percent manganese, 10 percent cobalt. GM will use them in its new line, including in the Hummer, and Tesla will put them in some of its Model 3s in China. Most big battery manufacturers, including Panasonic (which supplies many of Tesla’s batteries), have vowed to gradually reduce and eventually eliminate cobalt. Nickel is the key to energy density. Tesla, VW, and others are working on special high-nickel battery varieties that will be used for specialty vehicles that require extra-high energy density, like larger SUVs and trucks. But not every vehicle needs that, and nickel supply constraints are looming, so work is also being done to further boost manganese — a much more stable, abundant material — and reduce cobalt. Silicon Anodes Many LIB developers are experimenting with silicon as an anode coating, partially or completely replacing graphite. Silicon holds on to nine times more lithium ions than graphite, so energy density improves (range expands by 20 percent), and a silicon battery can charge and discharge much more quickly than graphite batteries, so power density improves as well. But silicon expands when it absorbs ions, so it breaks down quickly; cycle life is still much lower than graphite. Automotive cells with NCA or NCM cathodes paired with Si-dominant anodes will increase energy density by up to 50%, thereby dropping the $/kWh cost by 30-40% in less than a decade. Silicon-as-anode doesn’t operate via intercalation. Instead of nestling into the anode, ions react with the silicon and bond with it, a process called “conversion.” That makes it more difficult to peel the ions off without damage, but it can hold way more ions. Fluorides Cathodes Metal fluoride-based cathodes (like iron fluoride or copper fluoride) and sulfur-based cathodes — which also operate via conversion rather than intercalation and can also store more ions. It’s plausible that with a conversion cathode and an engineered low-swell silicon anode, the cycle life of Li-ion can be extended all the way to 10,000 full cycles while also having the highest energy density. Only that combination — a conversion-based anode and a conversion-based cathode — that can bring LIB prices down to “~$50/kWh by 2030 and ~$30/kWh by 2040. Lithium ferro phosphate (LFP) LFP use a lithium-iron compound as cathode. A few years ago, it looked like LFPs were going to be displaced by NMCs and NCAs, but lately they’ve made a comeback and now have a decent case that they could take the lead in the EV and stationary storage markets. They have already captured almost half of the Chinese EV market. LFPs use lithium ferrophosphate (LiFePO4) as the cathode, replacing nickel, manganese, and/or aluminum. The advantages relative to nickel-based competitors: cheaper on a materials basis (though not yet on \\$/kWh); higher cycle life (Matt Roberts, previously executive director of the Energy Storage Association, now working at battery company Simpliphi, says his company’s LFP batteries are warrantied for 10,000 cycles, compared to 2,500 to 5,000 for cobalt batteries.); higher power density; high safety and low toxicity (“They&#39;re almost literally bulletproof, in that they can&#39;t catch fire,” says Schick.); replaces problematic and/or rare metals with iron, which is safe and abundant. In exchange for these advantages, LFPs offer lower energy density (there are fewer spaces for ions to intercalate). However, because they are so safe, LFPs do not require the same protective packaging as NMCs and NCAs, so they can gain some of that efficiency back at the pack level. Tesla says that, while LFPs have 50 percent of the energy density of their high-nickel competitors, an LFP-based vehicle can still get 75 percent of the range. Current LFPs are not going to feature in high-performance vehicles, but most vehicles aren’t that. They are “good enough, essentially, for any kind of commuter car,” Schick says. “I think you’re going to see a whole bunch of economy cars that are LFP.” LFP will be used in taxis, ride-share vehicles, and fleet vehicles, along with scooters and rickshaws and motorcycles. It will be the cheap, reliable, everyday option. LFP in energy storage markets Energy density is also less important in the energy-storage market, where price, capacity, and safety rule. LFP’s high cycle life and low costs make them attractive in the grid-storage market As for distributed, behind-the-meter storage, in some markets like California and New York City, Tesla home batteries (still NMC) are not allowed inside garages, thanks to the risk of thermal runaway, which can lead to fires. LFPs have passed an extensive regimen of safety tests and will be available everywhere; that gives them a tangible market advantage. With sufficient manufacturing scale, the price of any battery approaches the price of its materials, and LFP uses incredibly cheap materials. Of all the lithium-ion chemistries, LFP may play the largest role in accelerating the world’s transition to sustainable energy. Lithium manganese oxide (LMO) and lithium manganese nickel oxide (LMNO) Manganese is abundant, safe, and stable at a wide variety of temperatures, though its energy density is lower than cobalt or nickel. Because LMOs don’t contain cobalt and avoid the threat of thermal runaway, they are used in medical equipment, as well as power tools, electric bikes, and EVs. Lithium sulphur (Li-S) Li-S burst on the scene to some excitement in the late ’00s, demonstrating that a cell with lithium as the anode and sulfur as the cathode — two elements with extremely low atomic weight — could double the specific energy of conventional LIBs. Plus sulfur is incredibly cheap. One problem is that sulfur has very low conductivity, so something (usually carbon) has to be added to pull in the ions. More importantly, Li-S batteries degrade quite quickly and have low cycle life. To date, they remain commercially unavailable. Williamson A new lithium-sulphur battery design from Australian researchers could slash the quantity of lithium needed in a battery by half, and eliminate one of the key flaws in the battery chemistry. Researchers at Monash University say they made major advances in the development of lithium-sulphur batteries, which are emerging as a popular chemistry because they can store at least double the amount of energy per unit weight (Wh/kg) compared to lithium ion batteries. Sulphur is also very common, easy to obtain, and cheap, compared to lithium which also comes with political issues around its provenance. This battery chemistry doesn’t need any nickel or cobalt either. But early lithium-sulphur batteries didn’t perform well because sulphur from the positively-charged cathode would dissolve into the electrolyte and attack the lithium anode. This rapid decay quickly reduced the number of times a battery could charge and discharge. The solution is to protect the anode with a barrier – an idea that has also been divined by researchers at the Argonne National Laboratory in the US, who created a layer between the cathode and anode to prevent reactive sulphur species from reaching the negative electrode. The researchers at Monash say they have coated a lithium foil anode with a nanoporous polymer, which they say allows lithium ions to move through while protecting the anode from corrosive sulphur compounds. Lithium-sulphur batteries replace expensive metals such as cobalt and nickel in their cathode with widely available waste product sulphur, which reduces the cost massively. In this discovery, we’ve further lowered the amount of lithium needed, by half, which helps even more in lowering the cost. We were able to cycle this battery around 300 times with quite stable performance. Because lithium-sulphur batteries store twice the energy, fewer cycles are also needed to keep up with their counterparts. We have gotten to over a thousand cycles now in some lithium-sulphur battery examples. The thin polymer coating contains tiny holes less than a nanometre in size which allow lithium ions to move freely while acting as a scaffold for lithium to intercalate into the anode (when ions are inserted into the host – anode – matrix). Metallic lithium is a bit of a double-edged sword. Lithium is packed full of energy, but in a bad battery, this energy is wasted on side reactions. On the other hand, if the energy is channelled correctly, it can make some incredible energy storage devices that are easier to make. Williamson (2023) New design could make lower cost, more efficient lithium-sulphur batteries a reality McNamara Abstract Lithium metal batteries, in particular lithium–sulfur chemistries, hold great promise in energy storage from potentially increased gravimetric storage density and diminished reliance on transition metals, lowering resource demand and hence overall unit cost. However, these cells can have their feasibility improved to a greater extent by lowering the demand for lithium within their construction and reducing the polysulfide shuttling effect. Rising lithium costs and a lack of recycling options indicate the use of excess lithium to mitigate cycling stability issues is sub-optimal. Herein, the direct casting of a lithophilic, superglassy, nanoporous PTMSP polymer separator directly onto the lithium anode is described. PTMSP’s bi-modal, sub-angstrom pore size distribution results in selective rejection of polysulfide species, while its high fractional free volume acts as a stabilizing matrix for deposited lithium as well as possessing a high ionic conductivity of 8.8 × 10-4 S cm−1. The coated anodes exhibit 5.7 times more dense lithium over controls, translating to improved cycling performance due to increased capacity retention and improved lithium utilization at low (&lt; 3) N/P ratios for extended cycle life (&gt; 250 cycles), at practical sulfur loadings (4 mg cm−2). These developments are promising steps for more widespread adoption of lithium sulfur batteries and other metallic lithium systems. McNamara (2023) A Nanoporous Permselective Polymer Coating for Practical Low N/P Ratio Lithium Metal Batteries Lithium metal anodes Solid lithium metal makes for a great anode, in that it is highly prone to releasing electrons and ions. The problem is that lithium is highly reactive and ions tend to form “dendrites,” or tree-like formations, that reduce energy density and cycle life and increase the chances of a short or fire. It was problems with lithium’s reactivity that originally led to the addition of graphite to the anode, so the ions could intercalate rather than plating. Solid Electrolytes (solid-state) The liquid electrolytes used in most LIBs limit the kinds of electrodes that can be used and the shape of the battery cell; plus, they are often flammable, a safety hazard. “While there are technical reasons why this technology appears to be the holy grail of batteries,” writes SILA Nanotechnologies, “the reality is that even if the technology works (and that is a big ‘if’ after 40 years of development) it is unlikely to find more than niche opportunities in the market.” Lithium titanium oxide (LTO) LTO batteries have lithium-titanate nanocrystals coating the anode, which increases surface area and allows for many more electrons to be released much faster than graphite. Consequently, they have incredibly high power density (they can release energy quickly) and can recharge faster than any other LIB. They also have high cycle life and high recharging efficiency. They are lower voltage than conventional LIBs and thus have lower energy density, but because of this they are also extremely safe to operate. Amazing performance - Crazy price. Lithium-air (Li-air) Li-air, which uses lithium metal as the anode, a variety of materials as the electrolyte (that’s where research is most intensive), and as the cathode … air. Li-air has incredibly high specific energy (energy per unit of weight), theoretically as high as the specific energy of gasoline. In practice, only a fraction of that potential has been demonstrated, but even that fraction is about five times the specific energy of conventional LIBs. All sorts of improvements in electrolytes, cycle life, and scalability will be needed before Li-air will become practical, but in terms of 2030 dark horses, this is one to watch. 8.3 Nickel-Hydrogen Spector EnerVenue isn’t banking on the latter. It’s trying to compete with lithium iron phosphate (LFP), the lithium-ion chemistry that’s ascendant for stationary storage for its good value and fire safety relative to other chemistries. “We can do everything [LFP] can do in the stationary [storage] world, and then some,” Heinemann said. Nickel hydrogen can charge and discharge quickly or slowly, multiple times a day, at any temperature, he added. The physical cause of thermal runaway, which is the culprit behind lithium-ion fires, simply doesn’t exist in the tank structure of nickel-hydrogen batteries; that means EnerVenue doesn’t need any fire-suppression or HVAC equipment, which adds cost and eats up energy. That hardiness shows up in cycle life, the metric for how many times a battery can charge and discharge before it wears out. LFP batteries lose some capacity over a few thousand cycles; typical warranties impose limits on how the batteries can operate in order to prolong their life. Grid storage developers typically plan on periodic augmentation with new lithium-ion cells to keep storage plants operating at full capacity, but that undercuts project economics. EnerVenue warranties 20,000 cycles over 20 years with effectively no restrictions on how to operate. But the product is designed to last 30,000 cycles, Heinemann said; some of the nickel-hydrogen units up in space ran for 100,000 cycles. “Our technology is much more forgiving,” Heinemann said. ​“It changes the mindset from ​‘A battery is a consumable that has to be babied’ to ​‘It lasts as long as the solar panels or wind turbines it’s paired with, maybe longer.’” As for round-trip efficiency — how much of the power that goes in is able to be withdrawn and used — EnerVenue says it hits 85% to 90%. That’s comparable to lithium ion, and better than some batteries. That’s a metric where non-lithium technologies often suffer. Success will require managing risks of nickel scarcity or price spikes. Last year, for instance, nickel commodity prices surged 250% in a couple days, forcing a halt to trading. Spector (2023) https://www.canarymedia.com/articles/batteries/this-nasa-tech-might-just-spur-a-major-grid-battery-breakthrough 8.4 Iron-Air Form Energy If Form’s battery works like it’s supposed to, it will store renewable energy so cheaply that a power plant can deliver emissions-free energy around the clock for days on end. That could create a viable alternative to fossil-fueled plants for ensuring a 24/7 supply of reliable electricity as the grid decarbonizes. A novel design based on rusting and de-rusting iron is not an obvious darling for the titans of climate finance. But Form is graduating out of the venture-capital space and drawing growth equity from some massive institutions. The startup modeled its reliability testing on processes the automotive industry uses to ensure vehicles and all their components are safe enough to put on the road. The first field deployment — a 1-megawatt/150-megawatt-hour pilot for Minnesota utility Great River Energy — was scheduled for late 2023. Site preparation is underway, Jaramillo said, but completion is now expected in 2024. Form is also working with utility Georgia Power on installing iron-air batteries, but that project is still navigating the regulatory process. Canary Media (2022) Form Energy wins $450M to rust iron for multiday energy storage AZO Materials The power in an iron-air battery comes from the interaction of iron with oxygen. The steel oxidizes nearly exactly as it would during its corrosion phase within that procedure. The oxygen necessary for the reaction may be taken from the ambient air, eliminating the requirement for the cell to store it. The high energetic densities with 1,200 Wh/kg produced by metal-air batteries are attributed to these component savings. Compared with the usual lithium-ion that has 600 Wh/kg, iron-oxygen batteries save more energy. Iron-air batteries are relevant in this context. Because both ferrous and sodium - the building blocks of alkali solutions - are highly abundant, they have a high potential for growth. Simultaneously, the ferrous electrodes are extremely durable, capable of withstanding over 10,000 full cycles. This equates to a life span of around 30 years. Iron-oxygen batteries are also resilient to overcharging, overcurrent, and partial discharge. A rechargeable iron-oxygen battery is able to supply 100 hours of energy at operating cost compared to traditional power stations and less than a tenth of the price of lithium-ion batteries. Due to their exceptional energy density, evident environmental acceptability and extraordinary reversibility as opposed to other metal-air batteries, iron-air batteries have re-gained substantial research interest. To fully leverage the energy density of steel to the best capability, the anode-to-overall cell material ratios should be as big as feasible, aiming for prospective iron-oxygen cell performance that is practically viable. A steadily growing quantity of electrochemical oxidation carbonyl metal particles may serve as an alternative source of activated metallic surface for a sharply increased discharge rate during creation, which is especially important for thick electrodes instead of thin electrodes. Additionally, microstructural alterations in the conductor are produced by hydrogen evolution throughout the initial formation. This varies by the state of the creation of porosity carbonyl ferrous-anodes; the mechanism implies the existence of an active layer on the exterior, and inactive because quasi on the interior of porous carbonyl anodes. Because of its low price, easiness of oxidation, numerous oxidation states, and its capability to be cathodic electrodeposited from an electrolyte solution, ferrous is an appealing element for a battery. The iron-air battery can be thought of as a replacement for iron-nickel with alkaline cells. The major benefit is that no iron dendrites develop throughout the charging mode. Either way, significant hydrogen change occurs. Additional issues raised by NASA, who conducted the first research of iron-air batteries, include self-discharge, the possibility of damaging iron oxidation processes, and water loss. A laminated iron electrode with a rectangular area of 100 cm2 was found to have long-term performances and adequate features. Researchers estimated that the batteries could be produced in a 400 cm2 electrode area module with a specific capacity of 140 W h kg-1 capable of 1000 cycles of US$30 (kg h)-1. Due to recent advancements in nanomaterials and the potential of utilizing efficient nanostructured electrode catalysts to get a better energy density via greater surface size Fe nanoparticles, the iron-air battery technology has advanced in recent years. Additional reasons include the low availability and price of iron, as well as the abundance of oxygen in the atmosphere. In a planar parallel arrangement, ferrous–oxygen batteries typically have two air-breathing electrodes with one metal electrode inside. During the cycles of charge-discharge, the oxygen ions in the regenerative iron-air cell are intended to achieve both advancement and elimination of oxygen. One of the obstacles to the implementation of effective metal-air cells is the lack of a counter electrode capable of withstanding large positive potentials throughout oxygen advancement. Iron-air batteries have differences to other metal-air batteries because the oxidation reaction within iron-air batteries requires solid-phase evolutions. The ferrous electrode does not quite form dendrites all through plating, but the structure and quantity of the electrode could adjust even during charging and discharging processes due to the incorporation of the voluminous, non-soluble materials Fe(OH)2 and Fe3O4 that have higher viscosity from Fe. Components of the electrode might become inactive as a result of an unbalanced voltage and current transmission during the dissolution and plating cycles. The best way to improve iron-air battery utilization is to create more parts of solid electrode available to the electrolytes by increasing the surface of the electrode with nano-sized ferrous particles. Some recent research has looked at nanomaterial iron electrodes and found that they had higher charge capacities than typical Fe3O4 powdered electrodes. The electrodes are all well made to make the greatest use of nanoparticulate iron as the active components and bifunctional oxygen catalyst. To get the best result for the nanoparticle iron material, the iron’s surface area must be in contact with the electrolyte with the biggest area possible. AZO Materials (2022) Iron-Air Batteries: A Breakthrough in Green Energy Roberts 8.5 Flow Batteries Flow batteries circulate a liquid electrolyte through stacks of electrochemical cells and have long held the promise of 10-hour durations, tens of thousands of cycles, minimal degradation and no limitations on depth of discharge. This performance promise has lured venture capital investment and research and development — but so far, the investments have yielded few commercial, competitive flow battery products. Volts Flow batteries operate on a fundamentally different principle than the batteries we’ve looked at so far. Rather than storing energy in metals on the electrodes, energy is stored as a dissolved metal in an aqueous electrolyte. The anolyte is stored in one tank; the catholyte is stored in another; pumps circulate the fluids past electrodes (sometimes in a fuel cell), where they don’t quite mix, thanks to a thin separator, but they exchange ions and electrons, generating electricity. The key conceptual difference is that flow batteries separate energy (the amount stored) from power (the rate at which it can be released). If you want more power, you make the electrodes bigger. If you want to store more energy, you make the tanks of electrolytes bigger. And electrolytes are fairly cheap, so it’s cheap to increase capacity. This is in contrast to LIBs, which double in cost with each doubling of energy capacity. In theory, flow batteries can scale up to almost any size, relatively cheaply. So as the demands for storage get bigger — six hours, eight hours, 12 hours — the economics of flow batteries look better and better relative to LIBs. A variety of different metals can be used in the electrolyte. For a long while, vanadium was expected to be the breakout candidate, but materials costs remain stubbornly high. Companies have tried with zinc (like the late ViZn, and also see below) and iron (like ESS, which is still going strong). Recent history is littered with failed flow battery companies. Flow batteries have been the next big thing for a really long time. The problem, as ever, is the steady march of LIBs down the cost curve. For a three-or four-hour system, a lithium ion battery outperforms any flow battery now. Flow batteries can theoretically expand their energy capacity indefinitely, for little more than the cost of the electrolyte goop to fill the tanks (though pumps and other accoutrement add to the cost a bit). When we’re below $100 per kilowatt-hour on the cost of [LIBs] we are really close to the cost of the goop. Flow batteries aren’t going to be able to catch up to LIBs, at least not any time soon. Volts CanaryMedia Instead of storing clean electricity in lithium-ion batteries, ESS makes a “flow battery” that moves electrons with a liquid mixture of iron and salt. ESS stands to capitalize on a few trends in the clean energy market: Ten-year-old ESS announced in May that it would go public by merging with a special-purpose acquisition company (SPAC), a recently popular alternative to a traditional initial public offering. ESS will be traded on the New York Stock Exchange with a valuation north of $1 billion and the ticker symbol GWH. It already has a factory pumping out flow batteries just south of Portland, Oregon. The company built out the factory with a $30 million Series C raise in 2019, led by SoftBank’s SB Energy and Bill Gates–backed Breakthrough Energy Ventures. the factory will be able to produce more than 1.5 gigawatt-hours of storage capacity annually. Technicians take the pieces from the robots, compile them and load them into cargo boxes docked along the wall of the factory. After hooking up pipes and electronics and loading the tank with the dry powdered version of the iron liquid, a truck carries away the finished Energy Warehouse. Leveraging cheaper materials than its competitors. Conventional batteries become overly pricey when scaled up for many hours of storage; all ESS needs to do is add more rusty water to the tank, which makes the incremental cost of additional storage much smaller. ESS is it’s a relatively simple technology when it comes to flow batteries.” In contrast, vanadium flow batteries, a perennial contender in the lithium-ion challenger category, suffer when vanadium prices skyrocket, as they have in recent years. Flow-battery technologies that utilize low-cost redox species such as iron will have a cost advantage over vanadium. One known challenge with iron flow batteries is hydrogen generation at the negative electrode that eventually leads to cell failure. A commercial iron flow battery will either prevent the generation or mitigate it somehow. ESS dealt with that by inventing what it calls a Proton Pump, which connects to the tank and passively mixes the unwanted hydrogen back into the solution. That maintains the balance of pH and states of charge in the liquid electrolyte circulating through the system. That invention is a key commercial differentiator for ESS. Other flow chemistries need to cease operations regularly to rebalance their electrolytes. Another general hangup for flow batteries is the cost of membranes or separators that allow electrons to pass but keep other materials from mixing. But ESS uses off-the-shelf separators from the battery industry. Then there’s the worry that a system that pumps liquid for decades eventually will leak. In the latest product iteration, ESS reduced the number of pipe connections per Energy Warehouse from 170 to 36. Instead of relying on threaded pipe connections that need to be tightened just right, the new pipes get “thermally fused,” which means they are melted together. And for winning over customers to the technology itself, ESS points to an unusual program hammered out with German reinsurance giant Munich Re. That firm studied the technology and decided to offer a performance warranty, which covers costs if the technology doesn’t perform as promised. “Munich Re helps address the technology risk for the first few score buyers. CanaryMedia Spector CMBlu What is CMBlu doing differently to break through the flow-battery malaise and make a technology that really helps decarbonize the electric grid? The company stands apart from the flow-battery pack by augmenting liquid electrolytes with solid components to squeeze more energy storage into its physical footprint. And by choosing organic components, CMBlu avoids having to use the often-controversial minerals that mainstream clean-energy technologies rely on. “We’re transitioning from a fossil fuel economy to a renewables and metals economy — there’s so much metal” needed to manufacture wind, solar and conventional batteries, Kaun said. In contrast, ​“What we have here at CMBlu is a rechargeable polymer, a rechargeable plastic. We can make the plastic [and] recycle the plastic.” CMBlu fills its storage tanks with a solid polymer that holds a charge, and then transfers it to and from the liquid electrolyte, which is pumped into an electrode stack for charging and discharging cycles. Kaun likened that system to how the human body stores energy in fat, and then transfers that energy to blood sugar when it needs to be delivered to cells. This mimicking of human anatomy could give CMBlu an edge in the metric of energy density, or how much storage capacity can be packed into the physical footprint of the product. Avoiding the use of metals distinguishes CMBlu’s technology from that of conventional lithium-ion batteries or certain chemistries that are popular in the flow-battery field, like vanadium redox. The biggest flow battery in the world is reportedly a 100-megawatt/400-megawatt-hour vanadium redox flow system in Dalian, China. Other major flow-battery projects include ESS’ multiyear contract to install 2 gigawatt-hours of iron flow batteries in Sacramento to help the municipal utility reach zero carbon by 2030. Invinity, formed by the merger of two flow-battery startups, is selling a new-generation product and recently clinched Department of Energy grants to support 84 megawatt-hours installed across several sites in the U.S. In other words, the flow-battery market is still in its trial period, and no clear winners have swept the field yet. Spector (2023) Will this startup finally crack the code on flow battery tech? 8.6 Zinc Batteries that exchange zinc ions instead of lithium ions — it’s the second-most-popular metal for batteries. Zinc has the particular advantage of being light and energy dense like lithium, so with relatively modest adjustments, it can slipstream into the lithium-ion manufacturing process. Zinc is plentiful, cheaper than lithium, largely benign, and makes batteries that are easier to recycle. Like other lithium alternatives, zinc sacrifices energy density, but makes some of it back up in savings on safety systems at the battery-pack level, thanks to the lack of any need for fire suppression. This puts it in the same markets as LFP: smaller commuter/city vehicles, robo-taxies, scooters, e-bikes — and energy storage. Some in the zinc crew have larger designs: “We think we can coexist with lithium-ion and replace lead acid,” says Michael Burz, president and CEO of EnZinc, which has developed a new zinc anode it says can come close to LIBs on energy density. Remember, lead-acid batteries are still ubiquitous. “Forklifts use them. Airplanes. Snowmobiles.“ says Burz. “Data centers have huge banks of lead-acid batteries they use for switchover power.” It’s still a $45 billion global market. EnZinc thinks it can hit a sweet spot: close to the energy density of LIBs, close to the low cost of lead-acid, safer than either, and good enough to substitute for a big chunk of both. Zinc anodes are “cathode agnostic,” so Burz envisions, rather than becoming a battery manufacturer, becoming an anode supplier — “Zinc Inside,” modeled on “Intel Inside” processors. Research is underway on a number of cathodes, from manganese and nickel to, just as with lithium, air. A zinc-air battery “has a system-level specific energy of anywhere between 250 to 350 watt-hours per kilogram,” says Burz, well above most LIBs. The trick is making it controllable and rechargeable. Most of these batteries make the same basic claims: they are less energy dense than LIBs, but they are safer (no fires), they are made with benign and plentiful materials (no supply problems), and they are cheaper at high capacities/durations. It’s just that last part that’s tricky, since the price and capabilities of LIBs are a moving target. 8.7 Sodium-ion Lithium, nickel, and cobalt all have their issues. You know what material doesn’t? Salt. Sodium compounds can be substituted for lithium compounds to create sodium-ion batteries (NIBs), which have been the source of considerable hype for at least five years now. The basic idea and manufacturing process is the same for NIBs as LIBs — “you could use existing gigafactory structures to produce a sodium-ion battery,” says Steingart — but unlike the latter, the former can’t use graphite for the anode, because it can’t capture enough of the relatively bigger sodium ions, so something called “hard carbon” is typically used instead. Research is underway to find more energy-dense sodium compounds for the cathode and cheaper materials for the anode. “Sodium-ion has a lower energy density than lithium-ion,” says Tim Gretjak, an innovation analyst with Con Edison, “so all the materials that go into it have to be correspondingly that much cheaper.” Volts 8.8 Sodium-Sulphur Hill Sodium-sulphur batteries are not new, having been around for several decades, but they have been a significantly inferior alternative to traditional lithium-ion batteries. A research team led by Dr Shenlong Zhao from the University of Sydney’s School of Chemical and Biomolecular Engineering has developed a new type of sodium-sulphur battery. Using a simple pyrolysis process and carbon-based electrodes, which help to improve the reactivity of sulphur and the reversibility of reactions between sulphur and sodium, the researchers claim their new battery offers four times the energy capacity of lithium-ion batteries and is far cheaper to produce. The eponymous key ingredient in the sodium-sulphur (Na-S) battery is a type of molten salt that can be processed from sea water, which Earth has quite a lot of. As such, the main battery ingredient costs much less to produce than lithium-ion batteries. According to the University of Sydney researchers, the Na-S battery is more energy dense and less toxic, making it an even more attractive alternative to lithium-ion batteries which, though ubiquitous in electronic devices and for energy storage systems, are nevertheless expensive to manufacture and recycle. The Na-S battery developed by Dr Shenlong Zhao’s team has been specifically designed to provide a high-performance solution for large-scale renewable energy storage systems. The researchers have developed laboratory-scale batteries, which have also been successfully fabricated and tested in the University of Sydney chemical engineering facility. Dr Zhao and his team of researchers will now move on to improve and commercialise the fabricated Ah-level pouch cells. Hill (2022) Sydney Uni hails low cost, long life sodium-sulphur battery with super-high capacity 8.9 Liquid-Metal The battery will pass no current at room temperature, but on site, the contents of the boxes are super-heated (to 500°C), which activates the materials; the metals alloy and de-alloy, with the cathode being entirely consumed and then reformed, as the batteries charge and discharge. Because the contents are liquids, the battery has no “memory” — it is not affected or degraded by absorbing or releasing ions. This means it suffers virtually no loss of capacity over its lifetime; in fact, it works better if completely charged and discharged every few days. From the time they are first activated, liquid metal batteries require no outside heating or cooling for the lifetime of the system, eliminating a ton of system costs, and they can operate in a wide range of temperatures and conditions. The batteries contain materials less than half the cost of LIB materials, can be manufactured for less than half the cost of LIBs, and will run for 20 years at a “fraction of the cost” of LIBs. Volts 8.10 QuantumScape Wesoff QuantumScape claims to have solved the solid-state battery riddle that has stumped generations of battery scientists. It has created a commercially viable, rechargeable lithium metal battery with enough oomph to make it a contender to displace the internal combustion engine. With potential energy density exceeding 400 watt-hours per kilogram, solid-state battery deployment could mark the tipping point for adoption of electric vehicles and open up massive transportation markets. QuantumScape’s battery employs lithium metal anodes (referred to by the company as “solid state”), which could be the key to the next wave of EV battery performance gains, according to the findings from BMW shown in the chart below. Among its many innovations, QuantumScape has replaced the polymer separator used in conventional lithium-ion batteries with a ceramic separator, enabling the use of an anode of metallic lithium instead of one made of carbon or carbon-silicon. The metallic lithium anode is formed in situ when the finished cell is charged. A viable lithium-metal anode would allow higher energy density than is possible with conventional anodes and therefore longer driving range with the potential of faster charging, long cycle life and improved safety – qualities that are fundamental to changing the minds of those reluctant to purchase an EV. “I believe that retail investors and perhaps even the technical staff at Volkswagen have been severely underestimating the scale of the challenges facing QuantumScape. Manufacturing defect-free ceramic separators at meters-per-second speed is very unlike other manufacturing challenges faced by the battery or PV industry previously. This is insanely hard. Elon Musk always pounds his chest about”production hell” and how difficult making cars is. Making these ceramic separators will be at least 10× harder than any technical challenge Tesla [has] faced.” A business parallel with solar that the storage industry might want to avoid is how the photovoltaic market coalesced around a narrow set of Chinese-made silicon technologies after a U.S. venture capital frenzy that set the solar investment clock back a decade. Manufacturability, cost reduction and actual industrial policy won the solar market for China, rather than exotic science. QuantumScape is still a development-stage startup facing enormous technology and scaling risk as it attempts to move from lab to production. Wesoff Quantumscape Scam? SPAC’s are free to promote absurd financial projections and timelines that are illegal in a typical IPO. The current SPAC mania is partly fueled by insiders exploiting this loophole to lure retail investors, setting them up as bagholders for the pump and dump, when the lockup expires and they cash in. We think QS is a textbook case. Scorpion (2021) QuantumScape SPAC Scam (pdf) Massive investments in lithium-ion (Li-ion) battery manufacturing capacity, which is expected to more than triple to 1.3 TWh by 2023. Li-ion batteries’ scaling pathway is unlike that for silicon photovoltaic cells; investment continues to differentiate among chemistries with performance attributes that are best suited to specific use cases. Solid-state technology is poised to massively disrupt the storage industry by unlocking new opportunities for cheap, safe, and high-performing batteries. Meeting electricity goals for &lt;2C° global temperature increase may require deploying batteries much faster than Li-ion price decreases are predicted to enable (unless demand flexibility can be increased). High renewable penetration modeling scenarios generally assume that 3%–7% of the total installed renewable capacity is required as additional interday energy storage to account for forecast and demand uncertainty. If 60% of the 6,500 GW of current global electricity generation capacity were met with variable renewable energy, it would require between 120 and 280 GW of long duration storage, or enough capacity to power France plus Germany. RMI Breakthrough Batteries (pdf) 8.11 CO2 Canary Media Noon’s battery stores energy by using electricity to split the CO2 into solid carbon and oxygen gas; to discharge, it reverses the operation, oxidizing the powdery solid carbon. Canary Media (2023) 8.12 Graphene Wesoff Lyten recently raised $200M to scale up production of batteries made with graphene, a material that’s light, strong, conductive — and elusive. Since its discovery in 2004, graphene has been heralded as a miracle material. Lighter and stronger than steel and as hard as a diamond, it’s also a robust and flexible conductor of electrical and thermal energy. These properties are why graphene holds great promise for a range of clean energy applications — as long as it can be built inexpensively and at scale. Conventional 2D graphene is ​“pretty simple,” according to Dan Cook, Lyten’s co-founder and CEO — it’s a hexagonal lattice of carbon atoms in a flat layer. A number of companies aspire to produce the material with consistent commercial-grade quantity and quality, but it remains difficult to fabricate. In fact, much of what is sold as graphene today is actually just chunks of conventional bulk graphite. That’s good enough to be used in bicycle frames and other sports equipment, but it’s a far cry from the 1-atom-thick sheet of actual graphene needed to make, for example, a better battery. Lyten not only aims to solve the manufacturing challenges around graphene, but it also claims it can produce a much more useful material than standard graphene. When you get graphene in a three-dimensional version, then all of a sudden, you have properties that the original material could only dream of having. You have the ability to make it resistive. You can make it capacitive or inductive. You can make it structural; it can absorb light. We can actually shape the energy fields to create different forms of materials. Rechargeable batteries enhanced with graphene could enable a new generation of lighter, more durable energy-storage devices with shorter charging times and longer cycle lives. Graphene can also extend battery life because of its ability to better dissipate heat and improve battery qualities such as energy density by exploiting the material’s conductivity and large surface area. But so far, these advantages have only been proven out in lab settings. Lyten aims to change that. Unlike conventional lithium-ion batteries, the graphene-enabled lithium-sulfur batteries being developed by Lyten do not use nickel, cobalt or manganese, which reduces the environmental and ethical impacts of the battery supply chain. Lyten’s claim is that its 3D graphene can contain the unruliness of Lithium-sulfur batteries. Wesoff (2023) Is graphene a cleantech supermaterial? This startup thinks so "],["energy-storage.html", "9 Energy Storage 9.1 Pumped Hydro 9.2 Flow Batteries 9.3 Mechnical Energy Storage 9.4 Thermal Storage 9.5 Software (Market Flex) 9.6 Compressed Air", " 9 Energy Storage Wesoff The potential is great and the grid surely needs it, but long-duration energy storage still has a long way to go. Long-duration storage can accelerate the retirement of peaker plants, defer upgrades of transmission and distribution infrastructure, and improve the dispatchability of renewables such as solar and wind – theoretically, at least. Systems with more than 100 hours of energy storage capacity provide the most benefit to the grid. Long-duration storage can reduce the costs of deeply decarbonized electricity systems by 10 percent if the storage technology’s costs are below $20/kilowatt-hour. The savings could reach as high as 40 percent if long-duration storage costs could be reduced to $1/kilowatt-hour. Economic long-duration energy storage doesn’t exist. Yet. Today’s high-growth, gigawatt-scale energy storage market is absolutely dominated by lithium-ion batteries with short durations ranging from minutes to a few hours and used largely in ancillary, not energy, applications. 9.1 Pumped Hydro Pumped storage (which consists of pumping water from a low reservoir to a higher-elevation reservoir, and then releasing that water to spin turbines and generate power) accounts for 95% of all utility-scale energy storage in the U.S. America’s 43 pumped storage plants were mostly built between 1960 and 1990 to support nuclear power and have a total combined capacity of roughly 100 gigawatts of storage. There are a number of new plants in development that are being built, in this case, to help support renewables rather than nuclear. The problems with new pumped hydro projects: “They cost billions of dollars and take years to build, and can run afoul of environmental opposition New Design On Oregon’s Klamath River, a coalition of Indigenous groups, environmental advocates and other stakeholders will soon remove four hydropower dams erected between 1903 and 1962. The goal is to revitalize the ecosystem — and the surrounding economy. Upriver and roughly 10 miles northeast of Klamath Falls, as the crow flies, Rye Development hopes to build a new type of hydropower plant altogether. The Swan Lake Energy Storage Project would have no connection to surrounding rivers. Rye plans to excavate two 60-acre holes and fill them with water it has already secured. The water would cycle up and down the closed system as the plant charges and discharges electricity. Two ponds may not seem radical, but they mark a decisive break with the past. Canary Media (2022) Pumped Hydro Come-back 9.2 Flow Batteries Flow batteries circulate a liquid electrolyte through stacks of electrochemical cells and have long held the promise of 10-hour durations, tens of thousands of cycles, minimal degradation and no limitations on depth of discharge. This performance promise has lured venture capital investment and research and development — but so far, the investments have yielded few commercial, competitive flow battery products. 9.3 Mechnical Energy Storage Compressed-air energy storage remains a contender. The Los Angeles Department of Water and Power selected Range Energy and Mitsubishi Power Systems to develop underground salt caverns to store high pressure air (or someday hydrogen) Raise a rail car, a huge weight and a water column, respectively, with low-cost or curtailed energy, generating power when the mass is lowered. Quidnet will use excess renewable energy to store pressurized water underground in dry oil and gas wells. HydroStor Construction entails digging large-diameter shafts underground, one of which will be filled with water from aboveground tanks. Compressors on the surface will push air into the cavern, where it presses against the column of water to maintain a steady pressure. When released, that compressed air will spin 100-megawatt turbines that regenerate power. All that construction work is expected to take three to four years. Canary Hydrostor 9.4 Thermal Storage Thermal storage uses excess or curtailed power to charge a thermal “battery” made of materials such as molten salt or cryogenic liquids. HeatCrete Norway-headquartered EnergyNest makes its own branded ThermalBattery product which essentially stores heat in a patented form of concrete, which it has dubbed Heatcrete. A heat transfer fluid (HTF) at high temperatures passes through steel pipes cast into the ‘battery’, in technology that the company claims enables storage of energy at very low CapEx cost, using low-cost materials in a simple design. EnergyNest has previously said the Heatcrete materials can last 30 to 50 years of use without degradation. EnergyNest CanaryMedia 9.5 Software (Market Flex) Lee Kasten, a WECC-based power system operator, suggests that we should rely on software and markets, not hardware: “Where possible, we should use software over hardware. For example, don’t build a battery that costs a billion dollars, only works 2% of the time and only moves around 100 [gigawatt-hours] of electricity. Build an energy-imbalance market or an extended day-ahead market for $50 million or $100 million that moves around hundreds of gigawatts of electricity. Give all consumers price signals, and then watch the flexible consumers adapt to those price signals using software to manage existing loads.” Wesoff (canary Media) 9.6 Compressed Air Canary Media Hydrostor stores surplus electricity by compressing air into underground caverns. It updates a long-standing technology that never took off for electrical storage. Hydrostor thinks the tweaks it has made will allow underground storage to work in more places — just as grids increasingly need help turning wind and solar production into reliable 24/7 electricity. Goldman Sachs agreed and invested $250 million from its private equity division. That’s unusual for the long-duration energy storage sector, which typically draws riskier venture-capital investment. To build these projects, Hydrostor excavates an 8-foot-diameter shaft that goes 2,000 feet deep. Workers descend to hollow out a cavity, which the company then floods. When it’s time to ​“charge,” the aboveground facility compresses air and shoots it below the water barrier. To discharge, it releases the compressed air, which spins a turbine and produces electricity. These projects are designed for eight hours of discharge at full power capacity. But the technology can provide longer storage durations with a bigger cavern to hold more air. “All we need is reasonably competent bedrock.” Canary Media (2022) Hydrostor "],["amoniac.html", "10 Amoniac", " 10 Amoniac TU Spillvarme har en hovedrolle Fordi ren ammoniakk brenner dårlig og er vanskelig å tenne, er ideen til forskerne å bruke spillvarmen fra forbrenningsprosessen til delvis å spalte ammoniakken. Ammoniakk består av ett nitrogenatom og tre hydrogenatomer. Etter spaltingen sitter vi igjen med drivstoff som består av ammoniakk, nitrogen og hydrogen. Hydrogenandelen i dette brenselet bidrar med å sparke forbrenningsprosessen godt i gang, med god hjelp fra store mengder med oppvarmet luft fra omgivelsene. Det gir bevegelse og framdrift i motorens velkjente termiske prosess. At «arbeidsmediet» i denne forbrenningen er luft, gjør det ganske enkelt og billig å skalere opp prosessen, sånn at den kan tilpasses de største cargoskipene. For batteridrevne skip eller fartøy som bruker kraft fra brenselceller, er det en god del vanskeligere, sier Gruber. Muligheten for oppskalering gjør at løsningen er perfekt for store skip – som i dag ofte drives av forurensende tungolje. TU "],["aviation-fuel.html", "11 Aviation Fuel", " 11 Aviation Fuel Two-thirds (66%) of the warming from flying come from non-CO₂ forcings, including contrails. Ho Galuuci Startup Twelve broke ground on a commercial-scale facility in Washington state, which it says will be the first in the country to make sustainable aviation fuel from CO2. Nearly two years ago, the startup Twelve made its first batch of lower-emissions jet fuel at its lab in Berkeley, California. Using electricity, water and carbon dioxide, the company set out to make a synthetic fuel that could replace fossil-based kerosene. Now, Twelve is ramping up to make significantly higher volumes of its ​“E-Jet” fuel. Last week, the eight-year-old startup broke ground on a commercial-scale facility in Moses Lake, Washington. Once up and running by mid-2024, the facility will be the first of its kind in the country to make alternative jet fuel from CO2 and grid power. Airlines, Microsoft and Shopify have agreed to buy millions of dollars’ worth of E-Jet from the new plant. Twelve expects to churn out some 40,000 gallons of fuel per year before quickly expanding its production — potentially by as much as tenfold within the first year of operation. Twelve claims its alternative fuel can cut life-cycle greenhouse gas emissions by up to 90 percent compared to petroleum-based kerosene. Burning E-Jet in engines will still emit CO2. But the goal is “recycling” CO2. Companies are making strides to develop battery-powered planes and hydrogen-fueled jets. (Not far from Twelve’s site in Moses Lake, the startup Universal Hydrogen is test-flying a 40-passenger plane.) But both batteries and hydrogen are expected to serve mainly commuter and regional flights. Startups like Twelve and global fuel conglomerates alike are all racing to produce ​“sustainable aviation fuel” that can be blended with, and eventually be fully substituted for, fossil kerosene in existing jet engines — but they have a long way to go. Last year, major U.S. airlines consumed some 17.5 billion gallons of jet fuel. Less than 0.1 percent of the total, or 15.8 million gallons, was so-called SAF. the challenge isn’t just to make greater volumes of SAF but to produce it in completely new ways. Nearly all of the nation’s existing SAF supply comes from processing animal fats and used cooking oil, feedstocks that are in relatively scarce supply and are raising concerns of unintended knock-on climate risks. Illinois-based LanzaJet, a spinoff of LanzaTech, is making synthetic jet fuel from ​“waste-based” ethanol derived from municipal solid waste, agricultural residues and biomass. The company is currently building what it says is the world’s first ​“ethanol-based alcohol-to-jet” SAF production plant in Georgia, slated to be completed later this year. At full capacity, the Freedom Pines Fuels facility is expected to produce 10 million gallons of aviation fuel and renewable diesel per year. Another firm, New York–based Air Company, recently signed a $65 million deal with the U.S. Department of Defense to capture CO2 on military bases and turn it into jet fuel. The startup’s process involves combining CO2 and hydrogen in a chemical reactor to produce paraffins — colorless, oily liquids — for sustainable aviation fuel. Twelve uses another method to turn CO2 and hydrogen into juice for jet engines. To start, the company procures ​“waste” CO2 from industrial facilities. Twelve has developed its own electrochemical reactor to split carbon dioxide molecules into carbon monoxide using electricity. Engineers then use a separate electrolyzer to split water molecules into hydrogen and oxygen. The resulting mixture of carbon monoxide and hydrogen is known as ​“syngas.” Finally, the team deploys a multistep reaction called the Fischer-Tropsch process to convert the gas into a liquid — one that’s chemically identical to jet fuel made from petroleum. To produce SAF at its commercial-scale plant, Twelve will initially source CO2 from an ethanol refinery in neighboring Oregon and, later, from pulp and paper facilities in Washington, he said. As the nation’s SAF supply grows from a mere drop in the bucket to a sizable share of airlines’ jet fuel use, Twelve and other producers are likely to encounter a fresh set of hurdles. Many power plants and industrial facilities don’t have the infrastructure needed to capture, store, concentrate and transport the CO2 from their waste streams. Even if they did, those plants will likely generate less planet-warming gases over time as industries decarbonize and shift to renewable energy sources. For SAF producers that rely on CO2, a potential solution is to deploy ​“direct air capture” machines that suck the gas directly from the sky, though the technology remains expensive, energy-intensive and unproven at large scales. Companies that rely on hydrogen to make low-carbon fuels or power planes directly may also struggle to procure enough “green” supply. The availability of electricity is one of the biggest potential limitations when it comes to siting and scaling Twelve’s future SAF facilities. Galucci (2023) This new factory will turn CO2 into sustainable jet fuel "],["bio-fuel.html", "12 Bio Fuel", " 12 Bio Fuel Beslik Biomass is defined as any plant matter used directly as fuel or converted into other forms before combustion. Included are wood, vegetal waste (including wood waste and crops used for energy production), animal materials/wastes, sulphite lyes (also known as “black liquor”) and other solid biomass. The word biofuel is usually reserved for liquid or gaseous fuels, used for transportation. Currently biomass covers approximately 10 percent of the global energy supply, of which two-thirds is used in developing countries for cooking and heating. Biomass currently represents almost 60% of the EU’s renewable energy, more than solar and wind power combined, according to the EU’s statistical office, Eurostat. In the EU, Germany is the leading producer of solid biomass. Production volumes reached an estimated 12.8 million metric tons of oil equivalent, which was 2.6 million metric tons more than France, ranked second. Total primary energy production in the EU amounted to 96.9 million metric tons in 2019. One of the biggest problems related to large scale biomass supply is the energy density that is needed to decarbonise heavy CO2 emitting industries. Last year, a group of climate activists filed a lawsuit against the EU to challenge the notion that forest biomass is carbon neutral, a principle which is currently enshrined in the bloc’s renewable energy directive. According to this group “the treatment of biomass as carbon neutral runs counter to scientific findings” which shows that burning wood for energy typically emits 1.5 times more CO2 than coal and 3 times more than natural gas, the plaintiffs claimed. The European Court of Justice dismissed the case, saying the activists had failed to demonstrate how the directive was of “individual concern” to them. Which is by all means almost funny. Like fossil fuels, biomass releases carbon dioxide emissions upon combustion. While there is an argument for carbon balancing – due to the carbon dioxide that is removed from the atmosphere by trees and plants during their lifetimes via photosynthesis – these are nevertheless emissions that could be avoided if other renewable sources like wind or solar were used instead. In addition to CO2, burning biomass fuels results in the release of various other harmful gases such as carbon monoxide, NOx (nitrogen oxides), and VOCs (volatile organic compounds), which all contribute to air pollution. Providing the feedstocks used in biomass power plants – frequently in the form of wood pellets – requires large areas of forest and woodland to be cut down. Proponents will argue that all trees are replaced with new ones, that can grow, remove carbon, and be used for future energy needs. But this cycle requires strict adherence to sustainable land management and responsible agriculture throughout the supply chain. And as biomass power stations grow in number, demand for these materials will multiply accordingly – exerting greater pressure on a natural resource that is already under threat from other industries. There are biodiversity considerations too, because while an area of forest might be replaced to grow anew over time, the wildlife and ecosystems that are displaced by these actions are faced with a more immediate challenge that is not solved by planting young trees. Beslik Swiss voting A majority of Swiss voters also backed a free trade deal with Indonesia, which would expand palm oil imports at lower tariffs. Indonesia, a world away, is the site of a vast ecocide, destroying tropical forests (which are also carbon sinks) on an immense scale, driving countless species, including the Orang Utan, extinct, to make way for palm oil plantations. The palm oil goes to us consumers in the West. Roughly half of palm oil ends up burned in our cars as biodiesel, thanks to European Union rules aiming to replace fossil fuels with bio-based fuels. This rule arguably kick-started the ecocide in Indonesia — the clear result of problem shifting (from climate disaster to biodiversity calamity, although in fact the climate benefits of bioenergy are extremely dubious) rather than tackling the root cause: unsustainable overconsumption. The other half of palm oil ends up in food and cosmetics, and can easily be substituted with other, slightly more expensive, ingredients. In this case, a majority of the Swiss people voted, in full knowledge, to continue to participate in ecocide and exploitation, far across the world, for cheaper access to various goods including palm oil, and for preferential terms for Swiss firms in Indonesia. They decided that a few economic advantages in consumption and production were worth it: they made the calculus, and decided that ecocide was worth it. It’s one thing when your government or business leaders decide that ecocide is worth it to gain economic advantage. It’s something else entirely to be walking around the streets of a country where a majority of the people you meet and greet made the ghastly calculation to be complicit in ecocide for a few more francs. A few months ago, a majority of the Swiss population voted differently, but in vain. They voted to curb multinational corporations, and hold them accountable for human rights violations (including environmental damage) overseas. Sadly, this majority was not sufficient: a majority of cantons (regions) was also necessary, and this fell woefully short. A majority of people in the rural cantons, the places upheld as the heartland of traditional Swiss values by the far-right, voted to continue to allow multinational corporations based in Switzerland to violate human rights and degrade the environment overseas with complete impunity. Again, a few more Swiss francs are worth more than child labour or poisoning local populations. Nice values, eh? Steinberger "],["fuel-cells.html", "13 Fuel Cells", " 13 Fuel Cells Fuel cells are scaling up exactly as solar and wind did in previous decades. Fuel cells electrochemically convert hydrogen and oxygen into direct-current electricity. Fuel cells employ an assortment of electrolytes, catalysts and temperatures. But in almost all cases, the membranes are expensive to fabricate, and the technologies require precious metal catalysts (typically platinum or palladium) or high process temperatures. Input fuels range from natural gas to methanol to hydrogen. Ongoing research and development efforts, some led by the DOE, are underway to reduce the need for expensive metals and to improve the reliability and lifetime of the fuel cell stack. Common technologies include proton-exchange membrane (PEM), solid oxide (SOFC), phosphoric acid (PAFC) and molten carbonate (MCFC). There are a number of other technologies, all adept at destroying investor capital. GE, GM, Hyundai, Honda, Johnson Matthey, Panasonic, Siemens, Samsung, LG, Sharp, Toshiba and Toyota have all invested in (and in many cases ultimately abandoned) fuel cell technology. Bloom Energy, Doosan and FuelCell Energy build large stationary fuel cells, using SOFC, PAFC and MCFC technologies, respectively. Plug Power, on the other hand, targets its PEM fuel cell system at powering forklifts and other vehicles in the enormous materials handling market. While Bloom’s and FuelCell Energy’s equipment runs on natural gas, Plug Power’s PEM fuels cells run on pure “five nines” hydrogen and work most productively with a hydrogen infrastructure at the customer site. The U.S. Department of Energy continues to fund fuel cell development, most recently with $33 million to support hydrogen and fuel cell research and development, infrastructure supply-chain development and validation, and cost analysis activities. Wesoff "],["geothermal.html", "14 Geothermal 14.1 Heat Storage 14.2 Deep Drilling 14.3 Enhanced Geothermal 14.4 US - Nevada 14.5 China 14.6 US", " 14 Geothermal It is the only other thing that you could essentially attach to the current grid, almost anywhere in the world — in fact, below current cities. It seems like science fiction, right? Drilling to the center of the earth? But it’s not. It’s a super exciting clean power generation area Figure: U.S. geothermal resources at 10 kilometers depth CanaryMedia Enthalpy Enthalpy is a property of a thermodynamic system, defined as the sum of the system’s internal energy and the product of its pressure and volume, H = U + pV. It is a convenient state function standardly used in many measurements in chemical, biological, and physical systems at a constant pressure. The pressure-volume term expresses the work required to establish the system’s physical dimensions, i.e. to make room for it by displacing its surroundings. As a state function, enthalpy depends only on the final configuration of internal energy, pressure, and volume, not on the path taken to achieve it. The unit of measurement for enthalpy in the International System of Units (SI) is the joule. Other historical conventional units still in use include the British thermal unit (BTU) and the calorie. The total enthalpy of a system cannot be measured directly because the internal energy contains components that are unknown, not easily accessible, or are not of interest in thermodynamics. In practice, a change in enthalpy (ΔH) is the preferred expression for measurements at constant pressure, because it simplifies the description of energy transfer. When matter transfer into or out of the system is also prevented, the enthalpy change equals the energy exchanged with the environment by heat. Enthalpy - Wikipedia 14.1 Heat Storage Low Enthalpy Aquifer Technology Ruden Energy 14.2 Deep Drilling 14.2.1 Ultra Deep Drilling Quaise Energy Inc. has teamed with the Laboratory for Scientific Computing (LabSC) to develop computational models of the interaction of high-energy beams with geological materials. The models developed will provide understanding of the fundamental physics underlying the operation of a gyrotron-powered millimeter-wave (MMW) energy drilling system developed by Quaise, a spin-off company born from research at MIT (Massachusetts Institute of Technology) Plasma Science and Fusion Center. This new, breakthrough technology will be used for MMW drilling to reach depths of 10-20 km below the earth’s surface, which is beyond what can be accomplished today using conventional drilling. Deep drilling will enable harvesting supercritical geothermal energy with power densities several order of magnitudes larger than wind or solar energy, thus opening the opportunity for accessing a clean, carbon-free and power-dense energy source anywhere around the world. The successful development of a commercial system has to overcome technical challenges related to the interaction of millimeter electromagnetic waves with basement rock formations at extreme conditions and far-field transport of material. Think Geoenergy (News) Canary Media on Quaise The earlier experiments at MIT produced 10-centimeter-deep holes in palm-sized slabs of rock. At Oak Ridge, Quaise has vaporized 3-foot-deep holes into larger rocks using the national lab’s more powerful megawatt-size gyrotrons. This year, the startup is planning to drill about 30 feet into the actual ground outside the Oak Ridge facility. The company is planning to drill a 330-foot well in New Mexico or Colorado with the oil-and-gas drilling contractor Nabors Industries, which invested $12 million in the startup last year. After that, Quaise could drill a 3,300-foot-deep hole near the Newberry Volcano in Bend, Oregon. AltaRock, which is studying the crater, has called the site ​“probably the biggest untapped geothermal resource in North America.” Geologists believe a shallow magma body sits only 6,500 to 16,500 feet below the surface. Drilling here would allow Quaise to validate its technology in supremely hot conditions without having to dig too far down to start. If Quaise succeeds in demonstrating its novel approach, the startup plans to drill ultra-deep wells not just on the sides of volcanoes but primarily alongside existing power plants. Instead of coal or natural gas, steam produced with the earth’s heat could drive turbines and generate electricity using existing infrastructure. Power plants tend to be built near population centers, reducing the need for the long transmission lines that connect remote wind and solar farms. And existing plants tend to sit atop private property, which could potentially help Quaise avoid legal challenges like those facing geothermal projects on public land. Among the biggest risks for any advanced drilling system is seismic activity. In recent years, geothermal projects using different types of technologies were shut down in Switzerland, South Korea and France after triggering earthquakes and rattling surrounding cities. Canary Mediaon Quaise (2022) North Sea Geothermal Platforms in the UK North Sea could be converted to run on geothermal energy, providing power to the offshore field facilities or feeding power to markets in Europe via trans-North Sea interconnector grids. Traditionally, once an oil or gas field reaches the end of its productive life, its production platform is decommissioned. The structure may be removed and taken ashore for recycling/reuse, or part of the platform may remain on the seabed, perhaps creating an artificial reef. However, another alternative is becoming a more viable option – re-using the platform to extract geothermal energy. If applied to redundant platforms in the North Sea, this could create a whole new industry employing thousands of workers in new productive jobs in the offshore and onshore support sectors. Figure: UK 50 hottest geothermal gradient wells by degrees C. The UK continental shelf (UKCS) where many platforms are situated has a relatively thin earth’s crust – around 10 km (6.2 mi) thick compared to 40-70 km (25-43 mi) thick on land – which gives the wells their high bottomhole temperatures. There are more than 50 wells in the region with a geothermal gradient more than 122°F (50°C)/km, the highest being 296°F (147°C)/km. At Total’s Elgin-Franklin high-pressure/high-temperature (HP/HT) gas condensate development in the UK central North Sea, one of the wells was drilled to a depth of 6,100 m (20,013 ft), with a temperature of 387°F (197°C) and a pressure of 16,750 psi (1,155 bar). Heat from these wells could be employed to generate electricity on board the platform that could in turn be routed to the UK’s national grid via subsea cables. North Sea platforms have the advantage of being surrounded by cold sea water, which is at a much lower temperature than the onshore air cooling towers that are the conventional means of condensing a generating plant’s working fluids after they have passed through the turbines. It would also be possible to re-inject waste heat remaining in the fluids back into the subsurface oil-bearing level to increase field pressure and flows, thereby enhancing secondary oil recovery and extending field life. Furthermore, discovery of further oil fields might follow when drilling to greater depths to tap the geothermal energy beneath the platforms. Geothermal energy has huge potential when set in context against other energy reserves. All fossil fuels, i.e. coal, oil and gas, come from the earth’s crust. The crust makes up only 0.4% of the total mass of the planet, the remaining 99.6% being hotter than 932°F (500°C) within the crust, increasing to 9,032°F (5,000°C) at the core. The pressures within the earth are constantly generating this heat naturally. This means that geothermal energy is infinite in its nature, as it is naturally renewable. Recent research carried out in Russia, in the Kola Peninsula, has revealed moving fluids and open fractures at depths more than 12 km (7.5 mi). This discovery has led to a review of current deep geological thinking and has opened the development of geothermal energy extraction for electrical power generation. There are three types of geothermal energy. One – Geo-pressure – is where you have a high wellhead pressure, to which you can attach a hydroelectric type turbine to generate the power or electrical power from a natural gas letdown station. The twin screw turbine design from Langson Energy produces 1 MW electrical output, operating in temperatures from 350-550°F (177-288°C) and up to pressures of 600 psi (42 bar). This is already deployed by the natural gas industry to generate power, where the gas mains changes pressure to use the power instead of it being wasted. A second is created by separating the gas from the oil/water brine and using it like a diesel-type generator, i.e. burning the gas to produce power, which could provide an alternative to flaring on certain offshore installations. The third involves using the temperature, as in the high-pressure steam, steam, Organic Rankine Cycleturbine system. This approach could be applied to platforms or on a nearby support vessel. Offshore Magazin 14.2.2 Drilling Technology Løberg (facebook ‘Geotermisk Elektrisitet’ 210123) Ny brønnteknologi for geotermisk elproduksjon under utvikling. #regjeringen #enova I dag utvikles det meget raske og fleksible boreløsninger, f.eks en vanntrykk-hammer som ikke trenger borestreng, en kompakt drillbit under vanntrykk som borer i stål og fast fjell, som kan brushe opp casinger som er klogget, det er utviklet plasmaboresystemer som kan nå store dyp og høye temperaturer. Closed loops i gamle brønner kan bruke f.eks CO2 som arbeidsgass, og med temperatur og trykk på 4500 meter kan gassen bli superkritisk, slik at den kan trekke til seg store mengder varme. Det er kjent kunnskap (CO2 har vært brukt i kjøleskap etc i 20-30 år). Det er gjort prototyper som har testet CO2 i lukkede sløyfer/closed loops, og det er gjort beregninger på potensialet. Det er flere selskaper som spesialiserer seg på closed loop-systemer. Norge er faktisk med på dette også, som på Island, Indonesia etc. Vi kan gjøre det på Svalbard, og fase ut vårt kullkraftverk, som nå skal gå på diesel som er fraktet dit med skip. Hva med naturgass i mellomtiden, før det fases over til geotermisk el og varme? Det finnes mye info på våre sider om dette. Skal vi få geotermisk el og varme opp i bevisstheten, må vi sette oss inn i elementene jeg har nevnt i denne posten. Bare studer linkene under og del med de som bør oppdatere seg: Hammergy, norsk utviklet boreteknologi Drillbit utviklet av Fraunhofer IEG i Bochum og Fraunhofer-Chalmers Research Center for Industrial Mathematics FCC i Sverige. High-Temp 330C Drilling System (pdf) Plasmakutting kan brukes ned til 10 000 meter med temperaturer på 300-400 grader. Drilling/smelting med millimeter-bølger Roterende vannjet Liu (2017) (pdf) Closed loop selskap, EAVOR Gjenbruk av en brønn med closed loop og CO2 Les rapporten her (pdf) Se en informativ film her Lær om ulike geotermiske systemer og closed loop 14.3 Enhanced Geothermal The startup (Fervo) uses horizontal drilling techniques and fiber-optic sensing tools gleaned from the oil and gas industry. Technicians create fractures in hard, impermeable rocks found far below earth’s surface, then pump the fractures full of water and working fluids. The super-hot rocks heat those liquids, eventually producing steam that drives electric turbines. The idea is to create geothermal reservoirs in places where naturally occurring resources aren’t available. Gallucci (2023) America’s first ​‘enhanced’ geothermal plant just got up and running Canary Media Companies and U.S. agencies are increasingly turning to ​“enhanced” geothermal approaches, which, broadly speaking, aim to create their own geothermal reservoirs instead of relying on naturally available resources. Technicians drill into hard, impermeable rocks found thousands of feet below the earth’s surface to create fractures, which they pump full of water and working fluids. The super-hot rocks then heat those fluids, eventually producing the steam that drives electric turbines. A recent Department of Energy analysis found that enhanced geothermal projects could provide potentially 90 gigawatts of power to America’s grid by 2050, enough to meet the power needs of more than 65 million U.S. homes. To do so, companies will need to develop and significantly scale up technologies that today exist mainly as pilot and demonstration projects. They’ll also need to move carefully to avoid triggering earthquakes and causing potential damage in the process of drilling and fracturing rocks. Fervo, which has raised over $180 million from investors, began drilling at the Blue Mountain geothermal field in Humboldt County, Nevada in early 2022. The startup’s team drilled two wells that reach 7,700 feet deep and then connect with horizontal conduits stretching some 3,250 feet long. During the well tests, which took place in May, the startup flowed fluid into the reservoir, where the liquid reached temperatures of up to 376 degrees Fahrenheit and achieved a flow rate of 63 liters per second, that translated to 3.5 megawatts of power. When you think about the metrics to be successful for geothermal, it comes down to how much flow rate you have and how much reservoir volume is there, so that your fluid can stay down there long enough to get hot. Going horizontally, instead of just a simple vertical well, completely changes the game in terms of how effective you can be on a per-mile basis. Canary (2023) Startup claims breakthrough in turning the earth’s heat into clean power 14.4 US - Nevada CanaryMedia Fervo, which raised a $28 million Series B round in April, aims to make geothermal more competitive by applying advanced drilling technologies from the oil and gas industry. Horizontal drilling and underground fiber-optic sensors already make oil and gas drilling more efficient, but they can reduce costs and increase productivity for geothermal as well. Small modular nuclear reactors require years of regulatory vetting before they can be built. Long-duration energy storage needs to be tested, at small scales and usually for years, before customers will trust it. But advanced geothermal doesn’t involve any radically different technology, so its path to market could be swifter. Geothermal doesn’t have to reinvent the wheel. The partnership with Google extends beyond power delivery and into data crunching as well. Fervo will work with Google’s cloud analytics and artificial intelligence capabilities to study the data collected while drilling. One objective is to understand how to optimize geothermal plants as flexible resources, Geothermal currently supplies just 0.4 percent of U.S. electricity. But a Department of Energy study suggests this 24/7 renewable resource could reach as much as 120 gigawatts installed by 2050, delivering 16 percent of the country’s electricity, if technological improvements take hold and gain traction. CanaryMedia 14.5 China Smelror (in Norwegian) I gjennomsnitt øker temperaturen nedover i dypet med rundt 3° C for hver 100 m. Geologiske variasjoner som varmeproduksjon og termisk konduktivitet kommer først inn som en viktig faktor ved 1000 m og dypere. Her vil varmeproduksjon fra nedbryting av radioaktivt materiale ha mye å si for den geotermiske gradienten. Geotermisk energi i dag ikke dekker mer enn 1 % verdens totale behov for energi. 10 av de 15 landene med høyest andel geotermisk energi utviklingsland (inkludert El Salvador, Guatemala, El Salvador, Filippinene og Kenya). Et av de landene som satser sterkest på å utnytte geotermisk energi er Kina. Målet er å produsere 560 000 GWh elektrisitet innen 2015. Selv om dette ikke vil dekke mer enn 1,7 % av landets totale energiforbruk, vil dette tilsvare et forbruk på 68,8 millioner tonn standard kullekvivalenter. Det er beregnet av bruken av grunn geotermisk energi i de 287 største byene vil kunne tilsvare en produksjon på 2,8 millioner GWh elektrisk strøm. Fratrukket nødvendig strømbruk til utvikling og drift av varmepumpene, vil dette tilsvare et forbruk på 250 millioner tonn kullekvivalenter, noe som igjen vil tilsvare 500 millioner tonn redusert utslipp av karbondioksid. I tillegg vil Kina produsere geotermisk energi fra 12 ulike områder med varme kilder, tilsvarende et forbruk på 4,52 millioner tonn kullekvivalenter, tilsvarende et redusert utslipp av karbondioksid på hele 1,3 milliarder tonn. Potensialet blir ennå mer overveldende hvis man også tar i betraktning muligheten for utnyttelse av dyp geotermisk energi på en regional skala også utenfor de 12 høytemperatur-områdene. Dette vil kreve en storstilt satsning på energibrønner boret ned til mellom 3000 m og 10 000 m. I følge nyere beregninger kan man få ut energi tilsvarende 860 trillioner tonn kullekvivalenter, noe som tilsvarer mer enn 7 trillioner GWh elektrisk strøm, og som er nok til å dekke Kinas totale årlige energiforbruk 260 000 ganger. For å kunne utnytte potensialet for geotermisk energi må det framskaffes ny geologisk kunnskap. Vi må kjenne til løsmassemektigheter, ha kunnskap om hydrogeologiske forhold i undergrunnen, kjenne til porøsitet og permeabilitet i reservoarene dypt nede i undergrunnen, vi må kjenne til varmeproduksjon og varmeledningsevne i de ulike bergartene, og vi må vite hvor store rom i undergrunnen de geotermiske reservoarene strekker seg over. Bærekraftig energi for alle handler mye om geologi. De som ønsker å gjøre en forskjell ved å bidra til en verden med økt bruk av miljøvennlige energikilder, gjør klokt i å velge en utdanning som gjør at de kan omsette gode intensjoner til konkret handling. I så måte er kunnskap innen geologi et godt og nødvendig fundament. Smelror 14.6 US In this paper, we identify the technological, economic, and political reasons that the United States has failed to exploit its geothermal resources. We provide actionable policy recommendations to sustainably and economically utilize the vast energy reserves under our feet. Geothermal Everywhere: A New Path for American Renewable Energy Leadership "],["hydro-power.html", "15 Hydro Power", " 15 Hydro Power Today the technical potential for hydropower development around the world is much greater than the actual production: the percent of potential hydropower capacity that has not been developed is 71% in Europe, 75% in North America, 79% in South America, 95% in Africa, 95% in the Middle East, and 82% in Asia-Pacific. This is significant if you think about transition we need to make. Due to the political realities of new reservoirs in western countries, economic limitations in the third world and the lack of a transmission system in undeveloped areas, perhaps 25% of the remaining technically exploitable potential can be developed before 2050, with the bulk of that being in the Asia-Pacific area. Some countries have already developed their hydropower potential and have very little room for growth: Switzerland for example produces 88% of its potential and Mexico 80%. Today we have around 1,300 GW of installed hydropower capacity globally. According to the International Renewable Energy Agency (IRENA)’s Global Renewables Outlook 2020, this figure will need to grow by around 60 per cent by 2050 to help limit the rise in global temperature to well below 2 degrees Celsius above pre-industrial levels. While a large hydropower facility can often provide low-cost electricity for 50 to 100 years after being built, the upfront construction costs can be large. This, combined with the fact that suitable places for reservoirs are becoming rarer over time, means that large-scale hydropower plant construction costs may continue to rise. A study by researchers from IIASA and China investigated the impacts of different levels of global warming on hydropower potential and found that this type of electricity generation benefits more from a 1.5°C than a 2°C climate scenario. Environmental concerns about dams typically centre on their blockage of fish migrations or their upstream impacts, such as the inundation of habitats and the displacement of human communities. However, dams can cause serious downstream impacts as well. Although the public generally thinks of threats to aquatic organisms solely in terms of water quality, hydrological alteration – the modification of downstream water flow regimes caused by dams and infrastructure – is one of the primary causes of the degradation of freshwater ecosystems worldwide. Unfortunately, hydropower’s global track record of managing environmental and social impacts and assuring equitable distribution of social benefits has been less than stellar. So the occasional generality thrown out by the most vigorous proponents, that hydro is ‘environmentally friendly’, rings hollow and calls for a more careful analysis. No hydropower project is likely to be 100% sustainable. All projects must be viewed as more or less sustainable. To address the environmental and social concerns and guide the pursuit of sustainability, several entities have developed policies that articulate ‘sustainable hydro power’, including Green Hydro in Switzerland and the Low Impact Hydropower Institute in the US, in addition to the guidance on sustainability presented in the World Commission on Dams report. Recently, the International Hydropower Association (IHA) has developed both sustainability guidelines and a sustainability assessment protocol. Collectively, these documents describe specific measures for hydropower planning and operation that can be used to evaluate the sustainability of a hydro project or programme. Beslik "],["hydrogen.html", "16 Hydrogen 16.1 Home heating Hydrogen 16.2 Green vs Blue Hydrogen 16.3 White Hydrogen 16.4 Wind to Hydrogen 16.5 Hydrogen Aviation 16.6 Green Hydrogen Cost Reduction 16.7 Green Hydrogen- ‘Gas of the Future?’ 16.8 Hydrogen Mirage?", " 16 Hydrogen For the past 100 years hydrogen has been used predominantly in ammonia production , oil refining, methanol production , and ten percent for other things like the plastic containers that keep salad fresh, the fabric of the seat cushions on your furniture, or the material on your winter jacket. Hydrogen has never had a market other than to make our food, make gasoline, and make the products we use every day. We never had the technology to use hydrogen for energy because the technology for hydrogen fuel cells has been incubating in its infancy for over 50 years. Well guess what? Hydrogen fuel cell technology has blossomed in the past decade to bear fruit for a clean energy future. We don’t need to use hydrogen for refining oil in a future state economy, and we don’t need to make it like our fathers, grandfathers, and great grandfathers used to in the early 1900s. It’s misleading to use century old backward looking stats as the basis for a forward-looking study when new blue hydrogen investments will be using autothermal reformation (ATR) and carbon sequestration. RMP (2021) The Truth about Blue Hydrogen ATR Kim Abstract The autothermal reforming (ATR) process for hydrogen production saves considerable energy for the reaction compared with endothermic steam methane reforming (SMR). However, it requires a supply of pure oxygen, for which an air separation unit (ASU) is needed; this hinders the adoption of ATR in industrial applications because of both the high capital and operating costs. At the same time, in liquefied natural gas (LNG) regasification terminals, the cold energy from the regasification process is typically wasted. Coincidentally, the temperature of this waste cold energy matches that required for ASU operation. Thus, in this paper, a novel ATRprocess is proposed in which an ASU and LNG regasification are integrated in order to make use of the cold energy as an operating utility and achieve hydrogen production from the LNG. For the sake of comparison, the proposed system and a conventional SMR process are optimized using a genetic algorithm to evaluate and benchmark its performance. With an LNG feed stream of 827.5 kmol/h, the optimal case of the proposed system produces 2508.0 kmol/h hydrogen gas with a purity of 99.2%; the exergy destruction is reduced by 18.6% and its overall exergy efficiency is approximately 25.4% higher than that of the SMR process. Also, an economic evaluation is performed using the net present value (NPV) as an indicator. The NPV of the proposed system is 338.9 million USD, which is 72.6% higher than that of hydrogen production with the SMR process from LNG. [Kim (2021) Process Integration of an Autothermal Reforming Hydrogen Production System with Cryogenic Air Separation and Carbon Dioxide Capture Using Liquefied Natural Gas Cold Energy}(https://pubs.acs.org/doi/10.1021/acs.iecr.0c06265#) Csiro A combination of SMR and combustion of the fuel (methane), where steam is added to the oxidation process. The heat from the oxidation component supplies the energy required for the steam reforming process. Why is it important? Steam methane reforming and partial oxidation are integrated into a single system, in which heat integration has been incorporated. Characteristics Inputs: Hydrocarbons, oxygen, steam, heat By-products: CO2 Operating temperature: &gt;500°C Energy efficiency: 60-75% Benefits High selectivity to syngas Flexible H2/CO ratio for syngas production, for subsequent production of synthetic fuels Less clean-up required than pyrolysis options Can use CO2 as a reagent Lower process temperature than partial oxidation Heat from fuel combustion is used to supplement the SMR component - lower parasitic heat load as a result The SMR and combustion components are integrated into a single unit Low carbon/soot formation which minimises pressure drop and reduces OPEX Compact design, small footprint relative to other fossil fuel conversion methods Rapid start-up time Limitations Extensive control system required Requires air or oxygen Requires CCUS to achieve low carbon emissions Csiro (2023) Autothermal reforming (dry or steam) 16.1 Home heating Hydrogen Guardian In the remote hills of Cumbria, a few miles north of Hadrian’s wall, three nondescript terrace houses stand side by side, quietly offering a glimpse of a low-carbon future. The houses are intentionally unremarkable in every way but one: they are the first in the UK to run on a blend of clean-burning hydrogen as part of the most sophisticated hydrogen testing facility in the world. Welcome to Hystreet. Engineers at the five-hectare site are testing whether hydrogen can safely replace the fossil-fuel gas pumped through transmission pipes and local grid networks into British homes as part of the government’s efforts to meet climate targets. “Ninety-nine percent of people don’t think about where their gas comes from, or how it gets there,” says Antony Green, National Grid’s hydrogen tsar and head of the FutureGrid project. His task is to create a realistic replica of the UK’s gas system to test whether the same pipelines that have carried gas from the North Sea into homes since the 1970s could transport low-carbon hydrogen in the future. Heating British homes accounts for 15% of the country’s total emissions, meaning a low-carbon alternative will be crucial to cut emissions to net zero by 2050. But the testing site is also key to understanding how hydrogen can be transported to major factories and industrial clusters to help tackle emissions from polluting factories and power plants. “The evidence we have built over the last few years shows that we can do this,” Green says, walking along the length of a giant gas pipe. “It’s all very well and good doing the paperwork. But you still need to prove it.” Using the UK’s existing gas infrastructure to carry hydrogen is no simple task. It is more combustible than the traditional methane-rich gas we have learned to use safely in our homes, and its smaller molecules mean it is three times more likely to leak from pipelines or into homes than fossil gas. On the plus side, hydrogen is also lighter, meaning it is more likely to dissipate than to pool and create a combustion threat. Like natural gas, hydrogen is odourless, so would have the same distinctive smell added to help people quickly notice a leak. When it burns, it is hard to see in daylight, so the hob has an adjustment that produces a visible flame, similar to that of a traditional gas hob but redder in colour. For the sceptics, the challenge of overhauling the UK’s 4,000 miles of underground gas pipelines is too costly a step when heating and cooking could run on a low-carbon electricity system instead. The opposing factions in the debate run along predictable industry lines. National Grid and other companies that operate legacy gas infrastructure or gas production projects tend to favour home hydrogen, to prolong the life of existing assets. Energy companies that invest in low-carbon electricity generation tend to back electric heat pumps as the future for low-carbon homes. Blue Hydrogen Although blue hydrogen is widely considered “low-carbon”, it has failed to win favour among climate campaigners. Despite using carbon-capture technology to trap emissions from the process, about 10% to 15% of the CO2 in the fossil gas would find its way to the atmosphere. It would also require continuing offshore gas production, which carries a hefty carbon footprint. Guardian 16.2 Green vs Blue Hydrogen Longdon Abstract Hydrogen produced using fossil fuel feedstocks causes greenhouse gas (GHG) emissions, even when carbon capture and storage (CCS) is used. By contrast, hydrogen produced using electrolysis and zero-emissions electricity does not create GHG emissions. Several countries advocating the use of ‘clean’ hydrogen put both technologies in the same category. Recent studies and strategies have compared these technologies, typically assuming high carbon capture rates, but have not assessed the impact of fugitive emissions and lower capture rates on total emissions and costs. We find that emissions from gas or coal based hydrogen production systems could be substantial even with CCS, and the cost of CCS is higher than often assumed. Carbon avoidance costs for high capture rates are notable. Carbon prices of $22–46/tCO2e would be required to make hydrogen from fossil fuels with CCS competitive with hydrogen produced from fossil fuels without CCS. At the same time there are indications that electrolysis with renewable energy could become cheaper than fossil fuel with CCS options, possibly in the near-term future. Establishing hydrogen supply chains on the basis of fossil fuels, as many national strategies foresee, may be incompatible with decarbonisation objectives and raise the risk of stranded assets. Longdon Highlights • Emissions from gas or coal based hydrogen systems are substantial even with CCS. • Fugitive emissions are rarely included in national and international H2 strategies. • CCS is an expensive option for decarbonising hydrogen production. • Electrolysis with renewable energy could become cheaper than fossil fuels with CCS. Longdon (2021) ‘Clean’ hydrogen? – Comparing the emissions and costs of fossil fuel versus renewable electricity based hydrogen (paywall) Guardian Green hydrogen beats blue on emissions and financial cost. Guardian (2021) Green hydrogen beats blue on emissions and financial cost 16.3 White Hydrogen Ambrose For more than a decade, the village of Bourakébougou in western Mali map has been powered by a clean energy phenomenon that may soon sweep the globe. The story begins with a cigarette. In 1987, a failed attempt to drill for water released a stream of odourless gas that one unlucky smoker discovered to be highly flammable. The well was quickly plugged and forgotten. But almost 20 years later, drillers on the hunt for fossil fuels confirmed the accidental discovery: hundreds of feet below the arid earth of west Africa lies an abundance of naturally occurring, or “white”, hydrogen. Today, it is used to generate green electricity for Bourakébougou’s homes and shops. But geologists believe that untapped reservoirs of white hydrogen in the US, Australia and parts of Europe have the potential to provide the world with clean energy on a far greater scale. the US Geological Survey has said that even if only a small fraction of hydrogen under the Earth’s surface could be recovered, there would probably be enough to last for hundreds of years. One borehole drilled in 1921 on Kangaroo Island produced as much as 80% hydrogen. Another, on the nearby Yorke peninsula, was close to 70%. The burgeoning hydrogen industry’s supporters include Bill Gates. The billionaire investor, through his company Breakthrough Energy, was reportedly one of five backers to pour about $90m into Koloma, a company based in Colorado which is hunting natural hydrogen along the US’s Midcontinental Rift System. The 1,200-mile tectonic fault running through North America is also being targeted by Natural Hydrogen Energy, a startup due to begin exploration work alongside Australia’s HyTerra in Kansas later this month. In Europe, which remains gripped by a gas supply crisis as a result of of Russia’s invasion of Ukraine, white hydrogen has been discovered in France, in the Lorraine mining basin. And a British company, Getech, is adapting software developed to find oil to locate hydrogen deposits. The true potential of white hydrogen will depend on the findings from these early projects. Oil companies including Total and Engie in France, and Repsol in Spain, have taken modest steps on white hydrogen. They don’t want stranded assets, but white hydrogen could cannibalise their primary market. Ambrose (2023) Prospectors hit the gas in the hunt for ‘white hydrogen’ 16.3.1 How green is blue hydrogen? Howarth Hydrogen is often viewed as an important energy carrier in a future decarbonized world. Currently, most hydrogen is produced by steam reforming of methane in natural gas (“gray hydrogen”), with high carbon dioxide emissions. Increasingly, many propose using carbon capture and storage to reduce these emissions, producing so-called “blue hydrogen,” frequently promoted as low emissions. We undertake the first effort in a peer-reviewed paper to examine the lifecycle greenhouse gas emissions of blue hydrogen accounting for emissions of both carbon dioxide and unburned fugitive methane. Far from being low carbon, greenhouse gas emissions from the production of blue hydrogen are quite high, particularly due to the release of fugitive methane. For our default assumptions (3.5% emission rate of methane from natural gas and a 20-year global warming potential), total carbon dioxide equivalent emissions for blue hydrogen are only 9%-12% less than for gray hydrogen. While carbon dioxide emissions are lower, fugitive methane emissions for blue hydrogen are higher than for gray hydrogen because of an increased use of natural gas to power the carbon capture. Perhaps surprisingly, the greenhouse gas footprint of blue hydrogen is more than 20% greater than burning natural gas or coal for heat and some 60% greater than burning diesel oil for heat, again with our default assumptions. In a sensitivity analysis in which the methane emission rate from natural gas is reduced to a low value of 1.54%, greenhouse gas emissions from blue hydrogen are still greater than from simply burning natural gas, and are only 18%-25% less than for gray hydrogen. Our analysis assumes that captured carbon dioxide can be stored indefinitely, an optimistic and unproven assumption. Even if true though, the use of blue hydrogen appears difficult to justify on climate grounds. Howarth (2021) How green is blue hydrogen? St.John Why blue hydrogen can be worse than burning gas After spending several years searching for evidence, Howarth teamed up with Mark Z. Jacobson, a professor of civil and environmental engineering at Stanford University, to conduct their own research. The results, published in August 2021, indicate that in almost all cases, blue hydrogen ends up emitting more greenhouse gases compared to simply burning fossil gas directly for heating and industrial processes. Several factors contribute to this counterintuitive result, Howarth said. The first is that wells, pipelines, compressor stations and other gas-delivery infrastructure leak methane, the primary component of fossil gas. Methane is a relatively short-lived but powerful greenhouse gas, with more than 80 times the global warming impact of carbon dioxide over a 20-year period. “You can’t use natural gas without fugitive methane,” Howarth said. ​“It may not be a huge amount, but it has a huge climate impact.” Howarth and Jacobson assumed that 3.5 percent of fossil gas used to make hydrogen is leaked — higher than what’s reported by the oil and gas industry and tracked by the U.S. Environmental Protection Agency, but in line with recent third-party analysis from satellite and aerial monitoring. They also considered emissions from the energy needed to drive the gray-hydrogen steam methane reformation (SMR) process, which is usually generated with fossil gas in today’s hydrogen facilities, as well as the energy needed to operate carbon capture equipment. These emissions largely erased the climate benefits of capturing carbon from SMR plants, yielding only a 9 percent to 12 percent reduction in CO2 equivalent emissions compared to unabated gray hydrogen production, the analysis found. What’s more, the energy used to convert fossil gas to hydrogen and capture the carbon emissions increases the emissions impact of every unit of blue hydrogen compared to the comparable unit of fossil gas it might replace for heating and industrial purposes, which make up a significant portion of the end uses being considered by many clean hydrogen hubs. Due to this extra energy and emissions impact, burning blue hydrogen for these purposes is about 20 percent more emissions-intensive than simply burning fossil gas to begin with, the analysis found. Even if the assumed methane leakage from upstream fossil-gas infrastructure in the researchers’ models was reduced to about 1.5 percent — roughly the current estimate from EPA — the use of blue hydrogen only reduced greenhouse gas emissions by 18 to 25 percent compared to making gray hydrogen, and the emissions associated with burning it for fuel remained greater than sticking with fossil gas, according to the report. A February report from the Energy Futures Initiative, a nonprofit research group run by former Energy Secretary Ernest Moniz, states that the average ​“life-cycle emissions” of blue-hydrogen production — a measure that includes leakage from gas wells and pipelines as well as gas used to power hydrogen production and carbon capture — add up to more than a third of the typical emissions from gray-hydrogen production and are ​“often out of control of the production facility.” Nor is it clear that blue hydrogen can be cost-competitive against green hydrogen in the long run. But the scale of blue hydrogen projects eclipses that of the more numerous but smaller green hydrogen projects, accounting for 95 percent of the total production capacity. This indicates the scale of the fossil fuel industry’s ambitions for expanding the use of fossil gas to serve the hydrogen market. St.John (2023) The case against the US government’s big ​‘blue hydrogen’ bet 16.4 Wind to Hydrogen Siemens Gamesa and Siemens Energy have announced plans to invest €120m ($146m) in a five-year strategy to unlock the potential of harvesting green hydrogen from offshore windpower. The companies are collaborating on a solution to integrate an electrolyzer into an offshore wind turbine as a single synchronized system to directly produce green hydrogen. 16.5 Hydrogen Aviation Heynes In 2008, Boeing flew the first hydrogen-powered aircraft whilst ZeroAvia flew the world’s first hydrogen-powered commercial aircraft in 2020. But the main industry player, already in the works to present hydrogen-powered aircraft to the market, is Airbus. For hydrogen-powered aircraft, hydrogen can be used in two ways: as a fuel source for fuel cells, when hydrogen reacts with oxygen to produce electricity that powers the engine or alternatively it can be used directly as a fuel source in a modified engine. Airbus is looking at both these methods for aircraft with the company presenting three models of modified aircraft that would be operated using hydrogen and has already committed to have the first aircraft in service by 2035. Because hydrogen can be extracted from water, airports could generate their own hydrogen fuel, reducing the need for fuel transportation, eliminating related emissions and possible transportation safety hazards. The potential of hydrogen as aviation fuel is undeniable, but there is still a long way to go. The constantly growing attention to aviation sustainability will act as a catalyst in making a hydrogen-powered aircraft a reality. Revolutionary hydrogen technology for aviation and urban air mobility has been unveiled today (2nd March) by HyPoint – and the Californian fuel cell specialist is expecting to start shipping the product in 2022. HyPoint says its NASA award-winning turbo air-cooled hydrogen fuel cell system will cut years off commercial delivery timelines for hydrogen aircrafts and unlock the emerging hydrogen aviation market. The company claims its technology delivers an unprecedented combination of specific power and energy density and has passed key validation testing to prove its technical viability. Heynes 16.6 Green Hydrogen Cost Reduction IRENA Green hydrogen currently costs between two and three times more than “blue” hydrogen, which is produced using fossil fuels in combination with carbon capture and storage (CCS). Falling renewable power costs and improving electrolyser technologies could make “green” hydrogen cost competitive by 2030. The International Renewable Energy Agency (IRENA) outlines strategies to reduce electrolyser costs through continuous innovation, performance improvements and upscaling from megawatt (MW) to multi-gigawatt (GW) levels. Among the findings: Electrolyser design and construction: Increased module size and innovation with increased stack manufacturing have significant impacts on cost. Increasing plant size from 1 MW (typical in 2020) to 20 MW could reduce costs by over a third. Optimal system designs maximise efficiency and flexibility. Economies of scale: Increasing stack production with automated processes in gigawatt-scale manufacturing facilities can achieve a step-change cost reduction. Procurement of materials: Scarcity of materials can impede electrolyser cost reduction and scale-up. Efficiency and flexibility in operations: Power supply incurs large efficiency losses at low load, limiting system flexibility from an economic perspective. Industrial applications: Design and operation of electrolysis systems can be optimised for specific applications in different industries. Learning rates: Based on historic cost declines for solar photovoltaics (PV), the learning rates for fuel cells and electrolysers – whereby costs fall as capacity expands – could reach values between 16% and 21%. Ambitious climate mitigation: An ambitious energy transition, aligned with key international climate goals, would drive rapid cost reduction for green hydrogen. The trajectory needed to limit global warming at 1.5oC could make electrolysers an estimated 40% cheaper by 2030. IRENA (2020) Green Hydrogen Cost Reduction (pdf) 16.7 Green Hydrogen- ‘Gas of the Future?’ Tooze Already one hundred and fifty years ago Jules Verne touted hydrogen as a possible fuel. In the late 1960s GM showed off its electrovan powered by a hydrogen fuel cell. In the 1990s and 2000s the hydrogen economy was again in vogue. President Bush and Governor Schwarzenegger were both fans. But again and again, the great hydrogen revolution failed to materialize. H2 began to acquire the reputation as the fuel that had the brightest future and always would have. But history is not fate. And our times are different. Whereas, previously, hydrogen was touted as an alternative to petrol. The urgency of the climate agenda and the push for net zero has widened its role. Green hydrogen is no longer seen just as a long-shot substitute for gasoline. It is touted as the Swiss army knife of the energy transition, suitable for applications across industry, transport, agriculture, home-use and for the power sector itself. In recent years a huge coalitions of corporate interests has assembled behind green hydrogen. The Hydrogen Council, launched at Davos in January 2017, has grown into a massive corporate array advised by top tier consultancy McKinsey. One recent compilation found that hydrogen projects currently under discussion totaled a staggering 957 GW in electrolyzer capacity. The Hydrogen Council and the IEA settle more modestly for a figure of 500 GW by 2050. Currently, the combined rating of all hydrogen electrolyzers in operation around the world is less than 1 GW. The hype around hydrogen is indicative of the way in which the energy transition has morphed, some might say mutated, into an industrial policy race with hundreds of billions in subsidy, markets numbered in trillions and huge slices of the economic structure up for grabs. $1.1 trillion going into the energy transition in 2022. This surge in spending is good news, but what is less clear is the direction of travel. This need not be seen as a bad thing. It would be naive to imagine that a process as complex as the energy transition can be planned decades in advance from the top down. Governments, businesses, producers and consumers are all embarked on a process of experimentation and learning. And, by their decision, weighted highly unequally, they are creating facts on the ground, which if not irreversible certainly imply long-term commitment. Hydrogen may burn relatively cleanly and yield electricity through fuel cells, but since hydrogen has to be produced through the application of energy, the process is inherently inefficient. Why not simply apply the electricity you are using to make hydrogen to the task at hand? The members of the hydrogen coalition are all obviously incumbent fossil fuel and petrochemical interests looking for a bridge to the new era. If realized, their ambitious hydrogen projects may overload the available supply of green power, for little real benefit. By diverting badly needed clean power, green hydrogen vanity projects may even slow down the energy transition. And the subsidy regimes that are being put in place could become self-perpetuating. As Gernot Wagner and Danny Cullenward recently warned, “hydrogen could become the next corn ethanol”, a ruinously inefficient and environmentally damaging creature of subsidies that are too big to kill. What hydrogen exposes is that there is still huge uncertainty about many basic parameters of the low-carbon future, about the technologies that will emerge as dominant, the likely structure of prices and the patterns of supply and demand for key materials and other inputs. Hydrogen has never in fact been produced at scale by hydrolysis. Total global capacity is less than 1 GW. We simply don’t know if any of it can be done. There is a real risk that big new hydrogen projects might divert clean power from more efficient strategies for decarbonization. The key frontline in debates about the huge new hydrogen subsidies concerns additionality. In the debate about hydrogen there is one fact that too often goes unremarked. The hydrogen economy is not a hypothetical. Our economies as they stand already rely on the production of huge volumes of hydrogen for two basic chemical processes: the processing of oil and the production of fertilizer. For these purposes we produce over 90 million tons of hydrogen annually. It is an industry worth $160 billion or so. And because hydrogen is won from natural gas and sometimes coal it is an extremely dirty process. These uses for hydrogen do no have the glamour of green aviation or fuel-cell vehicles. But they are essential and they are very dirty. Globally, hydrogen production today accounts for emissions equivalent to those of an economy like that of Germany. Demand will be eased by the winding down of the oil industry which is the major consumer of industrial hydrogen. But the rest of the chemical industry remains, most crucially of all the fertilizer industry. Industrial hydrogen is produced on the spot in chemicals plants. It isn’t shipped in like coal or oil or natural gas. Creating a market for industrial green hydrogen is not simply a matter of technology. It requires the creation of a new commodity market and a new wave of highly specific investment. Tooze (2023) Carbon notes #5: Green hydrogen, the “gas of the future”? 16.8 Hydrogen Mirage? Tooze In technological terms, hydrogen may represent a shimmering image of possibility on a distant horizon, but in political economy terms, it has a more immediate role. It is a route through which existing fossil fuel interests can imagine a place for themselves in the new energy future. The presence of oil majors and energy companies in the ranks of the Hydrogen Council is not coincidental. Hydrogen enables natural gas suppliers to imagine that they can transition their facilities to green fuels. Makers of combustion engines and gas turbines can conceive of burning hydrogen instead. Storing hydrogen or ammonia like gas or oil promises a solution to the issues of intermittency in renewable power generation and may extend the life of gas turbine power stations. For governments around the world, a more familiar technology than one largely based on solar panels, windmills, and batteries is a way of calming nerves about the transformation they have notionally signed up for. Tooze (2023) Hydrogen Is the Future—or a Complete Mirage Collins on Liebreich Collins (2021) Liebreich: ‘Oil sector is lobbying for inefficient hydrogen cars because it wants to delay electrification’ Liebreich’s Hydrogen Ladder Liebreich (2021) The Clean Hydrogen Ladder Debunking Liebreich RMP (2022) Michael Liebreich’s Hydrogen Ladder Debunked Debunking Ulf Bossel Dr. Bossel does not understand how our energy grid works and where waste is really occurring. Dr. Bossel’s thesis statement is that converting water to hydrogen is a wasteful use of electricity. Ironically, we often have more generating capacity than we can use or transmit and we are curtailing electricity generation that hydrogen production could easily soak up and save for cloudy and windless days. It’s actually wasteful to not create hydrogen because so much electricity is going unemployed. RMP will drive this point home throughout this rebuttal to Dr. Bossel’s thesis and we will look at data from various sources like CAISO to support this thesis argument. Economics is a complex subject and it’s imperative that all things are considered which is where Dr. Bossel’s arguments fail. Producing, compressing, and storing hydrogen might seem wasteful in a laboratory analysis, but the opposite is true in the real world. Let’s talk about electricity “curtailment”. Curtailment of carbon zero renewable electricity is when wind &amp; solar electricity generation capacity exceeds society’s immediate needs and the grid operator does not allow that electricity onto the grid. Terawatt hours are being wasted each year on grids around the globe because of not employing that capacity to store energy as hydrogen. Dr. Bossel has made good contributions to the science of physics and makes good points about the energy density issues of methanol and ammonia versus those of compressed or liquefied H2. To those points, RMP finds common ground with Dr. Bossel. But, where economic science is concerned, Dr. Bossel’s thesis quickly falls apart and becomes not credible. Dr. Bossel’s failure to include natural gas in his analysis shot his argument in the foot from the very first paragraph. By failing to demonstrate knowledge of how a high-voltage electricity grid works, Dr. Bossel further disqualifies himself as a credible source. RMP (2017) Debunking Dr. Bossel’s Anti-Hydrogen Thesis Debunking Mark Jacobsen - The Truth about Blue Hydrogen Blue hydrogen has been making headlines this summer. It’s made from natural gas in a two-part process called autothermal reformation (ATR). The ATR process is different than the steam reformation (SMR) process used to make most hydrogen over the past century. There has never been a market for hydrogen for energy so we learn more &amp; more each year about how to make zero, net zero, and low emission hydrogen for future markets. What distinguishes the ATR process of making hydrogen from the SMR process is a step called partial oxidation. In the first part of the ATR process, methane (CH4) is partially oxidized to create syngas. In the 2nd part of the ATR process, called a water-gas shift reaction, the syngas is mixed with steam to make carbon dioxide &amp; hydrogen. What makes the hydrogen “blue”, and why the US Department of Energy is talking about this pathway for hydrogen production, is that ~95% of the CO2 from the process is easily separated from the natural gas and can be permanently sequestered such that it will never reach our atmosphere. The reason this hydrogen is being called “net zero” is because the by-products of the process can be used to make the energy required to execute the process as well producing excess energy. he first major flaw in the Jacobson/Howarth paper is that the study employs a 20-year window to assign a greenhouse gas equivalency to methane instead of the 100-year window the IPCC uses3. While it’s true methane is a powerful GHG that’s 100 times more potent than CO2, it goes away in about 12 years. Integrating methane’s warming potential over a 20-year window gives it a CO2 equivalency of 86 which is the figure that Howarth &amp; Jacobson use in their study. Over a 100-year window, however, it’s only 28 which is 1/3 of the figure Howarth and Jacobson employ. The 20-year equivalency exaggerates the overall impact of methane emissions because over most of the 100-years, the methane would be long gone from the atmosphere. The next major flaw in the report conflates fugitive methane emissions from oil production rather than natural gas production. Natural gas is often vented or flared at oil wells because there is no way to get it to market and it’s therefore unwanted. We need to eliminate the use of oil for so many reasons and natural gas emissions is just one more reason to add to the list. We need to eliminate the use of oil as soon as humanly possible. Oil wells are responsible for releasing more methane emissions from subsurface wells than natural gas wells or any other type of well. This point is conflated in the Jacobson/Howarth study further eroding is credibility. It’s disingenuous to compare leakage rates at oil wells -vs- gas wells which is what the Howarth/Jacobson study does. This means the 2.6% figure for upstream emissions the Howarth/Jacobson study employs grossly overstates the emissions associated with use of natural gas. The third and most deceptive flaw of the Howarth/Jacobson paper is the sleight of hand of basing calculations on old school steam methane reforming (SMR) rather than autothermal reformation (ATR). One final flaw in the report is section 2.2 of the study calling out the extra power it would take to run the SMR process. This is actually how blue hydrogen achieves its ‘net zero’ claim through the ATR process. In section 2.2 of the Jacobson/Howarth study, Mark calculates 31.8g of CO2 are generated to create 1 MJ of hydrogen. The math is correct, but the irony again is sleight of hand in that this is the amount of CO2 saved because it will be sequestered. By capturing the CO2 that would otherwise be necessary to create the hydrogen while also having surplus power for nearby homes &amp; businesses, the blue hydrogen produced earns its “net zero” claim. The math and chemical equations in the Howarth/Jacobson paper are not scientifically accurate as laid out above. But the bigger problem is the way the study uses sleight of hand tricks to get you to look backward instead of forward. Howarth and Jacobson go to lengths calculating the chemical mole weights of carbon dioxide from making hydrogen from steam reformation. On page four of the study there’s a table to summarize how much carbon dioxide is released as flue gas in the SMR process. Let me say the same thing I said when I debunked Dr. Bossel’s pseudo-scientific paper: I don’t dispute the Howarth/Jacobson chemical calculations per se, but by plugging the wrong numbers into his equations, Mark arrives at the wrong results. RMP (2021) The Truth about Blue Hydrogen Mark Jacobsen Abstract Hydrogen is often viewed as an important energy carrier in a future decarbonized world. Currently, most hydrogen is produced by steam reforming of methane in natural gas (“gray hydrogen”), with high carbon dioxide emissions. Increasingly, many propose using carbon capture and storage to reduce these emissions, producing so-called “blue hydrogen,” frequently promoted as low emissions. We undertake the first effort in a peer-reviewed paper to examine the lifecycle greenhouse gas emissions of blue hydrogen accounting for emissions of both carbon dioxide and unburned fugitive methane. Far from being low carbon, greenhouse gas emissions from the production of blue hydrogen are quite high, particularly due to the release of fugitive methane. For our default assumptions (3.5% emission rate of methane from natural gas and a 20-year global warming potential), total carbon dioxide equivalent emissions for blue hydrogen are only 9%-12% less than for gray hydrogen. While carbon dioxide emissions are lower, fugitive methane emissions for blue hydrogen are higher than for gray hydrogen because of an increased use of natural gas to power the carbon capture. Perhaps surprisingly, the greenhouse gas footprint of blue hydrogen is more than 20% greater than burning natural gas or coal for heat and some 60% greater than burning diesel oil for heat, again with our default assumptions. In a sensitivity analysis in which the methane emission rate from natural gas is reduced to a low value of 1.54%, greenhouse gas emissions from blue hydrogen are still greater than from simply burning natural gas, and are only 18%-25% less than for gray hydrogen. Our analysis assumes that captured carbon dioxide can be stored indefinitely, an optimistic and unproven assumption. Even if true though, the use of blue hydrogen appears difficult to justify on climate grounds. Jacobsen (2021) How green is blue hydrogen? (Request pdf from authors) "],["nuclear.html", "17 Nuclear 17.1 COP28 Tripple Nuclear by 20250 17.2 Nuclear Dilemma 17.3 Fukushima 17.4 James Hansen’s Position 17.5 Fusion - ITER 17.6 Fusion - Helion 17.7 Fusion - Tiny 17.8 Fusion - Neither clean nor cheap 17.9 Thorium 17.10 Melted Salt Reactors 17.11 Uranium Supply Chain 17.12 SMR - Small Modular Reactor 17.13 Nuclear Leaks and Waste", " 17 Nuclear 17.1 COP28 Tripple Nuclear by 20250 Wesoff At COP28, many major players are banding together to plan a big ramp-up of nuclear power. But without China’s help, is the target realistic? Today’s global fleet of approximately 440 nuclear reactors has a combined capacity of around 400 gigawatts — enough that nuclear energy provides about 10 percent of the world’s power. But less than a paltry 4 gigawatts of nuclear energy has been connected to the grid in 2023. The global solar industry is forecast to install more than 400 gigawatts of capacity in 2023 alone. The goal of tripling the world’s nuclear output would require deploying an average of 40 gigawatts of nuclear power every year through 2050, according to the World Nuclear Association. (My back-of-the-envelope calculations point to an even higher number if replacing existing aged-out equipment is included in the mix.) The pledge also asks the signees to consider smaller and more innovative reactor designs in their grid planning and makes an appeal that they continue to maintain the existing reactor fleet, extending its lifetime if feasible and safe. Over the past few decades, the hefty price tag of building nuclear plants has been the industry’s Achilles’ heel. This poses particular challenges in market-based economies, where periods of high interest rates and inflation threaten the viability of mega projects. Ironically absent from the pool of signees is China, the only country with any real chance of meeting the COP goal. China aims to double its nuclear energy capacity by 2035 and is well on its way; as of this year, 22 nuclear plants are under construction in China with more than 70 planned. Perhaps the emphasis on nuclear at this year’s meeting reinforces the idea that we’re in the midst of a generational shift in sentiment about atomic power. Wesoff (2023) 20-plus countries pledge to triple the world’s nuclear energy by 2050 17.2 Nuclear Dilemma Beslik The nuclear dilemma. Nuclear power is dead. Long live nuclear power. Nuclear power is the only way forward. Nuclear power is too dangerous. Nuclear power is the safest power source around. Nuclear is nothing. Nuclear is everything. When I started looking at this I was stunned by how the same numbers and indicators can be interpreted in so fundamentally different ways. The same basic data set – nuclear plants currently in existence, those under construction, the status of new technologies, the history of costs and delays, and a few striking accidents – produces totally contradictory opinions and predictions. Nuclear power is a Rorschach test: You see what you want to see, a rosy nuclear future or an old-world dinosaur in a slow-death spiral. It’s a reflection of your own views on the energy present and future. In all likelihood, no one will be proven right or wrong for decades. The state of nuclear power Nuclear power today accounts for around 10 percent of the total electricity generation around the world. This varies sharply by country. In the U.S. the rate is about 20 percent, in Russia and Germany it is a bit lower than that, while some other European countries get 40 and 50 percent from nuclear reactors. France has long led the way proportionally, at more than 70 percent (it has the second most total reactors, behind the U.S.). China, though building rapidly, drew less than 3 percent of its power from nuclear in 2014. There are 442 reactors currently in operation globally, and the International Atomic Energy Agency says that 66 are currently under construction. Twenty-four of those are in China; no other country is currently building more than eight. Read more here and here. In general, the more experience accumulated with a given technology, the less it costs to build. This has been dramatically illustrated with the falling costs of wind and solar power. Nuclear, however has bucked the trend, instead demonstrating a sort of “negative learning curve” over time. The nuclear industry’s main hope for future expansion lies in a new generation of small modular reactors (SMRs) that generate less than 300 MW each and are amenable to assembly-line construction. These are still under development, however, with none licensed or under construction. In the U.S. nuclear power generates as much as $50 billion each year from electricity sales and revenue, and provides around 100,000 jobs. A handful of companies and governments are working to develop small-scale nuclear reactors (SMRs) that proponents say are safer, cheaper, and more compatible with renewables than traditional nuclear power. But critics contend the new technology doesn’t address concerns about safety and radioactive waste. Small modular reactors suffer from many of the same problems as large reactors, most notably safety issues and the unresolved problem of what to do with long-lived radioactive waste. And opponents say that even in a smaller form, nuclear power is expensive — it’s one of the costliest forms of energy, requiring substantial government subsidies to build and run, not to mention insure. In 80-odd years of nuclear power, in which more than 450 commercial reactors, many experimental stations and tens of thousands of nuclear warheads have been built, great stockpiles of different levels of waste have accumulated. About 0.2–3% by volume is high-level waste. Mostly derived from civil reactor fuel, this is some of the most dangerous material known on Earth, remaining radioactive for tens of thousands of years. It requires cooling and shielding indefinitely and contains 95% of the radioactivity related to nuclear power generation. Further 7% or so by volume, known as intermediate waste, is made up of things like reactor components and graphite from reactor cores. This is also highly dangerous, but it can be stored in special canisters because it does not generate much heat. Around 22,000 cubic meters of solid high-level waste has accumulated in temporary storage but not been disposed of (moved to permanent storage) in 14 western countries, along with unknown amounts in China, Russia and at military stations. A further 460,000 cubic meters of intermediate waste is being stored, and about 3.5 million cubic meters of low-level waste. Some 34,000 cubic meters of new high-level and intermediate waste is generated each year by operating civil reactors, The bitter reality is that there is no scientifically proven way of disposing of the existential problem of high- and intermediate-level waste. The truth is that whatever efforts are made to bury and forget it, it will not go away. Nuclear stands for 10% of world energy supply. Beslik 17.3 Fukushima Release of radioactive waste into sea Japan has announced it will release more than 1m tonnes of contaminated water from the wrecked Fukushima nuclear power plant into the sea, a decision that has angered neighbouring countries, including China, and local fishers. Official confirmation of the move, which came more than a decade after the nuclear disaster, will deal a further blow to the fishing industry in Fukushima, which has opposed the measure for years. The prime minister, Yoshihide Suga, told a meeting of ministers on Tuesday that the government had decided that releasing the water into the Pacific Ocean was the “most realistic” option, and “unavoidable in order to achieve Fukushima’s recovery”. The plant’s operator, Tokyo Electric Power [Tepco], and government officials say tritium, a radioactive material that is not harmful in small amounts, cannot be removed from the water, but other radionuclides can be reduced to levels allowed for release. China denounced the plan as “extremely irresponsible”, and accused Japan of reaching the decision “without regard for domestic and foreign doubts and opposition”. “This approach is extremely irresponsible and will seriously damage international public health and safety and the vital interests of the people of neighbouring countries,” the Chinese foreign ministry said in a statement on its website. South Korea summoned Japan’s ambassador, Koichi Aiboshi, the broadcaster YTN reported, while a high-level government official said Seoul “firmly opposes” the move, a view also expressed by Taiwan’s Atomic Energy Council. The US was supportive, describing Japan’s decision-making process as “transparent”. The announcement drew swift condemnation from environmental groups. About 1.25m tonnes of water has accumulated at the site. It includes water used to cool the plant, as well as rain and groundwater that seeps in daily.The radioactive water, which increases in quantity by about 140 tonnes a day, is now being stored in more than 1,000 tanks, and space at the site is expected to run out around next autumn. The International Atomic Energy Agency supports the decision, since radioactive elements, except tritium, will be removed from the water or reduced to safe levels before it is discharged. The IAEA has also pointed out that nuclear plants around the world use a similar process to dispose of wastewater. Experts say tritium is only harmful to humans in large doses and with dilution the treated water poses no scientifically detectable risk. Local fishing communities say the water’s release will undo years of hard work to rebuild consumer confidence in their seafood. Japanese officials have objected to media descriptions of the water as “contaminated” or “radioactive”, insisting that it be described as “treated”. Shaun Burnie, senior nuclear specialist with Greenpeace East Asia, said that claim was “clearly false”. “If it was not contaminated or radioactive they would not need approval (to release the water) from Japan’s nuclear regulator,” he said. “The water in the tanks is indeed treated, but it is also contaminated with radioactivity. The Japanese government has been deliberately seeking to deceive over this issue, at home and abroad.” Guardian (210413) 17.4 James Hansen’s Position James Hansen obviously thinks climate cannot be saved without nuclear power Development and deployment of 4 th generation nuclear power to the point that modular nuclear reactors provide electricity cheaper than coal. By 4 th generation I refer to all modern passively-safe reactors, whether thorium or uranium fueled – even nuclear fusion, which at long last is now closer than 50 years away. The preposterous fear of nuclear waste from power plants – which is held in containers where it harms nobody – is hyped, while poorly-contained waste from other energies is ignored. Waste from heavily-subsidized fossil fuels is spewed in the air freely. About 20,000 people per day die of outdoor and indoor air pollution – much of that pollution being waste from fossil fuel combustion. People drop like flies from that pollution – more than are killed by pandemics and wars combined – but politicians pay little heed. Even 2 nd generation nuclear power plants can be operated safely. The one serious accident in the U.S. – at Three-Mile Island – caused no deaths. Modern nuclear reactors that shut down in case of an anomaly and cool the nuclear fuel without need for external power are our safest energy. Neither the Chernobyl nor Fukushima accidents would have occurred with modern nuclear power. Passive safety features are available that allow reactor shutdown and cooling without external power or operator intervention. [James Hansen: Planet.Chapter47] 17.5 Fusion - ITER ITER was set in motion at the Geneva Superpower Summit in November 1985, when the idea of a collaborative international project to develop fusion energy for peaceful purposes was proposed by General Secretary Gorbachev of the former Soviet Union to US President Ronald Reagan. The People’s Republic of China and the Republic of Korea joined the Project in 2003, followed by India in 2005. Selecting a location for ITER was a lengthy procedure that was concluded in 2005, when the ITER Members unanimously agreed on the site proposed by the European Union. The ITER installation would be built near Aix-en-Provence in southern France. More than 200 tokamaks around the world have paved the way to the ITER experiment. Conceived as the last experimental step to prove the feasibility of fusion as a large-scale and carbon-free source of energy, ITER will be the world’s largest tokamak, with ten times the plasma volume of the largest tokamak operating today. ITER 17.6 Fusion - Helion Nuclear Fusion without Steam Kamps Fusion energy has been a fiery dream for lovers of clean energy since the first controlled thermonuclear fusion reaction was accomplished some 60 years ago. The technology promises all the benefits of current-generation nuclear fission generators, at a fraction of the risk, with far less radioactivity when running, and with very little radioactive waste. There’s been one catch: So far, it has been hard to get the fusion process to generate more energy than it has been consuming to keep the reaction under control. Helion, as a company, has been focusing less on fusion as a science experiment and more on a more important question: Can their technology generate electricity at a commercial and industrial scale? Some projects in the fusion space talk about heat, or energy, or other things. Helion is focused on electricity generation. We are building systems that are about the size of a shipping container and that can deliver industrial-scale power — say on the order of 50 megawatts of electricity. Fig: Deuterium and Helium-3 are heated, then accelerated through magnets, compressed and captured as inductive current. Animation courtesy of Helion Energy. June of this year, Helion published results confirming it had become the first private fusion company to heat a fusion plasma to 100 million degrees Celsius, an important milestone on the path to commercial electricity from fusion. Soon after, the company announced it had broken ground on building its factory to start the process of preparing for manufacturing of its seventh-generation fusion generator, which the company calls “Polaris.” TechCrunch was surprised to learn of the company’s $1.5 million round back in 2014, when the company said it would be able to get net power generation of fusion up and running within three years. Here we are seven years later, and it appears that Helion hit a couple of wobbles — but the company also found a focus along the way. We ended up pivoting a little bit in direction, to focus less on scientific milestones of energy and focus more specifically on electricity. We had to prove some of the technologies on the electricity, and electricity extraction side of things. At 50 megawatts, the generators could power around 40,000 homes, and with that amount of power, the technology could open some really interesting opportunities for distributed power grids. One interesting innovation in Helion’s power generation solution is that it doesn’t use water and steam as intermediary steps in the power generation. Instead of going via water, the company decided to skip a step and use inductive energy instead. The company is aiming to be able to generate more electricity than what it takes to run the fusion reactor by 2024. Kamps (2021) Helion commercializes fusion energy 17.7 Fusion - Tiny Wesoff The startup is spurning room-sized reactors in favor of a small, modular design. But it’s unclear when or if fusion technology — large or small — will pan out. To achieve fusion, hydrogen must be converted into plasma, a transformation that requires million-degree temperatures, much hotter than the core of the sun. In the medium of the plasma, negatively charged electrons separate from positively charged atomic nuclei. Fusion machines compress and confine the plasma in order to push the freed-up nuclei so close together that they overcome repellant electrostatic forces and ultimately fuse. This process releases neutrons, and it’s this energy that scientists dream of harvesting to do things like drive a conventional turbine. The holy grail for the fusion entrepreneur is a process that creates more energy than is needed to power it, also called net energy or positive output. A number of deep-tech startups are inching closer to this dream of prodigious energy at low cost. But right now, more energy is required to catalyze fusion than results from the process — a lot more. Many fusion startups use magnetic confinement of the plasma, where the plasma is stabilized by massive magnets and heated to extraordinary temperatures so the nuclei can fuse. Instead of magnets, Avalanche’s electrostatic approach relies on very high voltages. The startup’s prototype uses electrostatic fields to trap ions, while also employing a magnetron electron confinement technique to reach higher ion densities and increase the incidence of fusion reactions. If Avalanche can get the voltages and ion densities optimized, the resulting fusion reaction will produce neutrons that can be transformed into heat. That’s a big if. But this week, the company said it made significant progress on one piece of the puzzle. “For us, the metric is voltage, and we need to get to 300 kilovolts to get to the optimum fusion energy in our device. We hit 200 kilovolts in our lab, which is a big deal — getting that much voltage into a 12-centimeter-diameter tube,” said Langtry. “The next big milestone we’re hopefully announcing in a year or two is that we are able to densify this plasma not only at the right energy, but at densities where it’s interesting for energy generation,” he added. ​“We’ve got a lot of work to do.” The company emphasizes that its reactor’s component parts are already commercially available: Its magnetron is a variation of a microwave-oven component, and its electrostatic base technology is a derivative of another existing product. Riordan contends that Avalanche’s small scale differentiates it from other larger players in the space. The company doesn’t need a billion to make progress; it can do a lot with millions. He said the company will initially focus on hard-to-decarbonize sectors and niche uses of its tech. ​“We’re not trying to do grid-scale energy,” Riordan told Canary last year. ​“There’s a whole realm of industries that need to be decarbonized: long-distance trucking, aviation, maritime — huge carbon sources.” As cost comes down, markets will open up, said Langtry. “We do think space and defense is probably the first application for what we’re trying to do,” said the CEO. Wesoff (2023) Avalanche raises $40M to pursue vision of tiny nuclear fusion reactor 17.8 Fusion - Neither clean nor cheap Jassby The harsh realities of fusion belie the claims of its proponents of “unlimited, clean, safe and cheap energy.” Terrestrial fusion energy is not the ideal energy source extolled by its boosters, but to the contrary: It’s something to be shunned. Fusion reactions in the sun burn ordinary hydrogen at enormous density and temperature, sustained by an effectively infinite confinement time, and the reaction products are benign helium isotopes. Artificial (terrestrial) fusion schemes, on the other hand, are restricted to much lower particle densities and much more fleeting energy confinement, and are therefore compelled to use the heavier neutron-rich isotopes of hydrogen known as deuterium and tritium—which are 24 orders of magnitude more reactive than ordinary hydrogen. This gargantuan advantage in fusion reactivity allows human-made fusion assemblies to be workable with a billion times lower particle density and a trillion times poorer energy confinement than the levels that the sun enjoys. Unlike what happens in solar fusion—which uses ordinary hydrogen—Earth-bound fusion reactors that burn neutron-rich isotopes have byproducts that are anything but harmless: Energetic neutron streams comprise 80 percent of the fusion energy output of deuterium-tritium reactions and 35 percent of deuterium-deuterium reactions. An energy source consisting of 80 percent energetic neutron streams may be the perfect neutron source, but it’s truly bizarre that it would ever be hailed as the ideal electrical energy source. In fact, these neutron streams lead directly to four regrettable problems with nuclear energy: radiation damage to structures; radioactive waste; the need for biological shielding; and the potential for the production of weapons-grade plutonium 239. If fusion reactors are indeed feasible—as assumed here—they would share some of the other serious problems that plague fission reactors, including tritium release, daunting coolant demands, and high operating costs. There will also be additional drawbacks that are unique to fusion devices: the use of a fuel (tritium) that is not found in nature and must be replenished by the reactor itself; and unavoidable on-site power drains that drastically reduce the electric power available for sale. All of these problems are endemic to any type of magnetic confinement fusion or inertial confinement fusion reactor that is fueled with deuterium-tritium or deuterium alone. (As the name suggests, in magnetic confinement fusion, magnetic and electrical fields are used to control the hot fusion fuel—a material that takes an unwieldy and difficult-to-handle form, known as a plasma. In inertial confinement, laser beams or ion beams are used to squeeze and heat the plasma.) The deuterium-tritium reaction is favored by fusion developers because its reactivity is 20 times higher than a deuterium-deuterium fueled reaction, and the former reaction is strongest at one-third the temperature required for deuterium-only fusion. In fact, an approximately equal mixture of deuterium and tritium may be the only feasible fusion fuel for the foreseeable future. While deuterium is readily available in ordinary water, tritium scarcely exists in nature, because this isotope is radioactive with a half-life of only 12.3 years. The main source of tritium is fission nuclear reactors. The tritium consumed in fusion can theoretically be fully regenerated in order to sustain the nuclear reactions. To accomplish this goal, a lithium-containing “blanket” must be placed around the reacting medium—an extremely hot, fully ionized gas called a plasma. The neutrons produced by the fusion reaction will irradiate the lithium, “breeding” tritium. To make up for the inevitable shortfalls in recovering unburned tritium for use as fuel in a fusion reactor, fission reactors must continue to be used to produce sufficient supplies of tritium—a situation which implies a perpetual dependence on fission reactors. In addition to the problems of fueling, fusion reactors face another problem: they consume a good chunk of the very power that they produce, or what those in the electrical generating industry call “parasitic power drain,” on a scale unknown to any other source of electrical power. Below a certain size (about 1,000 MWe) parasitic power drain makes it uneconomic to run a fusion power plant. The problem of neutron-degraded structures may be alleviated in fusion reactor concepts where the fusion fuel capsule is enclosed in a one-meter thick liquid lithium sphere or cylinder. But the fuel assemblies themselves will be transformed into tons of radioactive waste to be removed annually from each reactor. Molten lithium also presents a fire and explosion hazard. If reactors can be made to operate using only deuterium fuel, then the tritium replenishment issue vanishes and neutron radiation damage is alleviated. But the other drawbacks remain—and reactors requiring only deuterium fueling will have greatly enhanced nuclear weapons proliferation potential. Jassby (2017) Fusion reactors: Not what they’re cracked up to be 17.9 Thorium Tekna Energibloggen Norge har atter engang trukket vinnerloddet! Vi sitter på nok thorium til å erstatte all norsk energiproduksjon i over 2000 år. Dette er også en teknologi som kan kutte alle utslipp fra store skip og kutte kostnadene samtidig. Norge kan således også ta igjen vårt tapte marked innen deep-sea og større skip, generelt. Mulighetene er enorme, men det kommer ikke av seg selv. Saltsmeltereaktorer Foredraget handler blant annet om en teknologi innen kjernekraft – saltsmeltereaktorer – som ble først utviklet på 1960-tallet men lagt i skuffen under den kalde krigen da teknologien ikke produserte materialer for våpen. Dette skjedde til tross for at det amerikanske kjernekraftkommisjonen anså dette som en meget lovende teknologi, og 3 år med feilfri drift av en prototype bekreftet dette – den har faktisk ingen av de risikoelementene vi tradisjonelt forbinder med kjernekraft. Teknologien er også mye billigere enn tradisjonelle kjernekraftverk og den er mye enklere å skalere fra smått til stort. Dessverre ble teknologien ansett for å ikke ha noen sikkerhetspolitisk betydning og lagt i skuffen, men etter 9/11 ble denne teknologien tatt frem fra støvet igjen av samme grunn – trygghet og ikke-spredning. I dag er teknologien en av de mest innovative teknologiene som finnes innen energiområdet. Dette er reaktorer som kan bygges så små som 1 MW men også så store som 3.000 MW, og blant de 67 forskjellige små reaktordesignene som nå utvikles, er saltsmeltereaktorer en av de mest interessante. Det er fordi saltsmeltereaktorer kan løse verdens utslippsproblematikk svært kostnadseffektivt med forventet kostnadsnivå på kun 25 øre/kWh, der er ingen risikoelementer som man tradisjonelt forbinder med kjernekraft, og den har lave investeringskostnader. Tekna (2021) Thorium og saltsmeltereaktorer 17.10 Melted Salt Reactors Seaborg I utgangspunktet vil Seaborg bruke et uranbrensel som kalles HALEU – High-assay low-enriched uranium. Det inneholder mellom 5 og 20 prosent U235. Vi vil blande uranet med fluor og lage urantetrafluorid, som vi så blander med andre salter basert på Na og Ka. De vil senke smeltepunktet til rundt 500 grader. Den høye berikningen av U235 er kostbar, men vi slipper å bruke brenselstaver, som er veldig kostbart. På grunn av den høye temperaturen i det smeltede saltet kan reaktorene produsere en variabel kombinasjon av strøm og varme eller produsere hydrogen. Det er i selve reaktoren at atomreaksjonen finner sted og varmen genereres. Den hentes ut via varmeveksling til nok en saltkrets som ikke inneholder radioaktive stoffer. Så varmeveksles den videre til en vannkrets med 200 bars trykk som produserer damp til en turbin som driver en generator. Alternativt kan energien tas ut som varme, eller noe midt imellom. Når reaktoren er fylt med det radioaktive saltet, vil den kunne gå kontinuerlig i tolv år uten tilførsel av mer saltbrensel. Da er det meste av det fissile materialet brukt opp, og resten vil være resirkulerbart. Det vil si at det kan gjenbrukes i en annen reaktor slik at mer energi kan hentes ut. Avfallets behov for lagringstid blir redusert til 300 år. I forhold til det som skal lagres, er det veldig små volumer sammenlignet med energien som produseres. Og den raske halveringstiden gjør lagringen enklere. Sammenlignet med tradisjonelle reaktorer er Seaborgs anlegg mye enklere, og de slipper kravene til kompliserte sikkerhetssystemer som driver opp prisen på konvensjonelle kjernekraftverk. I stedet for å bruke grafitt som moderator, bruker vi natriumhydroksid. Reaktoren har en driftstemperatur på i underkant av 600 grader. Det er ideelt til dampproduksjon og gjør at de kan bruke standard dampturbiner. Hvis reaktoren skal produsere hydrogen, blir det veldig effektivt mellom 800 og 900 grader. Selve reaktorkjernen er utformet som en sylinder med en diameter på to meter og høyde to meter. Når den er i drift må alt som inneholder oksygen holdes utenfor reaktoren. Den flytende steinen som saltet i praksis er, krever høy temperatur for å være flytende. Skulle noe skje, vil det stivne under 490 grader. Reaktoren kan ikke smelte ned eller eksplodere eller lekke radioaktive gasser til luft eller vann. Og teknologien kan ikke brukes for å lage våpen. Når saltet er flytende, er det også mulig å benytte såkalte ispropper, det vil si et materiale med høyere smeltepunkt. Skulle reaktortemperaturen blir for høy, vil proppen smelte og saltet renne ut i flere beholdere hvor kjernereaksjonene stopper. Seaborg vil installere reaktorene på båter eller flåter. Det er plass til mellom to og åtte reaktorer i en flåte, hver reaktor er på 100 MW. På grunn av monteringen på flåter kan leveringstiden reduseres til tre år. Teknisk Ukeblad (2022) 17.11 Uranium Supply Chain Tooze Rosatom: uranium supplier to the world Unlike Western companies in the nuclear business, Rosatom is involved in every part of the supply chain, from ore extraction to fuel enrichment and delivery. The company is as much an expression of the Kremlin’s geopolitical power as a profit-generating business. That state-level commitment has played to Russia’s advantage. When international investors turned away from nuclear power following the Fukushima accident in 2011, some Western companies involved in the fuel cycle, including Avrea SA in France, the US Enrichment Co and Westinghouse Electric Co, went bankrupt. Russia stepped in, building market share not only among the world’s existing fleet of nuclear reactors, but by offering generous financing for new foreign projects. Today, Rosatom’s 330,000 workers provide fuel assemblies to scores of old reactors in eastern Europe and Russia, and is building 33 new power units in 10 countries, including China and India, that will be locked into fuel contracts for decades ahead. Russia supplies almost as much enriched uranium to US reactors as the US generates itself; Tooze (2023) Rosatom: uranium supplier to the world 17.12 SMR - Small Modular Reactor Barnard SMR doesn’t have the conditions for success to be built and scaled economically in the 21st Century, and wind, water, solar, transmission and storage do. Nuclear energy and free markets aren’t compatible. Nuclear programs are state programs with subordinate corporate partnerships. No country globally has the conditions for success for nuclear generation in the 21st Century. That was a Cold War era success story based on a hyperawareness of the threat of nuclear war which is vastly diminished in the age of trade. What are small modular nuclear reactors? The name kind of gives it away. The vastly majority of the 450 or so nuclear reactors in the world that are generating electricity that gets put into the grid are gigawatt scale. That’s because attempts in the 1950s and 1960s to generate electricity with the scale of reactors on nuclear subs and aircraft carriers found that they were really expensive at that size. But gigawatt scale reactors are easy to turn into bespoke engineering megaprojects. Custom engineering for every site foregoes economies of manufacturing scale. So the SMR crowd decided to ignore most of history’s lessons about both the scale of reactors for commercial success and the conditions for success and lean into tiny reactors and lots of numbers. The hope was that Wright’s Law — where every doubling of the number of manufactured items once in production manufacturing would bring cost per item down by 20% to 27% — would enable them to be manufactured and deployed cheaply. However, the doubling requires an awful lot of reactors, and only under the most unrealistically optimistic of scenarios are they in the price range of wind and solar today by 2040. The last condition for success was repeating the deployment a bunch of times, but historically that’s been dozens, not thousands. Rigorously repeating deployment of the same proven design with skilled teams in a relatively short timeframe with lots of national control and military rigour isn’t a feature of the SMR bandwagon and proposals. The SMR crowd think that there’s a free market friendly version of nuclear generation. They mostly ignore the seven layers of overlapping security required for any commercial nuclear generation solution. They ignore the thermal and hence cost efficiencies of scale. They ignore the advantages of proven, simpler technologies in favour of novelty. They ignore the lack of military interest in a modern strategic nuclear program. They ignore that they don’t have a major economy and geography backing a winner and forcing it to success, no matter what it costs. And clearly they don’t pay much attention to modern competitors like wind and solar energy. There’s almost nothing new in the SMR technologies and designs by the way. Some of them were operational in labs in the 1950s. Most of the technologies were never commercialized anywhere despite various attempts. Their claims about being safer are pretty meaningless as passive safety features exist on lots of the operational GW scale reactors and no one is building unsafe reactors anymore. One thing that is relatively new is that most or perhaps all of the designs require high assay low enrichment uranium (HALEU). That would be fine, except that Russia had effectively been ceded a monopoly on the supply chain for the fuel. The US DOE (again) has created a consortium model to try to create a reasonably large domestic supply of the material, well above the scale currently available domestically for laboratory reactors. The Fukushima disaster, which will approach a trillion USD in economic and clean up costs for Japan by the time the dust has finally settled decades from now. If SMRs made sense, existing nuclear power generation facilities are the place to build them. They are already at the centre of the seven overlapping layers of defence that nuclear generation sites require from the international, all supply and waste chains and the physical and electronic security of the facility itself. Barnard (2023) What Drives This Madness On Small Modular Nuclear Reactors? 17.13 Nuclear Leaks and Waste 17.13.1 Sellafield Isaac Ministers who visit Sellafield for the first time are left with no illusions about the challenge at Europe’s most toxic nuclear site. One former UK secretary of state described it as a “bottomless pit of hell, money and despair”, which sucked up so much cash that it drowned out many other projects the economy could otherwise benefit from. Sellafield, a huge nuclear dump on the Cumbrian coast in north-west England, covers more than 6 sq km (2 sq miles). It dates to the cold war arms race, and was the original site for the development of nuclear weapons in the UK in 1947, manufacturing plutonium. It was home to the world’s first full-scale commercial nuclear power station, Calder Hall, which was commissioned in 1956 and ceased generating electricity in 2003. Fig: Sellafield is at the heart of the so-called “nuclear coast” in West Cumbria, sandwiched between the Lake District national park and the Irish Sea. At its southern end, BAE Systems in Barrow-in-Furness builds nuclear submarines. Land neighbouring the site has long been earmarked for a new nuclear power station but plans for Moorside collapsed in 2018 when the Japanese conglomerate Toshiba walked away. It has been at the centre of disaster and controversy, including the Windscale fire of 1957. The blaze was considered one of the worst nuclear incidents in Europe at the time, and carried a plume of toxic smoke across to the continent. The milk from cows on 200 sq miles of Cumbrian farmland was condemned as radioactive. Sellafield began receiving radioactive waste for disposal in 1959, and has since taken thousands of tons of material, from spent fuel rods to scrap metal, which is stored in concrete silos, artificial ponds and sealed buildings. A constant programme of work is required to keep its crumbling buildings safe and create new facilities to contain the toxic waste. The site is expected to be in operation until at least 2130. The estimated cost of running and cleaning up the site have soared. Sellafield is so expensive to maintain that it is considered a fiscal risk by budgetary officials. The latest estimate for cleaning up the Britain’s nuclear sites is £263bn, of which Sellafield is by far the biggest proportion. However, adjustments to its treatments in accounts can move the dial by more than £100bn, more than the UK’s entire annual deficit. The cost of decommissioning the site is a growing liability that does not count towards the calculation of the UK’s net debt. The site has a workforce of 11,000, with its own railway, road network, laundry services for normal and potentially radioactive garments, and its own police force with more than 80 dogs. It has almost 1,000 buildings. Sellafield’s impact on the environment has been a longstanding concern. Local animals, including swallows, have been found to carry radioactive traces from the site with them. Debate rages locally over just how toxic the “atomic kittens” – stray cats that inhabit the site – may be. Sellafield says cats are screened for radioactivity before they are rehomed. The activities at the site are a matter of significant scrutiny to countries including the US, Norway and Ireland, given that Sellafield hosts the largest store of plutonium in the world and takes waste from countries such as Italy and Sweden. Norwegians have long feared the effects of an accident at the site, with modelling suggesting that prevailing south-westerly winds could carry radioactive particles from a large incident at the site across the North Sea, with potentially devastating consequences for its food production and wildlife. Norway and Ireland were involved in efforts to halt the release of technetium-99, a radioactive metal, into the sea by Sellafield. In 2003, Norway accused Sellafield of ruining its lobster business. Isaac (2023) Sellafield: ‘bottomless pit of hell, money and despair’ at Europe’s most toxic nuclear site "],["ocean-turbines.html", "18 Ocean turbines", " 18 Ocean turbines Elkær Det japanske selskapet IHI Corp etter ti års forsøk nå er klart til sende en havturbin på 330 tonn ned i de opprørte farvannene der det teoretisk kan høstes opp mot 205 gigawatt miljøvennlig energi. Det svarer til effekten av mer enn 13 000 kraftige vindturbiner på land, og det kan fôre opp til hundre millioner moderne husstander med energi. Hhavstrømmer har historisk vist seg å være vanskelige å temme slik at de kan overføre elektrisitet til PC-er, TV-er og lamper. Likevel kan IHI Corps 330 tonn tunge prototype – med navnet Kairyu, det japanske ordet for ’havstrømmer’ – være første skritt mot å bygge et nettverk av kraftverk under havoverflaten. Kairyu blir senket i havet, der den blir festet til havbunnen og flyter om lag femti meter under havoverflaten. Det tjue meter lange skroget kan dreie i den mest energieffektive retningen slik at de kraftige havstrømmene bruser forbi turbinens elleve meter lange blader, som produserer hundre kilowatt energi. Når de undersjøiske uhyrene kobles sammen i et nettverk, på samme måte som i en havvindpark, vil effekten kunne nå 205 gigawatt. I havstrømmer kan man nemlig bare utvinne 50 til 70 prosent av det teoretiske potensialet. Det slår imidlertid vindenergi, der tallet er om lag 30 prosent, og solenergi, med 15 prosent av potensialet. Japan har i årevis satset på alternative energikilder, og i dag har landet den tredje største solenergiproduksjonen, etter Kina og USA. Ifølge energitidsskriftet NS Energy produserte Japan i 2021 i alt 63,2 gigawatt solenergi, noe som svarer til en tredjedel av det fulle potensialet til de undersjøiske strømmene. Vindturbiner har derimot aldri vært den store hiten i det kuperte landet, som siden ulykken på Fukushima-atomkraftverket etter et jordskjelv i 2011 har lett etter flere alternative energikilder. Til sammenligning produserte Fukushima-kraftverket om lag fem gigawatt, mens verdens største atomkraftverk, Kashiwazaki-Kariwa, produserte om lag åtte gigawatt ved full kapasitet. Men også dette anlegget er i dag stengt på grunn av jordskjelvfare. Elkær (2022) Havturbin på 330 tonn gir håp om endeløs grønn energi "],["oil.html", "19 Oil 19.1 Carbon Intensity of Crude Oil Production", " 19 Oil The global oil and gas industry consumes 3-4% of global primary energy supply to extract, transport, and refine energy products. 19.1 Carbon Intensity of Crude Oil Production Mansadi Abstract The goals of the Paris Agreement pose challenges to the oil and gas sector given the need to meet energy demand globally while limiting greenhouse gas (GHG) emissions. We preliminarily quantify the heterogeneity of crude oil well-to-refinery carbon intensities (CIs) by performing field-by-field life-cycle analysis (LCA) of nearly 9,000 global oilfields representing ~98% of 2015 worldwide crude oil production. The global volume-weighted average upstream CI estimate is 10.3 g CO 2 eq./MJ crude oil, with country-level emissions ranging from 3.3 to 20.3 g CO 2 eq./MJ. Gas flaring and thermal extraction of heavy crude oils are the two major drivers of high GHG intensities. Global methane venting and fugitive emissions are poorly documented, yet evidence suggests they can increase the CI estimates considerably. Upstream gas management strategies alone could potentially mitigate ~18 Gt CO 2 eq in the 21 st century. Policy insights from this analysis regarding resource management, resource prioritization and emerging technologies could enable a reduction in the GHG footprint from the oil and gas industry. Masnadi (2018) Global carbon intensity of crude oil production (pdf) (pdf SM) OPGEE Estimation Tool "],["renewable.html", "20 Renewable 20.1 Green energy: A Dead End", " 20 Renewable Solar and wind potential is far higher than that of fossil fuels and can meet global energy demand many times over, unlocking huge benefits for society. With current technology and in a subset of available locations we can capture at least 6,700 PWh p.a. from solar and wind, which is more than 100 times global energy demand. Opportunities unlocked The collapse in renewable costs in the last three years means that half of this solar and wind technical potential now has economic potential, and by the end of the decade it will be over 90%. The land required for solar panels alone to provide all global energy is 450,000 km2, 0.3% of the global land area of 149 million km2. This differs by country as highlighted below. carbonTracker 20.1 Green energy: A Dead End Tverberg Green energy is itself a dead end, but subsidizing green energy can temporarily hide other problems. Green energy sounds appealing, but it is terribly limited in what it can do. Green energy cannot operate agricultural machinery. It cannot make new wind turbines or solar panels. Green energy cannot exist without fossil fuels. It is simply an add-on to the current system. The reason why we hear so much about green energy is because making people believe that a green revolution is possible provides many temporary benefits. For example: The extra debt needed to subsidize green energy indirectly increases GDP. (GDP calculations ignore whether added debt was used to produce the added goods and services counted as GDP.) Manufacturers can pretend that their products (such as vehicles) will operate as they do today for years and years. The educational system is given many more areas to provide courses in. Citizens are given the hope that the economy will grow endlessly. Young people are given hope for the future. Politicians look like they are doing something for voters. Unfortunately, by the time that the debt comes due to pay for subsidized green energy, it will be apparent that the return on this technology is far too low. The overall system will tend to collapse. Green energy is only a temporary Band-Aid to hide a very disturbing problem. Its impact is tiny and short-lived. And it cannot prevent climate change. Tverberg (2023) Today’s energy bottleneck may bring down major governments "],["solar.html", "21 Solar 21.1 Software drives Solar Costs 21.2 Efficiency 21.3 Mediterranean 21.4 Solar Waste", " 21 Solar 21.1 Software drives Solar Costs Software will eat solar: Driving utility-scale solar prices below 1 cent per kilowatt-hour by 2025 Terabase Energy is on a mission to get utility-scale solar power prices below $0.01 per kWh by 2025 using software, as opposed to the DOE’s hardware strategy. Terabase Energy is on a mission to drive down utility-scale solar power prices to less than $0.01 per kilowatt-hour by 2025, by using software, automation and modeling to optimize power-plant operation. The VC-funded startup just acquired another startup to help realize that goal. Terabase’s aggressive cost target far exceeds the U.S. DOE’s SunShot 2030 goal of $0.03 per kWh for utility-scale photovoltaics by 2025. The DOE’s most recent solar funding went toward hardware improvements in nonsilicon solar approaches such as perovskites, cadmium-telluride thin films and next-generation concentrated solar power. The DOE target cuts the cost of solar energy by 60% within the next 10 years. But the 82% reduction in solar cost over the last decade (according to the International Renewable Energy Agency) came from economies of scale, better technology and supply chains at largely silicon-based solar plants, not the alternative technologies being funded by the DOE. And even silicon module pricing might be nearing the bottom of the cost curve: “The era of ever-declining solar module prices is largely behind us,” according to Yan Zhuang, president of Canadian Solar’s manufacturing operation, as reported in pv magazine. Global commodities such as aluminum and glass are a larger part of the solar module bill of materials. Software is eating solar It’s software, not hardware, that’s going to drive down utility-scale solar costs, according to Terabase, a startup that closed a $6 million Series A round late last year and has already made a small acquisition to further develop its platform. Terabase just acquired REPlant Solutions, a spinout of First Solar that makes solar power plant controls and has developed a 1.5-kilovolt direct-current (DC) architecture and a DC trunk bus. REPlant’s plant controller operates the solar power plant, acting as the ringmaster in an increasingly complicated process. Matt Campbell, Terabase CEO, tells Canary Media: “In the future…PV plants will become even more demanding, requiring sophisticated plant controls, the integration of storage, and hybridization with wind and other forms of generation.” The acquisition of REPlant adds advanced supervisory control and data acquisition (SCADA) and other controls systems to Terabase’s toolbox. REPlant has an installed base of more than 10 gigawatts across 80 solar plants. Big solar is more manufacturing than construction While solar hardware has gotten cheaper, soft costs are still stubborn and represent a more significant piece of the total project cost (a similar situation to the residential and commercial solar segments). “We have to keep fighting to reduce the cost,” said Campbell. “There’s a big difference between solar at 1 cent per kWh versus 1.5 cents per kWh.” In an earlier interview, Campbell said that he wants to eliminate the construction mindset: “Utility-scale solar is more manufacturing than construction, with tens of thousands of identical units. It’s not complex like a dam. It’s just big. […] It needs to be managed like row-crop farming and more of a modern, integrated supply chain. Most projects are still managed on Excel spreadsheets.” The way to get to super-cheap solar, according to the CEO, is with smart software “across the whole life cycle” — from procurement, to oversight of construction, to operations — “all on a common interconnected digital platform.” Terabase recently provided digital and engineering services for the 800-megawatt Siraj-1 solar power plant in Qatar, which will sell its power for $0.01449/kWh. (These low utility-scale solar cost numbers tend to happen in global markets with lower labor costs than the U.S.) CanaryMedia: Software drives Solar 21.2 Efficiency Stevenson To make a really efficient device, it is tempting to pick a material that absorbs all the Sun’s radiation – from the high-energy rays in the ultraviolet, through to the visible, and out to the really long wavelengths in the infrared. That approach might lead you to build a cell out of a material like mercury telluride, which converts nearly all of the Sun’s incoming photons into current-generating electrons. But there is an enormous price to pay: each photon absorbed by this material only produces a tiny amount of energy, which means that the power generated by the device would be pitiful. A better tactic is to pick a semiconductor with an absorption profile that optimizes the trade-off between the energy generated by each captured photon and the fraction of sunlight absorbed by the cell. A material at this sweet spot is gallium arsenide (GaAs). Also used in smartphones to amplify radio-frequency signals and create laser-light for facial recognition, GaAs has long been one of the go-to materials for engineering high-efficiency solar cells. These cells are not perfect, however – even after minimizing material defects that degrade performance, the best solar cells made from GaAs still struggle to reach efficiencies beyond 25%. Further gains come from stacking different semiconductors on top of one another, and carefully selecting a combination that efficiently harvests the Sun’s output. This well-trodden path has seen solar-cell efficiencies climb over several decades, along with the number of light-absorbing layers. Both hit a new high last year when a team from the National Renewable Energy Laboratory (NREL) in Golden, Colorado, unveiled a device with a record-breaking efficiency of 47.1% – tantalizingly close to the 50% milestone (Nature Energy 5 326). Until then, bragging rights had been held by structures with four absorbing layers, but the US researchers found that six is a “natural sweet spot”, according to team leader John Geisz. Getting this far has not been easy, because it is far from trivial to create layered structures from different materials. High-efficiency solar cells are formed by epitaxy, a process in which material is grown on a crystalline substrate, one atomic layer at a time. Such epitaxial growth can produce the high-quality crystal structures needed for an efficient solar cell, but only if the atomic spacing of each material within the stack is very similar. This condition, known as lattice matching, restricts the palette of suitable materials: silicon cannot be used, for example, because it is not blessed with a family of alloys with similar atomic spacing. Devices with multiple materials – referred to as multi-junction cells – have traditionally been based on GaAs, the record-breaking material for a single-junction device. A common architecture is a triple-junction cell comprising three compound semiconductors: a low-energy indium gallium arsen­ide (InGaAs) sub-cell, a medium-energy sub-cell of GaAs and a high-energy sub-cell of indium gallium phosphide (InGaP). In these multi-junction cells, current flows perpendicularly through all the absorbing layers, which are joined in series. With this electrical configuration, the thickness of every sub-cell must be chosen so that all generate exactly the same current – otherwise any excess flow of electrons would be wasted, reducing the overall efficiency. Stevenson 21.3 Mediterranean Tooze The Mediterranean has always been a conduit for energy. From the days of Roman dominance to the nineteenth century it was manpower in the form of slaves. Today it is mostly natural gas. Half-a-dozen pipelines connect Europe to Africa and the Middle East. The eu depends on the region for over a third of its natural-gas imports. In the age of renewable energy, countries on the Med boast some of the best conditions on Earth for harvesting natural forces. Solar capacity shows vast potential (see map). Spain basks in a daily average of 4.6 kilowatt-hours (kwh) of sunlight per square metre and Morocco in 5.6kwh, double what Germany can expect. Sparse populations mean that Spain and Portugal have ample land for such plants, as do the deserts of north Africa and the Middle East. In parts of Morocco and Mauritania both sun and wind are abundant, forming rare sweet spots where electrolysers can run virtually non-stop. “There are only ten such locations around the world,” explains Benedikt Ortmann, who runs the solar business of BayWa, a German energy and construction company. Tooze (2023) The Mediterranean Energy Hub 21.4 Solar Waste Smith We’ve switched to panel technologies that don’t contain most of these chemicals: US state health departments list a range of potential toxins in solar panels: arsenic, gallium, germanium, and hexavalent chromium. Except, most panels are crystalline silicon or cadmium telluride (CdTe), which don’t have arsenic, gallium, germanium, or hexavalent chromium in them. More information on where some of these claims might come from is in the footnote.5 The only health concern from solar panels is the small amounts of lead in silicon panels and trace amounts of cadmium in CdTe ones. The International Energy Agency flags these as the only potential human health risk too. In fact, engineers are working at removing the small amounts of lead from solar panels as well. Once again, solar skeptics are inveterate techno-pessimists, and consistently underestimate humanity’s ability to innovate around early technological hurdles. Solar waste problem is not going to be a deal-breaker. Smith (2023) At least five things for your Thanksgiving weekend "],["ocean-wave-power.html", "22 Ocean Wave Power", " 22 Ocean Wave Power Canary Media Many have tried to harness the ocean’s power to generate electricity, resulting in several embarrassing failures and little tangible success. Startup Eco Wave Power hopes to transcend that tempestuous legacy with a radically simple idea: capture the ocean’s energy closer to shore. This approach sacrifices the greater energy potential of larger offshore waves, but it avoids the destruction such waves wreak on machinery. Eco Wave Power installs and services its equipment on breakwaters or seawalls, eliminating the need for ships and divers. Commercializing dockside wave energy technology that has supplied power to the grid in Gibraltar for six years. The ocean packs a punch. The massive quantities of kinetic energy delivered in the form of waves could easily power the whole world, according to the folks who bother to tally that sort of hypothetical. But that presupposes that humans can tame Poseidon’s chaos. Attempts to capture this vast supply of energy have been the undoing of many well-meaning technological ventures. In 2014, Oceanlinx attempted a 12-month demonstration of a 1-megawatt wave power unit in Australian waters. Much of the AUD $6.6 million project was funded by the Australian government, which resorted to a cryptic use of passive voice to document the outcome: Complications were experienced during transportation of the device, 24 hours into the operation. The device was set down in shallow waters off the Fleurieu Peninsula in South Australia. As a result of the transportation complications, the device was damaged beyond repair. Scottish company Pelamis tried for a decade and managed to install a giant ​“sea-snake” contraption off of Scotland and in a short-lived ​“wave park” off the coast of Portugal in 2008. The 2.25-megawatt Portugal project cost $12.9 million and had to be pulled out of the water almost immediately due to technical problems. Pelamis ultimately ran out of money in 2014. In the U.S., Verdant Power has been trying to wring tidal energy out of the relatively mild waters of the East River since 2002. Early versions of the company’s device suffered from machinery being damaged by the very current it was supposed to be catching. As of summer 2021, this company had a three-turbine demonstration unit sending power to the New York City grid. The problem here, from a business standpoint, is that the output of a wave power generator isn’t anything special — it’s clean electricity, which can be produced cheaply by wind or solar plants. The floaters that Eco Wave Power installs are cheap but sturdy. Their bobbing motion pumps hydraulic fluid into a collector tank onshore. The collector releases the pressurized fluid to turn a motor and generate electricity. If a major storm comes, the floaters lift out of the waves to avoid damage. Instead of spending millions on an initial grid-connected project, Eco Wave Power built its Gibraltar unit for $450,000. Offshore designs typically need to anchor to the ocean floor, which creates an environmental disturbance. Braverman envisions her technology clinging to the miles of seawalls and breakwaters that protect harbors and coastal cities. “We don’t take prime real estate, we don’t take beaches, we don’t take surf zones,” Braverman said. ​“You’re not going to put a hotel on the breakwater.” Those human-built structures already exist, and many will likely be expanded in coming years to fortify against rising sea levels. Canary Media "],["wind.html", "23 Wind 23.1 Installed Capacity Megawatts 23.2 Icing 23.3 Bladeless 23.4 Power Supply Correlation Challenges", " 23 Wind 23.1 Installed Capacity Megawatts Country MegaWatts United Kingdom 10,383 China 8,990 Germany 7,747 Netherlands 2,500 Belgium 2,254 Denmark 1,701 Sweden 203 South Korea 136 Taiwan 128 Vietnam 99 Finland 73 Japan 65 United States 29 Canary Media (2020) New York takes early lead as large-scale offshore wind starts rolling in the US 23.2 Icing Icing can cost wind turbines up to 80% of power production Wind turbine blades spinning through cold, wet conditions can collect ice nearly a foot thick on the yard-wide tips of their blades. That disrupts blade aerodynamics. That disrupts the balance of the entire turbine. And that can disrupt energy production by up to 80 percent. TechExplore 23.3 Bladeless David Yáñez, the inventor of Vortex Bladeless, based just outside Madrid, has pioneered a turbine design that can harness energy from winds without the sweeping white blades considered synonymous with wind power. The design recently won the approval of Norway’s state energy company, Equinor, which named Vortex on a list of the 10 most exciting startups in the energy sector. Equinor will also offer the startup development support through its tech accelerator programme. The bladeless turbines stand at 3 metres high, a curve-topped cylinder fixed vertically with an elastic rod. To the untrained eye it appears to waggle back and forth, not unlike a car dashboard toy. In reality, it is designed to oscillate within the wind range and generate electricity from the vibration. The turbine is no danger to bird migration patterns, or wildlife, particularly if used in urban settings. For the people living or working nearby, the turbine would create noise at a frequency virtually undetectable to humans. Vortex is not the only startup hoping to reinvent wind power. Alpha 311, which began in a garden shed in Whitstable, Kent, has begun manufacturing a small vertical wind turbine that it claims can generate electricity without wind. The 2 metre turbine, made from recycled plastic, is designed to fit on to existing streetlights and generate electricity as passing cars displace the air. Perhaps the most ambitious divergence from the standard wind turbine has emerged from the German startup SkySails, which hopes to use an airborne design to harness wind power directly from the sky. SkySails makes large fully automated kites designed to fly at altitudes of 400 metres to capture the power of high-altitude winds. During its ascent the kite pulls a rope tethered to a winch and a generator on the ground. The kite generates electricity as it rises into the sky and, once completely unspooled, uses only a fraction of the electricity generated to winch back towards the ground. Today, the design can generate a maximum capacity of 100 to 200 kilowatts, but a new partnership with the German energy firm RWE could increase the potential output from kilowatts to megawatts. A spokesperson for RWE said the pair are currently looking for the ideal kite-flying site in the German countryside. Guardian 23.4 Power Supply Correlation Challenges Hjelmeland Abstract Offshore wind power projects are currently booming around the North Sea. However, there are inherent correlation challenges between wind farms in this area, which has implications for the optimal composition of locations and the scale-up of installed capacities. This paper is aimed at addressing the correlation problem by minimizing the variance of total wind power accumulated around the North Sea. We show that this nonlinear convex optimization problem can be solved by applying the Augmented Lagrangian Algorithm (ALA). The premise of the study is that more interconnections between the EU countries will be prioritized in order to optimize and smooth out the wind power production patterns. A publicly available dataset with historical hour-by-hour data spanning over 20 years was used for the analysis. We explore two distinct scenarios for Norwegian offshore wind development. In the first scenario, we consider the ongoing activities on the European continental side of the North Sea and their implications for Norway. Here, we illustrate the advantages of focusing on expanding wind power capacity in the northern regions of Norway to enhance the overall value of the generated wind power. In contrast, the second reference scenario neglects these interconnections, resulting in a significantly greater concentration of offshore wind development in the southern parts of Norway, particularly in Sørlige Nordsjø II. Additionally, our work estimates the wind power correlation coefficient in the North Sea as a function of distance. Furthermore, we analyze deviations and intermittencies in North Sea wind power over various time intervals, emphasizing that the perceived integration challenges are highly dependent on the chosen time resolution in the analysis. Hjelmeland Memo The scientific literature lacks detailed studies regarding the ecological risks of offshore wind power. There are several initiatives underway to coordinate the development of offshore wind around the North Sea. For instance, the North Sea Energy Cooperation (NSEC) by the European Commission supports and facilitates the development of offshore grid development and the large potential for renewable energy in the region6. The group includes countries like the Netherlands, Belgium, Germany, Denmark, Norway, and Sweden. Another ongoing project is the North Sea Wind Power Hub (NSWPH) that aim to ensure that offshore wind development does not become fragmented, country by country, but transnational and coordinated. A core concept is to build a large artificial island in the North Sea that would serve as a hub for connecting offshore wind farms in the region. The project was founded by Transmission System Operators (TSOs) from Denmark, Germany, and Netherlands. The Norwegian government has stated plans to grant licenses for 30GW offshore wind by 2040. There are plans for over 500GW of installed offshore wind around the North Sea. Figure: Map over the North Sea showing the status of the wind farms and map over the areas the Norwegian water resources and energy directorate (NVE) has pointed out as potential offshore wind areas. Norway’s efforts to harness offshore wind power are well underway, with an 88MW floating wind farm, Hywind Tampen, officially launched in 2023. It is the world’s first floating wind farm to power offshore oil and gas platforms, providing electricity for the Norwegian North Sea’s Snorre and Gullfaks oil and gas fields. The estimated investment cost of Hywind Tampen was $691 million or $7.9 million per MW, with the Norwegian authorities pledging up to $262 million via Enova subsidies. In addition, the Norwegian business sector’s NOx fund offered up to $64 million. The wind farm is expected to meet about 35% of the electricity demand of the two fields. This will cut CO2 emissions from the fields by about 200,000 tonnes annually. Studies1 that the wind power sectors in countries such as France, Germany, and Denmark will require continued subsidies to remain profitable Cannibalization To understand the impact that larger shares of intermittent energy have on the grid, we may introduce the cannibalization effect and its influence on the capture price obtained by intermittent energy sources17. In essence, the cannibalization effect suggests decreasing power prices when a substantial amount of energy is supplied simultaneously. As a result, the capture price obtained by these power plants could be lower than that obtained by baseload power plants, which can operate continuously. Reducing the correlation between various offshore wind farms can lessen the overall power variability across the entire fleet. Enhanced stability in power output, in turn, increases the chances of generating electricity during peak market prices, thus leading to a higher capture rate. Consider, for instance, that strategic placement of these wind farms might lift the capture rate from 80 to 85%. For a hypothetical 30 GW Norwegian offshore wind power fleet, this difference is significant. With a 50% capacity factor and a baseload power price of $60/MWh, this 5% boost in the capture rate could result in $394 million in additional annual revenues, equivalent to $3/MWh. This scenario highlights the importance and potential financial benefits of smart placement and operation strategies. In light of these considerations, this paper focuses on exploring the correlation challenges related to offshore wind power in the North Sea to evaluate the potential for increasing its value, with a focus on the Norwegian case. The concept of complementarity between wind and solar power has been suggested as a potential hybrid energy system solution to reduce seasonal variability, though its effectiveness varies depending on the temporal scale Another innovative approach involves co-locating offshore wind and wave power to mitigate variability in individual renewable energy sources. There are several proposals for producing hydrogen as an ancillary service to address offshore wind power variability and facilitate further expansion. A hybrid hydrogen-battery storage system has been presented as a promising solution. Alternative approaches, such as demand flexibility measures, have been proposed in lieu of strategically distributing wind farms. This paper aims to contribute to the optimization of offshore wind farm placement by introducing a method to reduce overall correlation among buildouts, thus enhancing geographical smoothing. This paper focuses on a particular research gap that has been identified. A comprehensive analysis of the interactions between different sites within a country and its interactions with other North Sea producers is currently missing. We aim to fill that gap with a focused case study for Norway. The primary contributions of this paper include: A correlation analysis of offshore wind power in Northern Europe; and, A methodology to minimize the variance of a wind power fleet using the Augmented Lagrangian Algorithm (ALA), which has the benefit of obtaining the unique optimal solution to nonlinear, convex optimization problems. A weighted combination of the proposed Norwegian offshore wind farm locations ensures the least intermittent power output. This approach is fundamentally different from focusing on building out in locations with the best resources and being less concerned about the variability. Algorithm A recent work has investigated the potential for strategically distributing wind farms in Brazil to significantly mitigate seasonal variability in offshore wind power using a genetic algorithm (GA)33. Given the nonlinear and convex nature of our optimization problem, we have determined that our proposed ALA is a more suitable choice than GA. Marine spatial planning (MSP) Marine spatial planning (MSP) represents another avenue for coordinating offshore wind buildouts with other marine activities, thereby maximizing synergies with offshore energy generation Marine spatial planning (MSP) represents another avenue for coordinating offshore wind buildouts with other marine activities, thereby maximizing synergies with offshore energy generation Marine spatial planning (MSP) represents another avenue for coordinating offshore wind buildouts with other marine activities, thereby maximizing synergies with offshore energy generation Figure: Scatter plot of correlation coefficients (r) between different wind farms in North Sea with respect to distance and exponential curve fit of the data points. Results include hourly (1h), daily (24h), weekly (168h), and monthly (720h) time resolutions. Correlation coefficients drop below 0.5 at distances exceeding 384km at an hourly resolution and 808km at a weekly resolution. During longer periods of wind drought, such as the one experienced in Europe in 2021 , the geographical dispersion of wind power farms becomes less effective, emphasizing the need for backup capacity and storage for longer durations. The majority of correlation coefficients are positive, implying that negative correlation, which would be advantageous for risk diversification, is rare. Geographical dispersion of wind farms over long distances can significantly smoothen wind power output, but this effectiveness decreases over extended time periods. Given that the North Sea spans approximately 580 km in width and 960 km in length, wind farms established by individual countries will inevitably impact others. The collective output from proposed Norwegian offshore wind farms exhibits far less variability than individual parks. If the objective is to minimize variance, less than a third of the buildouts will occur in southern Norway. Tradeoff between maximizing wind power output and reducing correlation. This implies a need for strategic objective weighting, particularly when the ambition to diminish correlation competes with reducing the levelized cost of electricity (LCOE). Higher correlation can indeed lead to power price cannibalism. Figure: The scale of the dots for each area represents how offshore wind power should be built out to reduce the variance of the wind power fleet as a whole with hourly (1h) resolution. Note how the model gives little weight to the Norwegian offshore wind farms in the south (especially Sørlige Nordsjø II) when including wind farms for all countries around the North Sea. Prioritizing the development of the wind resource areas farthest away is key to reducing wind power variance around the North Sea. As there is a lot of activity on the European continental side of the North Sea8, it would be desirable to find ways to reduce the overall wind power correlation. Building a significant portion of Norwegian offshore wind capacity in the northern regions of Norway aligns with our optimization. This poses a dilemma, as these remote locations are also the costliest to develop. In a scenario where one accepts a higher wind power correlation, there will be more sporadic energy scarcity and abundance, and more of the aforementioned backup capacity will be required. By 2031, Europe will only be capable of storing about 10 minutes of its electricity in batteries (i.e. 89GWh) Norwegian hydropower, with about half of Europe’s storage capacity (i.e. 87TWh), can play a role. However, the hydropower fleet’s dispatchable power capacity is limited to just 30GW. While expanding the Northern offshore wind farms would indeed lead to decreased correlation, it might not be economically feasible. This claim is backed by the historical costs of transmission grids in Norway, which stand at approximately $1.2 million per km per GW59. Considering Norway’s length of 1748km, the cost of a transmission line across the country would approximate $2100 million per GW transmission. This cost has to be contrasted with the potential increase in the capture rate, which could result in savings annually of $394 million, by 5% increase capture rate, as explained in the introduction. The net present value of such a hypothetical improvement is $4600 million, assuming an economic lifetime of 25 years and a discount rate of 7%. Clearly, the financial viability of such a project needs to be studied in further detail. Hjelmeland (2023) Correlation challenges for North Sea offshore wind power: a Norwegian case study Alonzo, B., Concettini, S., Creti, A., Drobinski, P. &amp; Tankov, P. Profitability and revenue uncertainty of wind farms in Western Europe in present and future climate. Energies 15, 6446 (2022).↩︎ "],["varia.html", "24 Varia 24.1 US Legislation", " 24 Varia 24.1 US Legislation Clips from ‘Volts’: Congress in december 2020 passed an enormous $900 billion coronavirus relief bill attached to an enormous $1.4 trillion omnibus spending bill, adding up to a super-enormous $2.3 trillion megabill — at 5,593 pages, the longest bill Congress has ever passed. Buried in that megabill is the most substantial energy legislation passed in the US in over a decade. The legislation will include a bill that would sign the US on to the Kigali Amendment to the Montreal Protocol, which would reduce the use of hydrofluorocarbons (HFCs) by 85% over 15 years. HFCs (used in air-conditioning, refrigerants, aerosols, etc.) are potent greenhouse gases, so full international implementation of the amendment is projected to avoid 0.5°C worth of warming all on its own. Nuclear power and carbon capture, utilization, and storage (CCUS) get the bulk of the funds. There’s a ton of stuff on innovation, not only funding specific RDD&amp;CA programs, but making more structural changes like establishing an Office of Technology Transitions in DOE and authorizing DOE to support regional clean-energy labs and incubators. There’s also a focus on funding demonstration and commercialization programs and technology transfer programs to accelerate innovation. It authorizes the Federal Energy Regulatory Commission (FERC) “to compensate persons with scientific, technological, engineering, and mathematical skills at a higher level than the rate allowed under the civil service.” This is nifty because FERC cases are incredibly important to the future of the electricity system, and big utilities can afford to hire high-priced experts to testify; this gives FERC more ability to hire its own experts. US Pandemic/Climate Energy Bill (Volts) "],["distributed-energy-resources.html", "25 Distributed Energy Resources", " 25 Distributed Energy Resources Centralized energy generally refers to utility-scale power generators (or energy storage) hooked up directly to the transmission grid: coal or natural gas plants, wind farms, solar fields, grid-scale battery stacks, what have you. The big stuff. Distributed energy consists of anything that generates, stores, or manages electricity on distribution grids: rooftop solar panels, ground-mounted “community solar” arrays, consumer batteries, electric vehicles, building energy management software, and the like. A new way to model the energy system that takes into account DERs and the services they provide. They used it to study the effect of DERs on the electricity system and the results are summarized in “A New Roadmap for the Lowest Cost Grid.”. The cheapest possible carbon-free US grid involves vastly more centralized renewable energy, but it also involves vastly more distributed energy. What’s more, far from being alternatives, they are complements: the more DERs you put in place, the more centralized renewables you can put on the system. DERs are a utility-scale renewable accelerant. The practical implication is that going all out on DERs is to everyone’s benefit, up and down the electricity supply chain, from utilities to consumers. It is difficult to exaggerate just what a revolutionary change this represents in energy modeling and how much it turns conventional wisdom on its head. By making distribution grids visible to their model and co-optimizing those grids with the transmission system, the team at VCE uncovered a source of grid flexibility that could save a decarbonizing electricity system some half a trillion dollars through 2050. That’s real money. Volts "],["virtual-power-plants.html", "26 Virtual Power Plants", " 26 Virtual Power Plants A network of decentralized, medium-scale power generating units such as wind farms, solar parks and combined-heat-and-power units, as well as flexible power consumers and storage systems. In practice, a VPP can be made up of multiple units of a single type of asset, such as a battery or a device in a demand response program, or a heterogeneous mix of assets. These units “are dispatched through the central control room of the virtual power plant but nonetheless remain independent in their operation and ownership,” adds Next Kraftwerke. In other words, a VPP is to a traditional power plant what a bunch of internet-connected desktop computers is to a mainframe computer. Both can perform complex computing tasks, but one makes use of the distributed IT infrastructure that’s already out there. A key feature of VPPs is that they can aggregate flexible capacity to address peaks in electricity demand. In this respect, they can emulate or replace natural-gas-fired peakers and help address distribution network bottlenecks — but usually without the same capital outlay. What’s the difference between a virtual power plant and a microgrid? Microgrids (and minigrids) also often involve a mix of distributed renewables, storage, flexible demand and fossil-fuel plants. But there are important differences, as well: VPPs are integrated into the grid. Microgrids are often off-grid, and in an on-grid setting, they are designed to be islanded so they can carry on working independently if the grid goes down. VPPs can be assembled using assets connected to any part of the grid, whereas microgrids are usually restricted to a particular location, such as an island or a neighborhood. The two concepts use different systems for control and operation. VPPs are managed via aggregation software, offering functions meant to mimic those of a traditional power plant control room. Microgrids rely on additional hardware-based inverters and switches for islanding, on-site power flow and power quality management. Another difference concerns markets and regulation. VPPs are aimed at wholesale markets and do not usually require specific regulation. Microgrids, on the other hand, are more focused on end-user power supply. What’s the difference between a virtual power plant and demand response? This one is a bit trickier, and it’s tied up with the semantics of the energy industry. The term “demand response” dates back decades to programs that enlisted factories or commercial buildings to manually shut down loads in order to combat grid emergencies. While the industry has gotten much more sophisticated in the past decade or so, it does still include those manual programs alongside more automated and flexible ones. GreenTech "],["energy-use.html", "27 Energy Use 27.1 Energy Efficiency 27.2 Urbanization 27.3 Green Steel 27.4 Aluminum 27.5 Shipping 27.6 Food-Energy Nexus", " 27 Energy Use James Hansen obviously has not confidence in renewables China and India have large, growing economies that depend on fossil fuels, especially coal. They are also global leaders in producing and installing renewable energies. Their enthusiasm for renewables is whetted by the opportunity to be global suppliers of renewable materials such as solar panels. Their own use of renewables is extensive and growing, but there is no realistic expectation that renewables will displace the need for baseload electric power. Phase-out of fossil fuels in electricity production likely requires large expansion of renewable energies and at least a doubling or quadrupling of global nuclear power. Fig. Energy consumption (percent) in 2019 by fuel in world and 71 nations: BP data (black = coal, orange = oil; red = gas; gray = nuclear; blue = hydro; green = renewable). [Source:James Hansen: Planet Ch47] 27.1 Energy Efficiency 27.1.1 Rebound effect Rebound effect = Jevons Paradox Lange Literature on the rebound phenomenon has grown significantly over the last decade. However, the field is characterized by diverse and ambiguous definitions and by substantial discrepancies in empirical estimates and policy proposals. As a result, cumulative knowledge production is difficult. To address these issues, this arti cle develops a novel typology. Based on a critical review of existing classifications, the typology introduces an important differentiation between the rebound mechanisms, which generate changes in energy consumption, and the rebound effects, which describe the size of such changes. Both rebound mechanisms and rebound effects can be analytically related to four economic levels – micro, meso, macro and global – and two time frames – short r un and long run. The typology is populated with eighteen rebound mechanisms from the literature. This contribution is the first that transparently describes its criteria and methodology for developing a rebound typology and th at gives clear definitions of all terms involved. The resulting rebound typology aims to establish common con- ceptual ground for future research on the rebound phenomenon and for developing rebound mitigation policies. Figure: The relationship between energy efficiency improvements, rebound mechanisms and rebound effects. This figure presents the definitions and illustrates the conceptual distinction between rebound mechanisms and rebound effects. The size of the rebound effect is defined by the relationship between three levels of energy consumption: 1) “ex ante”, i.e. before the efficiency improvement, 2) “potential”, i.e. theoretically possible due to the efficiency improvement, and 3) “ex post”, i.e. actually realized by the efficiency improvement. Actual savings can be negative in case of backfire. Rebound mechanisms link the energy efficiency improvement to its actual impact on energy consumption. While there are a variety of existing classifications of rebounds ef­ fects, few articles have focused on developing coherent and transparent typologies. This paper aims to advance the foundations of such a ty­ pology by systematically building on existing typologies and being clear in its definitions and its development. We see our attempt as a step to­ wards developing a shared typology as envisaged by Dunlop [3]. A possible next step would be to challenge or advance this typology by attempting to incorporate non-economic rebound mechanisms and to see in how far and in what ways it would need to be adjusted. It is our hope that the transparency of our approach will provoke further critical debate within the community, at the end of which some form of agreed typology could emerge. Figure: Rebound effects and mechanisms at different economic levels and time frames. This figure provides an overview of the central categories in our typology. Four stacked analytical levels on which the rebound effect can be measured are distinguished. At each level, several rebound mechanisms, which are represented by the arrows, cause an increase in energy consumption. The respective types of mechanisms are categorized into short run and long run mechanisms. The connecting lines between the levels indicate that the rebound effect at a lower level is part of the rebound effect at a higher level. Lange (2021) Jevons Unravelled (pdf) 27.2 Urbanization Fix (twitter) Are cities sustainable? A loaded question, yes. But one we should think about nonetheless. Here’s an undeniable fact: urbanization correlates strongly with energy use per person. 27.3 Green Steel Roselund In conventional iron production, blast furnaces use coke — nearly pure carbon coal — to heat ore and separate oxygen from the iron in the ore. For this step, Hybrit uses a different, more efficient process called direct reduction. While this can be done with natural gas or even coal as the agent that removes the oxygen, Hybrit’s innovation is to use hydrogen made from electrolysis powered by renewable energy. The clean exterior of the facility is a testament to this: Instead of soot from unburned carbon and large amounts of CO2, the Hybrit plant emits clouds of water vapor. This is groundbreaking. Iron and steel production accounts for 7% of total global CO2 emissions, and climate and energy experts have long considered it one of the more challenging industries to decarbonize. Recycling of steel can mitigate some of the emissions — provided furnaces run on electricity from low- or zero-carbon sources — but will not suffice to meet global demand in the 21st century. Building all of the wind turbines, transmission towers and other infrastructure we need for global decarbonization will require a lot of steel. Additionally, growing economies in China, India and other rapidly developing nations are expected to drive a big increase in steel demand over the next few decades. Roselund (2021) Green steel is picking up steam in Europe (CanaryMedia) 27.3.1 Molten Oxide Electrolysis Soltoff Decarbonizing the steel industry is a major hurdle in dealing with climate change. Steel production is responsible for close to 7% of global greenhouse gas emissions, roughly equivalent to the annual emissions of all the cars on the world’s roads. But steel is also used to make cars, so these impacts are overlapping. And that gets to the very heart of why steel is such a big deal when it comes to climate change: It’s everywhere. Decarbonization often comes down to finding creative uses for electricity. The playbook is simple. You take a process that traditionally burns fossil fuels, and then you replace it with an alternative that uses clean electricity instead. Of course, this is much easier said than done, especially for heavy-duty industrial processes like steel production. In these cases, many solutions rely on green hydrogen as a sort of middleman. You can use clean electricity to produce hydrogen, and then burn hydrogen to make steel, with only water as a byproduct. A joint venture in Sweden is already producing fossil-free steel using this method, though still in relatively small quantities. One company, MIT spinout Boston Metal, is aiming to streamline the process by eliminating the green-hydrogren step and instead using electricity directly for making steel. Its process is based on technology called molten oxide electrolysis that uses electric current to separate oxygen from iron ore, a critical step in the steel production process. Boston Metal has already raised $85 million from climatetech heavyweights like Breakthrough Energy Ventures, The Engine, Prelude Ventures and Energy Impact Partners, along with several industry coalitions and corporate venture groups. It aims to build its first commercial steel plant by 2024 or 2025, and then license its technology to major steel producers. Producing 1 ton of steel with traditional methods releases almost 2 tons of CO2 into the atmosphere, and the world uses almost 2 billion tons of steel each year. In the short term, existing steel can be melted down with electricity and reused, which can displace the need for new product — but only to a point. Most of the world’s steel needs can only be met with primary production because recycled steel doesn’t work for certain high-grade applications, and more significantly, there’s just not enough of it to keep pace with demand. The traditional way to make steel is to melt iron ore at very high heat (over 1,500 degrees Celsius), then refine it from iron oxide into pure iron and fortify it with small amounts of carbon. It’s a complex process that emits carbon at different stages. Some emissions come from the heating process, which usually involves burning a heat-refined form of coal called coke. A bit of the carbon from the coke gets dissolved into the iron, turning it into steel. Another chunk of emissions comes from chemical reactions that occur as the iron oxide is purified of its chemically bonded oxygen. That oxygen reacts with dissolved carbon and breaks off as carbon dioxide gas. Over half of the emissions come from a single piece of equipment used in the process: the blast furnace, where the iron ore is converted into a form called pig iron. Decarbonization of the iron and steel industry basically means decarbonization of the blast furnace. Green hydrogen appears to be the most promising route to decarbonizing steel, with hydrogen-powered direct reduction of iron as a key step in cutting the emissions from blast furnaces. This process is now being used in green steel projects in Europe, and some of China’s biggest steelmakers are exploring it as well. Molten oxide electrolysis is too nascent. The core principle of using electricity to refine metal has been around for a while. In fact, electrolysis has been a key part of making aluminum for over 100 years. Boston Metal’s molten oxide electrolysis process applies this technique to iron, which requires hotter temperatures. Aluminum electrolysis happens at temperatures just under 1,000 degrees Celsius, while iron electrolysis requires about 1,600°C, a temperature far hotter than molten lava. To start, the iron ore is melted with heat produced from electricity. Then it’s placed in a cell structured almost like a giant battery. At the top, an anode provides electric charge. At the bottom, a cathode receives the electric charge. In between, the charge flows through an electrolyte, which in this case is a scalding bath of molten materials. The electrolyte contains a variety of elements bound to oxygen, including aluminum, silicon and calcium. All of these oxides are more stable than iron oxide, so the iron oxide is the first to separate when exposed to electric charge, breaking down into pure oxygen and iron. The iron, still liquified, sinks to the bottom where it can be tapped out and turned to steel. The composition of the electrolyte is a critical advantage of its technology. All of those other elements in the electrolyte are also present in iron ore as impurities, but the impurities stay behind in the electrolyte after the pure iron is removed. That means the process works even with low-grade iron ore, which is cheaper and more plentiful than higher-grade ore that has fewer impurities. Another of molten oxide electrolysis’s advantages compared to direct reduction of iron is its efficiency. The reason is fairly intuitive. By cutting out the hydrogen step, MOE puts energy directly into steel production, removing interim stages where energy can be lost. MOE requires higher temperatures than hydrogen-based production, which eats into the benefits, but even taking that into account, MOE still winds up being more efficient. Making green steel using green hydrogen requires at least 30% more energy than MOE — and possibly as much as 50% to 60% more. Boston Metal says that its technology uses 4 megawatt-hours of electricity to produce 1 ton of steel. According to Columbia’s research on decarbonizing steel, replacing all the world’s blast furnaces with MOE manufacturing processes would require an amount of power equivalent to almost 20 percent of global electricity consumption in 2018. That means the steel industry would become one of the biggest users of electricity on the planet. But replacing all steel production with hydrogen-powered direct reduction of iron could require even more electricity. That means there’s no way to address steel’s climate impacts without installing a whopping amount of clean power generation, in addition to making sure that the grid is ready to reliably move around all that extra electricity. Soltoff (2022) Green steel without green hydrogen — can it work? 27.4 Aluminum Takemura Aluminum is a linchpin of the clean-energy economy. The gleaming metal is found not only in EVs and solar panels, but also in wind turbines, heat pumps and the transmission lines critical for building out the grid. By 2035, IRA-driven demand for aluminum for use in wind turbines and solar panels is forecast to exceed all of the aluminum produced in the U.S. in 2022. So the industry now faces the twin challenges of drastically ramping up production while simultaneously slashing CO2 from its operations, which currently account for 2 percent of global greenhouse gas emissions annually. The industry’s emissions stem mainly from the incredible amount of electricity used to produce primary aluminum, which is refined directly from ore rather than recycled metal. In this process, smelters zap 1,700˚F-plus baths of molten salt and aluminum oxide with electricity in order to precipitate out pure aluminum. In 2021, roughly 70 percent of U.S. aluminum’s emissions came from those electricity demands, according to a report by the nonprofit Environmental Integrity Project. Only one plant in the U.S. runs on renewable hydropower, while the others run on fossil fuels, including coal. Of the remaining 30 percent of emissions, about 20 percent are a byproduct of the electrolytic reaction itself — the fossil-fuel anode that conducts electricity releases CO2 and other greenhouse gases — and 10 percent come from making the anodes and producing the precursor aluminum oxide (alumina), according to the report. Several of the letter’s signatories, including Ford and GM, are also members of the First Movers Coalition, a group of companies leveraging their purchasing power to drive industrial decarbonization. They’ve pledged that of the primary aluminum they buy, at least 10 percent will be low-carbon — defined by the group as emitting less than 3 metric tons of CO2 per metric ton of aluminum — by 2030. There’s not much U.S.-made aluminum to buy these days, dirty or clean. Most primary aluminum is produced by China, followed by India and Russia. Meanwhile, the U.S. aluminum industry is in crisis: The sector has shrunk from 23 smelters three decades ago to just five in operation today. And those holdouts are struggling due to “spiking [fossil] electricity prices, lack of low-cost renewable energy and insufficient federal investment.” Takemura (2023) Can the US lead on clean aluminum? Ford, GM and others hope so 27.5 Shipping 27.5.1 Green methanol Gallucci “Green” methanol is gaining favor as a lower-carbon way to power cargo ships. But we’re going to need a lot more of it for the plan to work. Methanol, or \\(CH_{3}OH\\), is gaining favor as an alternative fuel. Methanol doesn’t produce harmful soot or particulate matter when burned. If made from renewables, the chemical can sharply curb carbon dioxide emissions compared to using oil-based fuels, though it still emits some \\(CO_2\\). The latest example of industry uptake comes from CMA CGM, one of the world’s largest container shipping companies and Walmart’s top shipping partner. The French carrier recently placed orders worth billions of dollars for new vessels that can run on methanol. Now CMA CGM expects to have 24 such ships in service by 2027, the company told Canary Media. The new orders, announced last week, give CMA CGM a lead over its competitor Maersk. The Danish shipping giant has 19 methanol-powered ships in the works and helped put methanol on the map in 2021 when it ordered the first methanol-burning container vessel, which is set to launch in early 2024. All told, Maersk’s new ships could help avoid 2.3 million metric tons of annual carbon dioxide emissions if they’re fully powered by renewably produced methanol, the company said. That’s akin to taking 512,000 gas-powered passenger cars off the road for one year. Within the next few years, the industry is expected to have some 125 vessels that can operate on methanol in service. That’s a small fraction — about 0.25 percent — of the roughly 50,000 merchant ships that move cargo internationally today. Companies and government agencies still need to invest billions of dollars to upgrade port infrastructure and add refueling equipment to keep these ships filled up. Bio-methanol made from plant biomass can curb emissions by 70 to 80 percent, researchers said. By contrast, conventional methanol from fossil gas, known as ​“gray” methanol, can actually increase CO2 emissions compared to marine gas oil, though it still results in fewer air pollutants. Although chemical tankers have carried methanol in their cargo holds for decades, companies didn’t begin using the liquid to power engines until relatively recently. In 2016, Waterfront Shipping began operating the world’s first ocean-going vessel capable of using methanol. The chemical tanker, named Lindager, has ​“dual-fuel” engines that can run on either methanol or oil-based fuels. Virtually all methanol-powered ships in use or being built today — including CMA CGM’s — have a similar set-up, which enables ship operators to switch between fuels depending on the price or availability. This compatibility is a major reason why methanol is quickly taking off. Waterfront Shipping now uses 18 dual-fuel vessels to carry cargo worldwide, representing 60 percent of its operating fleet. From a technical point of view, there’s quite a big difference compared with the other alternatives for shipping,” said Erik Hannell, CEO of Stena Bulk, a Swedish tanker shipping company. ​“You can actually invest in [methanol] today.”~ Stena Bulk has about 80 total vessels in its fleet, including four tankers with dual-fuel engines. Hannell said the ships primarily run on conventional methanol, though the company intends to blend more green methanol into the mix as supplies become more available — and as its customers grow more willing to pay a ​“green premium” to buy cleaner but costlier versions of the fuel. Methanol remains one of the few near-term options that shipping companies have for tackling emissions using existing infrastructure. Other promising zero-carbon alternatives, such as ammonia and hydrogen, are in much earlier stages of technology development and remain potentially decades away from achieving mainstream adoption. Gallucci (2023) This common chemical could help shipping giants start to decarbonize 27.6 Food-Energy Nexus Webber Food itself is just a means of energy storage, and a particularly good one at that. While photosynthesis is remarkably inefficient — averaging only 0.3% globally, compared to 90% or more in an electric motor — it stores energy for weeks to years. In the U.S. we use around 12% of our energy to produce food, in the form of inputs like diesel, fertilizer and electricity. Meanwhile, the food system itself provides fuel to the rest of the energy system, through ethanol and other forms of bioenergy. Webber (2023) The food-energy nexus (podcast) "],["energy-poverty.html", "28 Energy Poverty", " 28 Energy Poverty Without cheap, safe, low-carbon energy sources at scale we are stuck between the alternatives of high greenhouse gas emissions and energy poverty. The world lacks safe, low-carbon, and cheap large-scale energy alternatives to fossil fuels. Until we scale up those alternatives the world will continue to face the two energy problems of today. The energy problem that receives most attention is the link between energy access and greenhouse gas emissions. But the world has another global energy problem that is just as big: hundreds of millions of people lack access to sufficient energy entirely, with terrible consequences to themselves and the environment. The richest 1% in the EU emit on average 43 tonnes of CO2 annually – 9-times as much as the global average of 4.8 tonnes. The problem is larger for the extremely rich. The only countries that have emissions that are close to zero are those where the majority suffers from energy poverty. Roser (2021) The World’s Energy Problem "],["carbon-intensity-of-energy.html", "29 Carbon intensity of Energy", " 29 Carbon intensity of Energy That chart – fossil fuel carbon (GtC) emitted per unit energy (Gt of oil equivalent) – is arguably the best summary of how well the world is doing in moving to carbon-free energy. Atmospheric CO 2 may approximately stabilize when we reach a carbon intensity near 0.25, but carbon intensity must be closer to zero to draw down atmospheric CO 2 and cool Earth to a level that stabilizes ice sheets and terminates tundra melt. Unfortunately, global carbon intensity remains stubbornly high – almost 0.7 – in part because emerging economies, such as China and India, get much of their energy from coal. Moreover, progress in reducing carbon intensity is inadequate in most nations worldwide. Sweden is a notable exception. How did Sweden do it? Once, when a Swedish minister showed a graph similar to Fig. 47.5, I asked how they achieved the rapid drop of carbon emissions between the mid-1970s and mid-1980s? The answer: “we introduced combined heat and power.” Wonderful! This suggests that if we get the rest of the world to adopt combined heat and power, the global warming problem will be almost solved. Eh, not so much. A Swedish engineer told me that the main reason was that Sweden completed 10 nuclear power plants in that decade. I interpreted the conflicting answers as a difference between scientists and ministers. The scientist looks at numbers and facts objectively, while the minister bears politics in mind, and the Swedish government had become quite anti-nuclear. Both answers were true, but the minister was misleading – hiding the crucial information. [James Hansen: Sophies Planet Ch.47] "],["rebound-effect-1.html", "30 Rebound Effect", " 30 Rebound Effect First, note that when an energy service becomes more efficient, it also becomes less expensive. Say I trade in my old car in for a Prius, which is 20 percent more fuel efficient. Among other things, that means I spend (roughly) 80 percent what I used to spend to drive how much I used to drive. That’s money in my pocket I didn’t have before. What do I do with it? One thing I might do with it is buy more gas, i.e., drive more. In other words, I might respond to the lower cost of an energy service by increasing my demand for the service. The amount of primary energy I use for driving, which fell when I bought my Prius, would rebound back upward. That is the direct rebound effect. Or, I might use the extra money to, say, buy an iPad. Thing is, manufacturing and operating an iPad requires energy. So even if the energy I devote to driving drops, my total energy use could rebound back upward. That is the indirect rebound effect. (The same basic story applies to businesses. If their energy costs go down through energy efficiency, they invest some of the savings in more production, thus bumping their energy use back up.) Stepping back, there’s the question of economy-wide rebound effects. If we drive energy efficiency across the entire economy, then we lower what’s called the energy intensity of the economy, that is, how much primary energy it takes to get a unit of GDP. When the economy as a whole is more energy efficient, it takes less energy to create wealth. What is the macroeconomic effect of a drop in energy costs? The same effect we’d expect from a drop in labor costs or capital costs: faster growth. And insofar as stimulating growth means increasing energy use, that will wipe out some of the energy-saving gains of the efficiency. David Roberts "],["fossil-subsidies.html", "31 Fossil Subsidies", " 31 Fossil Subsidies This paper estimates the financial benefits accruing to fossil fuel producers (i.e., the producer incidence) that arise because of implicit fossil fuel subsidies in the United States. The analysis takes account of coal, natural gas, gasoline, and diesel, along with the implicit subsidies due to externalized environmental damages, public health effects, and transportation-related costs. The direct benefit to fossil fuel producers across all four fuels is estimated at $62 billion per year, a sum calculated due to the higher price that suppliers receive because of inefficient pricing compared to the counterfactual scenario where environmental and public health externalities are internalized. A significant portion of these benefits accrue to relatively few companies, and specific estimates are provided for companies with the largest production. The financial benefit because of unpriced costs borne by society is comparable to 18% of net income from continuing domestic operations for the median natural gas and oil producer in 2017–2018, and it exceeds net income for the majority of coal producers. The results clarify what the domestic fossil fuel industry has at stake financially when it comes to policies that seek to address climate change, adverse health effects from local pollution, and inefficient transportation. Producer Incidence The producer benefits of interest—i.e., the producer incidence (PI)—are based on the higher price that suppliers receive because of inefficient pricing compared to the counterfactual scenario where environmental and public health externalities are internalized. The direct financial benefit to fossil fuel producers of inefficient pricing across all four fuels is estimated at $62 billion per year on average, representing 11% of the total annual subsidy of $568 billion. The total subsidy is equivalent to an average of 3% of US Gross Domestic Product and equals the estimated value of the environmental, public health, and transportation-related externalities on an annual basis. To be clear, the focus here is not on direct subsidy payments that reduce the costs of fossil fuels, but rather on the implicit subsides that arise because of inefficient pricing that gives rise to social costs (1, 2, 6). While direct subsidy payments are common in many countries (7⇓–9), they are not in the United States. This paper also makes two methodological contributions to the literature on fossil fuel subsidies. First is a generalization and implementation of the standard International Monetary Fund (IMF) framework to separately estimate the PI and consumer incidence (CI). A key feature of existing studies—which focus on economic efficiency, environmental and health impacts, and government revenues—is the simplifying assumption of perfectly elastic supply. This implicitly assumes away fundamental concerns about the extent to which the fossil fuel industry benefits from subsidies and may therefore seek to prevent reform. The approach taken here uses empirically based estimates of supply elasticities to examine distributional implications, with a focus on PI. Conceptual Framework Implicit fossil fuel subsidies represent a hybrid of the standard tax and subsidy scenarios. This follows because externality-based, fossil fuel subsidies arise because of failure to implement efficient pricing, which confers an implicit subsidy. While different mechanisms are possible to establish efficient pricing, the most straightforward to illustrate the key points is Pigouvian taxation. Consider the market for a particular fossil fuel, characterized by the supply and demand curves in Fig. 1A. The initial equilibrium occurs at price p’ and quantity Q’, which is not efficient because of external costs in the form of environmental damages and adverse health effects. Assume for simplicity that the marginal external costs, denoted MEC, are constant. A Pigouvian tax equals the MEC and places a wedge between the supply and demand curves. If implemented, the Pigouvian tax would establish the efficient quantity Q∗ as the equilibrium and the prices buyers pay and sellers receive as pb∗ and ps∗, respectively. Figure: The PI and CI of an implicit fossil fuel subsidy. MEC represents the marginal external cost associated with each unit of Q. (A) The presence of no preexisting tax is assumed. The total implicit subsidy is the area MEC×Q’. The incidence measures capture the gain in producer and consumer surplus from inefficient pricing, i.e., the respective shaded areas excluding the vertically hashed triangles. (B) A case with a preexisting tax; the net corrective tax is the difference between the MEC and the preexisting tax. The implicit fossil fuel subsidy is defined as the sum of all shaded areas in Fig. 1A; i.e., the rectangle equal to MEC×Q’. This is an effective subsidy because it represents real costs borne by society—through environmental damages and adverse public health effects or foregone tax revenue—but not reflected in the market (1, 10). Of central interest here is the way that the total subsidy differentially benefits consumers and producers (i.e., the measures of incidence). The CI captures the change in net benefits to consumers (i.e., consumer surplus) because of the lower price they pay, and the PI captures the change in net benefits to producers (i.e., producer surplus) because of the higher price they receive. These measures are illustrated in Fig. 1A as the shaded areas labeled CI and PI, respectively, which do not include the vertically hashed triangles. The two regions represent the net gain to consumers and producers of maintaining inefficient pricing. Previous research nevertheless implicitly assumes the PI is zero, which follows because of the simplifying assumption of perfectly elastic supply (1, 2, 6, 8⇓–10). The assumption is reasonably motivated in previous analyses because of the focus on efficiency rather than distributional concerns between producers and consumers. The assumption is also reasonable in cases where the focus is on relatively small countries subject to the global supply of fossil fuels. The assumption does not, however, fully characterize markets in the United States, especially when it comes to coal and natural gas, which are less interconnected in a global market compared to oil. While less is known about supply elasticities compared to those for demand, existing research does provide a basis for informed assumptions that push away from the limiting case of perfect elasticity, especially for the United States. A final piece of the model to consider is the possibility for preexisting subsidies or taxes. An explicit, preexisting subsidy would be a direct government payment to reduce the producer or consumer costs of fossil fuels, but as mentioned previously, these are not common in the United States. Instead, implicit market subsidies do arise because of existing tax preferences for oil and gas firms, which have been estimated to cost the US government roughly $4 billion annually in foregone revenue (11). These subsidies are not accounted for in the present analysis because of the focus on nonmarket implicit subsidies. There are, however, preexisting taxes that affect the immediate applicability of Fig. 1A, and these must be taken into account to get an accurate measure of the fossil fuel subsidy in each case. Fig. 1B generalizes the framework to show how existing tax revenue associated with the initial equilibrium at Q’ is not included in the overall subsidy. In this case, the implicit subsidy is based on the net corrective tax (i.e., MEC minus the preexisting tax). The measures of incidence differ as shown but still represent the difference in the respective surplus measures. Overall Producer Incidence The methodological approach for estimating the incidence of fossil fuel subsidies requires several steps, all of which are described in detail in SI Appendix. First is obtaining price and quantity data for the different fuels. Second is estimating the MEC associated with each fuel. Third is obtaining information on preexisting taxes in order to calculate the net corrective taxes. Fourth is an approach for generating counterfactual prices that would emerge with efficient pricing. Last is obtaining estimates of supply and demand elasticities. Total Subsidy The results indicate a total subsidy across all four fuels of $592 billion in the most recent year, 2018. This number represents the external costs borne by society or foregone government revenue from inefficient pricing. Included in the external costs is the value of climate damages reflected in the social cost of carbon and adverse health effects from local pollution (SI Appendix, Fig. S1). The external costs for gasoline and diesel also include the value of congestion-based travel delays and accident fatalities, along with wear and tear on the roadways from heavy-duty, diesel fuel vehicles (SI Appendix, Fig. S1). Figure: The PI and CI of the subsidy for all fuels. (A) The measures of PI and CI for all four fuels (coal, natural gas, gasoline, and diesel) for the most recent year, 2018. Data for all other years are available in SI Appendix. Each measure is further partitioned into the underlying externalities, which are proportionally the same between both measures of incidence for each fuel (SI Appendix, Fig. S1). (B) The trend in PI over time for each fuel. While the producer benefits to coal have decreased 33%, those for all other fuels have increased substantially: 42% for gasoline, 52% for diesel, and 63% for natural gas. Kotchen (2021) Producer Benefits (pdf) SI (pdf) "],["germany.html", "32 Germany", " 32 Germany "],["norway.html", "33 Norway 33.1 Energy Statistics 33.2 Energy Policy", " 33 Norway 33.1 Energy Statistics Energifakta.no Access to reasonably priced hydropower has shaped energy use in Norway. Everyone has access to electricity, which is used for more purposes than in most other countries. Norway has a large energy-intensive manufacturing sector, and electricity is much more widely used to heat buildings and water than in other parts of the world. Because such a large proportion of electricity is produced from renewable sources, greenhouse gas emissions associated with stationary energy use are low in Norway. Norway’s population has risen by nearly 1 million since 1990 - 22.3%. 1990: 4.241.473 2015: 5.188.607 Strong economic growth has resulted in a doubling of GDP since 1990. Both production of and demand for goods and services that use energy are growing steadily. However, final energy consumption has risen by only 16 %, so that the Norwegian economy has become gradually less energy-intensive. Energifakta.no Our World in Data 500 Twh SB 250 Twh Source: SSB “Energy use refers to use of primary energy before transformation to other end-use fuels, which is equal to indigenous production plus imports and stock changes, minus exports and fuels supplied to ships and aircraft engaged in international transport.” Convertion: Kilo of oil equivalent unitjuggler 1 koe = 11.63 kWh Worldbank/IEA/OECD: Norway 2015 : 5818 koe = 67663 kwh per capita Our World in data : Norway 2015 : 101181 kwh per capita Wikipedia: Energi i Norge Statnett: Load Duration Curve 50timer med mer enn 24 000 Mwh i forbruk i 2021. Til sammenligning var det kun to timer med tilsvarende forbruk i tre foregående år. 33.2 Energy Policy Spetalen Nordmenn er ikke forbrukerne, vi er eierne av norsk vannkraft. Det er essensen av det hele! Vi er enige, dette er en nasjonal skandale og flause for Norge. Investor Øystein Stray Spetalen mener politikerne kan skylde på seg selv for at strømprisene er så høye. Vi er enige! «Norge har ikke et energiproblem, vi har et politikerproblem»! – Vi har dessverre fått inn en kunnskapsløs gjeng. De som har sittet i regjering de siste ti årene har glemt det viktigste, nettopp forvaltningen av kraften vår, sier Spetalen til Klassekampen. Han mener to grep kan løse situasjonen: En skatt på 78 prosent på all kraft som eksporteres til utlandet Norge må melde seg ut av Acer, som er EU-byrået for samarbeid mellom energiregulatorer. Han mener tilknytningen til EU fratar Norge selvstyre, og han mener unionen er grunnleggende antidemokratisk. Det har Spetalen svært rett i! Det er totalt svikt i Stortinget! De har glemt hvem de jobber for! 33.2.1 Energy Policy History Aam I «gamle dager», fram til midten an 1970-tallet, bestemte man hvor mye vannkraft man skulle bygge ut i Norge etter prinsippet om «bestemmende år». Da sa vi at vi skulle bygge ut så mye vannkraft at vi hadde nok kraft til innbyggerne i ni av ti år. Så fikk vi greie oss som best vi kunne i det tiende som ikke var så mye verre enn det niende. Litt sparing, m.v. og i verste fall litt rasjonering ville være tilstrekkelig for å få «endene til å møtes». Da var det en utfordring at vi de fleste år hadde vannkraft til overs. Så lagde vi strategier for å få omsetning på overskuddet. Et viktig element var å installere varmekjeler hos store forbrukere som kunne varmes opp med både el og olje og som kunne legge om til oljefyring når det var tørre perioder. Et annet element var å selge fleksibel, billig kraft til industrien – som kunne kobles ut når det var manko på kraft. Videre knyttet vi landsdelene sammen elektrisk slik at kraftselskap som manglet kraft pga lite nedbør kunne kjøpe kraft av andre som lå i områder som hadde hatt mer nedbør. Til slutt lagde vi utenlandsforbindelser til Sverige og Danmark for å kunne utveksle kraft med utlandet. Etter hvert lagde Samkjøringen av kraftverkene i Norge et spotmarked der kraftselskaper og industri kunne kjøpe og selge kraft av hverandre. Samkjøringen startet regionsvis fra i 1932 og ble landsomfattende i 1971. Den varte til 1991. Filosofien i «gamle dager» i de fleste land i Europa, USA og andre land som var avhengig av å bygge ut termisk kraft, var å skape et robust, sikkert og billig kraftsystem. Derfor satset man på diversifisering – litt kullkraft, litt gasskraft, litt kjernekraft, litt oljekraft og litt vannkraft. Da hadde man en robust risikoavlastning hvis ulike råvarepriser skulle gå opp mye i pris og man kunne spille råvareleverandørene opp mot hverandre. Riktig nok var gassprisen sterkt knyttet til olje, men de andre råvareprisene var til en viss grad uavhengige av hverandre. Dette har fungert bra i «100» år fram til nå og kostnadene for kraftproduksjon har ligget på et «fornuftig» nivå. For vannkraftlandet Norge var det meget gunstig å utvikle kraftutveksling med Europa. Norge hadde god tilgjengelighet på effekt ved at vannkraftmaskinene enkelt og raskt kunne øke sin produksjon. Videre kunne vi enkelt kjøre vannkraft stasjonene ned og kjøpe inn kraft fra EU for å lagre kjøpet i våre vannkraftmagasin. Samspillet mellom det termiske Europa og vannkraftlandet Norge var «perfekt match». Norge kunne utnytte muligheten for rask opp og nedkjøring av vannkraftverkene kombinert med å bruke vannkraftmagasinene til korttidslager. Dette tjente vi godt på. Videre kunne vi skaffe oss rimelig termisk kraft på natt og helger i tørrår hvor vi trengte å importere kraft. EU fikk tilgang til toppeffekt på hverdagene til en billigere penge enn å produsere toppeffekten selv. Utbyggingen av kraftkabler til Danmark og Nederland var drevet av slike tanker. Norge var det første landet til å liberalisere kraftforsyningen i 1990/91. Det var Senterpartimannen Eivind Reiten som var Olje-og energiminister 89/90 og miljøet rundt Handelshøyskolen i Bergen (blant andre Einar Hope) som var aktive pådrivere for å innføre en ny energilov som skilte ut kraftproduksjon som konkurranseutsatt virksomhet. Dette ble senere fulgt opp i Norden og i EU som har som mål å lage best mulig felles marked for kraft og gass i hele EU. Den norske stat tjener mye på salg av dyr gass til Europa. Det at Norge har lav magasinfylling i Sør-Norge gjør at kraftprisen der blir spesielt preget av kraftprisen i Europa. Norske kraftprodusenter får godt betalet for de nedskrevne vannkraftanleggene med dagens høye priser. Vanlige borgerne i Norge må imidlertid betale den høye prisen i det felles europeiske kraftmarkedet hvor prisen på dyr gass og CO₂ er styrende nå. Samspillet mellom det norske og nordiske kraftsystemet og EU er beskrevet blant annet i rapporten Nordic Grid Development Perspective 2021 (pdf), fra de nordiske nettselskapene (TSO’ene) og DNV-rapporten Energy Transition Norway 2021 Regjeringen har kommet borgerne i møte med krisepakken for strøm. og vil sette ned en energikommisjon. Aam (2021) Hva i all verden skjer med kraftmarkedet? Tamburstuen Dette er en strukturell krise. Og den må løses med radikale strukturelle, politiske tiltak, for dette var i k k e hensikten med energiloven: Ta kontroll over Nordpool og la Statnett og norske kraft- og nettselskap stå for omsetningen av strøm. Det er ingen mening i at et utenlandseid børs-selskap skal kunne tjene milliarder på omsetningen av norsk strøm og ta enorme utbytter ut av Norge la brukerne kjøpe strømmen fra denne nye enheten - basert på et prinsipp om kost pluss. På denne måten får produsentene og netteierne pløyd direkte tilbake en rimelig fortjeneste som brukes for oppgraderinger av kraftverk og nett. Det er et potensiale for en vesentlig økning av produksjonen i eksisterende vannkraftverk, uten naturinngrep. dette betyr at de såkalte strømselskapene forsvinner fra markedet sett et krav om minimum magasinfylling hele året - og sørg for at dette etterleves. Vår magasinkapasitet utgjør 70% av vår årlige produksjon av vannkraft og er bygget opp nettopp for å lagre energi – denne kapasiteten er unik i Europa reforhandle avtalene om utenlandskablene slik at Norge har nasjonal kontroll med hva som skal gå ut og inn av strøm Pålegg Statnett å investere i stamnettet slik at en unngår flaskehalsene som skaper de store prisforskjellene innenlands Dropp elektrifiseringen av sokkelen - gi oljeselskapene et lite skatteincentiv og krev at avgassene fra gassturbinene er renset innen en gitt frist Lag beredskap - lovmessig og finansielt - for å sikre at kraftverk og nett forblir på norske hender Bruk eierskapet i Statkraft og Statnett til å sørge for at disse to selskapene oppfyller sine oppdrag i Norge Styrk arbeidet med reell energiøkonomisering, og start arbeidet med geoenergi/jordvarme og med bioenergi – norske skoger flyter over av biomasse Tamburstuen 2 Trygve Tamburstuen gir her en opplysende og oppsiktsvekkende oversikt over hvordan vi er havnet i det nåværende kraftkaoset. Han gir også en helt nødvendig og presis beskrivelse av hva som gjøres for å få kontroll over kraftsituasjonen. KRONIKK Trygve Tamburstuen, styreleder i flere selskaper: GLADE DAGER I NEW YORK: Nasdaq-børsen har med sine allierte arbeidet for bygging av flest mulig grenseoverskridende strømforbindelser, fordi de med dette kontrollerer krafthandelen i Europa. De handler med alle kjente spekulasjonsobjekter, og vil selvsagt tjene mest mulig på hver transaksjon, skriver Trygve Tamburstuen.FOTO: BRYAN R. SMITH, AFP/NTB Energilovens intensjon var at brukerne kunne velge den leverandør som tilbyr lavest pris. Det store politiske feilgrepet var å gjøre strøm til en vare i et marked som ingen kontrollerer. Det var en reform ingen ba om, men et nettverk av sosialøkonomer drev fram dereguleringen, i en tid der konkurranseutsetting var det store mantraet. For å sikre en effektiv omsetning ble Statnett – tidligere Samkjøringen – satt til å drive en kraftbørs, og Statnett Marked ble etablert i 1993. Senere kom de nordiske og baltiske nettselskapene inn som eiere. Vi var interessert i å importere strøm fra naboland på kveld og natt, mens vi kunne eksportere på dagtid og utnytte magasinkapasiteten til å lagre strøm. I 2001 byttet Statnett Marked navn til NordPool, og det ble etablert flere datterselskaper. De kommersielle aktørene, inkludert Statnett, så at utenlandskabler og sammenkopling av kraftbørsene ville gjøre det nordiske kraftmarkedet integrert både fysisk og finansielt med det europeiske kraftmarkedet. Den finansielle krafthandelen omfatter handel med finansielle instrumenter som brukes som rene spekulasjonsobjekter. NordPool bygget opp en egen plattform for slik handel med kraft og CO₂-sertifikater. Og da starter børsifiseringen og den totale finansialiseringen av kraftmarkedet: I 2008 selger NordPool sin andel i kraftbørsen og mesteparten av sin øvrige virksomhet til Nasdaq OMX (Nasdaq er New York-børsen) for rundt 2.4 milliarder kroner. NordPool, som da var 50 prosent eid av Statnett, selger altså det viktige aktivum som den finansielle kraftbørsen er, uten at Stortinget behandler salget! Nasdaq OMX har helt klare ambisjoner. I dag styrer de kraftomsetningen i over 20 land i Europa. Nasdaq har to datterselskaper i Norge som nyter godt av dette: Nasdaq OMX Oslo omsatte i 2020 for 112 millioner og fikk et årsresultat på 29 millioner. Nasdaq Clearing omsatte for 195 millioner med et årsresultat på 107 millioner. Fortjenestemarginen i denne delen av krafthandelen er altså på 44 prosent. Nasdaq og deres allierte har selvsagt bevisst arbeidet for bygging av flest mulig grenseoverskridende strømforbindelser, fordi de med dette kontrollerer krafthandelen i Europa. De handler med alle kjente spekulasjonsobjekter, kort- og langsiktig, opsjoner, og vil selvsagt tjene mest mulig på hver transaksjon. Jeg omtaler ikke strømselgerne her, men en etablering av kraftbørsen er jo den mekanismen som åpnet for at 130 – i stor grad utenlandseide – strømselgere i dag operer i det norske markedet. Et ferskt eksempel er at Kintech Energy Spot, et datterselskap av et amerikansk selskap, har påført Bardu kommune et tap på 13 millioner ved å selge konsesjonskraft for kommunen! Så fullføres skandalen i 2019: da selger Statnett og partnerne 66 prosent av den fysiske kraftbørsen kalt NordPool Holding 2 AS, til den Nederland-baserte børseieren Euronext, som også eier Oslo Børs. I 2020 har Euronext et årsresultat i Norge på 575 millioner og tar ut 392 millioner i utbytte. NordPool Holding tar ut 75 millioner og TSO Holding, som eier de resterende 34 prosent av kraftbørsen, tar ut 480 millioner i utbytte – 30 prosent av dette tilfaller Statnett. NordPool Holding 2 AS eier NordPool AS og European Market Coupling Operator AS (navnet sier vel det meste). Prisen for de 66 prosent var 640,2 millioner, som må betegnes som et rent billigsalg i lys av den enorme strategiske verdien denne plattformen har. Igjen skjer salget uten at Stortinget behandler det. Sigbjørn Gjelsvik (Sp) stiller et spørsmål til statsråd Freiberg (Frp), men blir avvist og saken følges aldri opp. «Nasdaq har helt klare ambisjoner.» Fra å ha full statlig kontroll over både den fysiske og finansielle krafthandelen er det altså nå Nasdaq og Euronext – to av de største internasjonale børseierne – som kontrollerer ikke bare den norske kraftomsetningen, men også den europeiske. Og de har fått kontrollen for veldig beskjedne beløp, noe som bør rette søkelyset mot styre og daglig ledelse av Statnett i perioden. Vi er altså nå del av et fullintegrert europeisk kraftmarked. Samtidig har energipolitikken i EU slått helt feil, og utsiktene er dårlige: Europa har i overskuelig framtid lav selvforsyningsgrad på fornybar energi. Får vi ikke kontrollen over egen kraft, vil høye priser til husholdninger og industri i Norge forbli en permanent tilstand. Kontrollen kan vi få tilbake ved å: ta tilbake kraftbørsen, både den finansielle og den fysiske, og fjerne strømselgerne fra markedet reforhandle avtalene om kraftutveksling slik at vi selv kontrollerer flyten i kablene bestemme et transparent kostnadsregime for prisingen av kraft i Norge instruere Statnett til å investere i stamnettet i Norge for å fjerne flaskehalsene. Statnetts investeringer i de nye kraftkablene til Tyskland og Storbritannia er 18 milliarder. Hittil har strømmen bare gått én vei i disse kablene, og det er ingen grunn til å forvente at landene i overskuelig framtid vil kunne levere strøm til Norge – dette er altså rene eksportledninger. Vi sitter med uhyre sterke forhandlingskort: Norge dekker 25 prosent av Europas totale gassforbruk, som transporteres direkte i rør (USAs fabling nå om å redde Europas gassbehov med LNG på skip virker helt absurd). To tredeler av Norges gassreserver er ikke produsert ennå – vi er altså en langsiktig solid leverandør. Tyskland tar imot mest – 50 prosent i 2020 – mens Storbritannia tok 26 prosent. Dette er altså også de samme markedene som bare importerer norsk strøm og ikke leverer noe tilbake”. Tamburstuen (2021) El-markedet må endres strukturelt "],["serbia.html", "34 Serbia", " 34 Serbia Tooze Mining Lithium in Serbia I found this FT piece on the contested Rio Tinto lithium development in Jadar, Serbia intriguing, but also question begging. Asking the hive mind of twitter for more information, elicited this very sensible intervention by Aleksandar Milosevic on danas.rs Stick it through google translate. the dilemma to be decided is 1,000 well-paid jobs and about 30 million euros of annual state revenue (plus 2,000 jobs on construction and partly engaging domestic suppliers in a project worth over two billion euros), and against the relocation of 50 families, the destruction of some 400 hectares of land and the creation of tailings to the detriment of local flora and fauna. A potential battery factory with its jobs and taxes or a different calculation of state revenues changes the ratio of gains and losses. So, we decide on that. And not about whether we are for Vučić or not. Tooze "],["united-states.html", "35 United States 35.1 IRA", " 35 United States Links US Geothermal Everywhere: A New Path for American Renewable Energy Leadership 35.1 IRA Tooze Energy abundance strategy But the “cheap stuff is good” philosophy is integral to understanding the climate provisions of IRA, too, because the bill actually represents a conceptual revolution in how to approach climate policy. The original Obama-era idea was carbon pricing — making dirty energy more expensive — which failed in Congress only to be replaced by regulatory strategies like the Clean Power Plan that also aimed to make dirty energy more expensive. During the Trump years, activists and advocates continued to come up with more ways for the next Democratic administration to drive energy costs up. A big part of the push to declare Manchin a bad-faith actor was a sense that the White House was holding off on dramatic executive action in order to appease the West Virginia senator. If activists could convince Biden to give up on Manchin, then they could push him to do everything in his power to throttle fossil fuel production and drive the cost of dirty energy up. But that’s not what happened. Instead, IRA achieves emissions reductions almost entirely by trying to lower the cost of clean energy and electric appliances … and while Biden has certainly done executive actions on environmental issues, we’re also on pace to set an all-time record in American oil production this year. That’s the actual energy policy Democrats ended up enacting — an abundance-oriented approach to the energy transition that focuses on driving costs down. Tooze (2023) Energy abundance strategy - Yglesias on IRA "],["about.html", "A About", " A About Dyre Haugen and Dyrehaugen is Webian for Jon Martin - self-owned Globian, Webian, Norwegian and Canarian with a background from industrial research policy, urban planning and economic development consulting on global, regional and urban scales. I am deeply concerned about the (insane) way humanity (i.e. capitalism) interfere with nature. In an effort to gain insights in how and why this happens stuff is collected from around the web and put together in a linked set of web-sites. The sites are operated as personal notebooks. However, these days things can be easily published to the benefit of others concerned with the same issues. But be aware - this is not polished for presentation or peer-reviewed for exactness. I offer you just to have a look at my ‘work-desk’ as it appears in the moment. Any comment or suggestion can be mailed to dyrehaugen@gmail.com You can follow me on twitter as @dyrehaugen. Thanks for visiting! "],["links.html", "B Links", " B Links Current Dyrehaugen Sites: rcap - On Capitalism (loc) rclm - On Climate Change (loc) recs - On Economics (loc) rfin - On Finance (loc) rngy - On Energy (loc) renv - On Environment (loc) rsts - On Statistics (loc) rurb - On Urbanization (loc) rvar - On Varia (loc) rwsd - On Wisdom (loc) Blogs: rde - Blog in English (loc) rdn - Blog in Norwegian (loc) Discontinued: jdt - Collection (Jekyll) (loc) hdt - Collection (Hugo) (loc) Not listed: (q:) dhe dhn jrw56 (z:) rcsa rpad rstart "],["news.html", "C NEWS C.1 231018 The case against the US government’s big ​‘blue hydrogen’ bet C.2 221228 Sputtering Nuclear Sector C.3 221118 Floating Wind Farm C.4 220505 Silicon Batteries C.5 220116 Europe’s Energy Crisis C.6 211207 Novel lithium-carbon battery chemistry C.7 210710 Renewables passing Nuclear C.8 210629 Equinor triple UK hydrogen C.9 210604 Crane Battery C.10 210427 HeatCrete Thermal Storage C.11 210121 “Gas is over”", " C NEWS C.1 231018 The case against the US government’s big ​‘blue hydrogen’ bet St.John Can a ​“clean hydrogen hub” spend hundreds of millions of dollars turning fossil gas into hydrogen and still be considered clean? That’s the question that environmental groups and community activists are asking about the $7 billion in federal funding set to flow to hydrogen hub projects across the country. The hubs are meant to kick-start U.S. production of low- and zero-carbon hydrogen, an alternative fuel that could replace planet-warming fossil fuels in industries from heavy transportation to steelmaking. Of the seven public-private consortiums selected to compete for this funding by the Biden administration on Friday, five plan significant investments in ​“blue hydrogen,” the practice of making hydrogen from fossil gas but capturing and storing the carbon emissions from the process. Today, almost all of the roughly 10 million metric tons of hydrogen produced in the U.S. per year is so-called ​“gray hydrogen,” which is made with fossil gas and no carbon capture, and used for refining as well as fertilizer and other chemicals production. Can a ​“clean hydrogen hub” spend hundreds of millions of dollars turning fossil gas into hydrogen and still be considered clean? That’s the question that environmental groups and community activists are asking about the $7 billion in federal funding set to flow to hydrogen hub projects across the country. The hubs are meant to kick-start U.S. production of low- and zero-carbon hydrogen, an alternative fuel that could replace planet-warming fossil fuels in industries from heavy transportation to steelmaking. Of the seven public-private consortiums selected to compete for this funding by the Biden administration on Friday, five plan significant investments in ​“blue hydrogen,” the practice of making hydrogen from fossil gas but capturing and storing the carbon emissions from the process. Today, almost all of the roughly 10 million metric tons of hydrogen produced in the U.S. per year is so-called ​“gray hydrogen,” which is made with fossil gas and no carbon capture, and used for refining as well as fertilizer and other chemicals production. C.2 221228 Sputtering Nuclear Sector Canary Media The U.S. nuclear power market continued to sputter in 2022 as it faced regulatory, technical and financial setbacks — despite solid support from the federal government. This mirrors the global nuclear scene; plant closings and construction delays have resulted in nuclear falling to just 9.8 percent of global power generation in 2021, its lowest level since the 1980s, according to the World Nuclear Industry 2022 annual report. The United States generates more nuclear power than any other country in the world, with about 95 gigawatts of capacity, followed by China, but construction of new plants has been plagued by cost and schedule overruns, as well as an inability to keep up with the plunging costs of natural gas and renewable energy sources. Still, nuclear power provides a crucial 20 percent of U.S. electricity from the 92 light-water reactors that were built in a seemingly unreplicable construction binge in the 1970s and ​’80s. Some of these plants are struggling financially, many are approaching their decommission dates, and the only new large reactors constructed in recent memory, at the Plant Vogtle in Georgia, have been calamitous money pits brimming with incompetence and even fraud. Wesoff (2022) How did the US nuclear industry fare in 2022? C.3 221118 Floating Wind Farm World’s largest floating wind farm just started producing power Canary Media The first of 11 floating wind turbines in Equinor’s project off Norway is now generating clean electricity — which will be used to power an oil platform. Offshore wind development is surging globally as countries adopt ambitious climate change policies and wind equipment costs decline. The vast majority of today’s installations use fixed-bottom foundations to hoist turbines from relatively shallow seabeds. However, a small but quickly growing number of projects are using floating platforms and other buoyant technologies to generate wind power in deeper waters. About 56 gigawatts of offshore wind capacity was in place worldwide at the end of 2021. Of that total, only 121 megawatts — or 0.2 percent — involved floating turbines In the United States, the Biden administration aims to deploy 15 gigawatts of floating offshore wind capacity by 2035 — much of which will likely be installed off the coast of California. Equinor has been leading on floating wind development since 2009, when it installed the world’s first floating turbine off the coast of Aberdeenshire, Scotland. The company is majority-owned by the Norwegian government and changed its name from Statoil to Equinor in 2018. After floating the first turbine, Equinor added four offshore turbines to complete the 30-megawatt Hywind Scotland pilot project, which Equinor says is the world’s first commercial floating offshore wind farm. With the Hywind Tampen project, the company is expanding into its home waters. The wind farm is expected to meet about 35 percent of the fields’ electricity demand, reducing CO2 emissions by about 200,000 metric tons per year. Canary Media (2022) World’s largest floating wind farm just started producing power C.4 220505 Silicon Batteries Canary Media The German carmaker led a $400 million investment in American startup Group14 Technologies, which makes advanced batteries using silicon. Adding silicon to the anode (one of the key parts of a battery cell) could significantly improve the driving range and charge time of electric vehicles, two crucial metrics for their broader acceptance. But the technology is generally expected to be years away from widespread commercial adoption. Group14 aims to move up that timeline — its silicon anodes are on track to get into electric vehicles by 2023, CEO and co-founder Rick Luebbe told Canary Media. “Silicon batteries are here,” he said. ​“The technology is proven. Now it’s about scaling to meet the demand.” Group14 already has a factory outside of Seattle that produces 120 tons of silicon-carbon composite per year. But the new Series C funding will bankroll construction of another factory in central Washington, which will produce enough battery materials for 600,000 EVs per year when it’s fully operational in late 2023. A South Korea facility jointly developed with SK Group is coming online this year. Silicon anodes hold more energy than conventional graphite anodes. That inspired scientists to replace some or all of the graphite in the anode with silicon. Get Caught Up But this improvement doesn’t come without problems. Silicon expands and contracts as the battery charges and discharges, and those fluctuations can damage the battery. The trick for companies including Group14 is to harness the energy capacity of silicon while minimizing the damage it causes. Group14’s recipe, dubbed SCC55, uses ​“hard carbon-based scaffolding” to keep that silicon ​“in the most ideal form – amorphous, nano-sized, and carbon-encased,” according to the company’s website. In other words, the silicon sits in a miniscule scaffolding structure where it has room to expand and contract without weakening the structure of the anode, Luebbe explained. Achieving an ideal anode requires years of complicated laboratory science. Group14 grew out of a company called EnerG2 that focused on nanoengineering synthetic carbons; that parent company was sold to BASF, and then Group14 was spun out in 2015 to apply that technological approach to silicon anodes. Such scientific complexity usually harshes the vibes of venture investors hyped up on the prospect of quick software returns. But the prize in this case was particularly alluring. Silicon enthusiasts, Group14 included, claim that adding it to anodes can deliver 50 percent more energy density than today’s batteries. That could materialize as an EV that goes much farther on a single charge, or one that goes the same distance with a smaller, cheaper battery. Canary (2022)Porsche investment could unlock up to a 50% boost in EV battery density C.5 220116 Europe’s Energy Crisis Birol In Europe, governments should make natural gas storage part of their security of supply risk assessments, at both a national and regional level, including risks linked to the control of storage by entities from non-EU countries. And regulations should be improved to ensure that storage levels are adequate to cover end-user needs, with mandatory minimum storage obligations assigned to all commercial operators with gas retail portfolios. In addition, provisions on transparency and congestion management can help to ensure optimal utilisation of available storage capacity. On a global level, scaling up domestically sourced low-carbon energy supplies provides an opportunity to bring down emissions while at the same time tackling energy security issues related to fossil fuel imports and market volatility. However, potential energy security vulnerabilities do not disappear in a renewables-rich and more electrified energy system. Policy makers need to pay close attention to new clean energy supply chains, in particular the geographical concentration of many critical minerals – such as lithium, cobalt and rare earth elements – that are crucial components of many clean energy technologies. In my view, today’s situation underlines the fact that energy systems face significant risks if they rely too much on one supplier for a key element. Today, it is natural gas; tomorrow, it could be something else, such as lithium. Birol (2022) Europe and the world need to draw the right lessons from today’s natural gas crisis C.6 211207 Novel lithium-carbon battery chemistry A prototype battery-powered moped that can recharge in as little as 90 seconds could be on the road next year. The vehicle will be used to test a novel lithium-carbon battery developed by U.K.-based Allotrope Energy and unveiled in September by Mahle Powertrain, a British subsidiary of one of the world’s largest automotive suppliers. The fast-charging capacity is a result of the lithium-carbon battery’s high specific power, which tops 15 kilowatts per kilogram, according to Allotrope. This compares to a maximum of around 10 kW per kilogram for other lithium-ion chemistries. The fact that lithium-carbon batteries had not already been developed might seem odd given the battery industry’s keenness for novel chemistries. The holdup was because an essential component of the chemistry, nanoporous carbon, only recently began being used in battery development. In theory, the battery could be fully charged in just 60 seconds. The 90-second charging time is due to the limitations of charging infrastructure rather than the battery. The reason why it’s a 90-second charging is a buffered chargepoint. The charger has a battery inside it and the battery dumps its energy into the moped. For the larger batteries used in electric cars, there simply isn’t enough grid capacity to cope with lithium-carbon batteries. That’s why it’s unlikely the chemistry will be scaled up for larger vehicles. Another advantage of the lithium-carbon chemistry is that it does not use cobalt or nickel, two elements that pose supply-chain concerns in some other lithium-ion battery types. Canary Media C.7 210710 Renewables passing Nuclear BP Statistical Review of World Energy C.8 210629 Equinor triple UK hydrogen Norway’s state oil company Equinor will triple its UK hydrogen output, after setting out plans to build the world’s biggest hydrogen production plant with carbon capture and storage technology near Hull. Equinor plans to produce clean-burning “blue hydrogen” to supply the Keadby gas power plant in Lincolnshire, owned by energy company SSE, making it the world’s first full-scale power plant to burn pure hydrogen to generate electricity. Anders Opedal, the chief executive of Equinor, said on Monday that the company plans to produce another 1,200MW of blue hydrogen in the Humber area to help supply the Keadby hydrogen power plant. Guardian C.9 210604 Crane Battery Energy Vault stores clean electricity by stacking blocks of concrete. Energy Vault completed its first commercial-scale project in July 2020, when it connected a 5 megawatt/ 35 megawatt-hour block-stacking tower to the Swiss grid, the company said. The system’s six crane arms use electricity to hoist purpose-built concrete blocks and stack them into a tower; rapidly lowering the blocks discharges electricity. CanaryMedia C.10 210427 HeatCrete Thermal Storage Norway-headquartered EnergyNest makes its own branded ThermalBattery product which essentially stores heat in a patented form of concrete, which it has dubbed Heatcrete. A heat transfer fluid (HTF) at high temperatures passes through steel pipes cast into the ‘battery’, in technology that the company claims enables storage of energy at very low CapEx cost, using low-cost materials in a simple design. EnergyNest has previously said the Heatcrete materials can last 30 to 50 years of use without degradation. An investment worth €110 million (US$131.5 million) has been agreed by ‘thermal battery’ manufacturer EnergyNest which would make infrastructure equity investor Infracapital its biggest shareholder. Infracapital’s investment will be used by the thermal energy storage company towards delivering financed turnkey energy storage solutions in a range of international regions, EnergyNest made its first large-scale deployment in a research and demonstration project in Abu Dhabi with Masdar Institute, a 1MWth system developed between 2013 and late 2015. As with providers of other novel energy storage technologies, the company has been seeking to commercialise its products and offerings over the past few years and claimed that 2020 was its strongest year to date. In January last year Energy-Storage.news reported that the company was deploying a multi-megawatt solution at a brick making factory in Austria and in June announced a partnership with Siemens Energy to develop commercialised solutions — the pair had already worked together previously at the Abu Dhabi pilot project. A project with Italian energy major Eni at a solar energy plant is also already underway and another with Norwegian chemical company Yara is in development to produce steam for industrial use. EnergyStorage EnergyNest C.11 210121 “Gas is over” Europe needs to acknowledge that its future is no longer with fossil fuels, said the President of the European Investment Bank as he presented the bank’s 2020 results on Wednesday (20 January). “To put it mildly, gas is over,” Dr Werner Hoyer said at a press conference on the EIB’s annual results. EIB Gas is over "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
